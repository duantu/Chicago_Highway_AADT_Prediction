{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6841782c-b123-4a13-85c4-69a7a3dab6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "datadir = \"../data/\"\n",
    "datafilename = \"Certainty.xlsx\"\n",
    "datafilepath = datadir + datafilename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e42f6051-df94-493e-ac8a-b6983f804b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# Read in network information from sheet P0\n",
    "# For now we only use the folloiwng columns in each sheet\n",
    "cols_used = ['Link ID','ANODE','BNODE','A X_COORD','A Y_COORD','B X_COORD','B Y_COORD','Link Length(miles)','# of lanes-A','Capacity-A (veh/h)',\n",
    "            'auto volume(2010)-A','AADT(2010)-A','Speed(mph)-A','VMT-A']\n",
    "# # For now we assume all occurrences of the same link in the same sheet share the same attribute values\n",
    "df_p0= pd.read_excel(datafilepath, sheet_name='P0',usecols=cols_used).dropna(subset=['Link ID'])\n",
    "# df_p0_unique = df_p0.drop_duplicates(subset=['Link ID']).dropna(subset=['Link ID', 'A X_COORD', 'A Y_COORD', 'B X_COORD', 'B Y_COORD'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "624bc0f8-4dfb-4350-9220-16c60e9ff507",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Only Read in Project 3, 4 and 5\n",
    "integers = [ 3, 4, 5]\n",
    "def generate_combinations(integers):\n",
    "    all_combinations = []\n",
    "    # Loop through lengths from 1 to 6\n",
    "    for length in range(1, 1+len(integers)):\n",
    "        # Generate combinations of the current length\n",
    "        comb = combinations(integers, length)\n",
    "        # Convert each combination to a string, add \"p\", and add to the list\n",
    "        all_combinations.extend(['P' + ''.join(map(str, c)) for c in comb])\n",
    "    return all_combinations\n",
    "# Generate all combinations\n",
    "sheet_names = generate_combinations(integers)\n",
    "\n",
    "# Insert \"P0\" at the beginning of the list\n",
    "sheet_names.insert(0, \"P0\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6600ef0f-dc1c-41ab-962a-8dd2a513eb3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Read in all 2^6 sheets of data ###\n",
    "\n",
    "\n",
    "# ## Create a list of sheet names ##\n",
    "\n",
    "# # List of the first 6 integers\n",
    "# integers = [1, 2, 3, 4, 5, 6]\n",
    "\n",
    "# # Function to generate combinations\n",
    "# def generate_combinations(integers):\n",
    "#     all_combinations = []\n",
    "#     # Loop through lengths from 1 to 6\n",
    "#     for length in range(1, 7):\n",
    "#         # Generate combinations of the current length\n",
    "#         comb = combinations(integers, length)\n",
    "#         # Convert each combination to a string, add \"p\", and add to the list\n",
    "#         all_combinations.extend(['P' + ''.join(map(str, c)) for c in comb])\n",
    "#     return all_combinations\n",
    "\n",
    "# # Generate all combinations\n",
    "# sheet_names = generate_combinations(integers)\n",
    "\n",
    "# # Insert \"P0\" at the beginning of the list\n",
    "# sheet_names.insert(0, \"P0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e7b5dde9-a937-428d-9b0e-4a68b9cb8dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Preparing the sample set\n",
    "# dataset = []\n",
    "# for ii in range(len(sheet_names)):\n",
    "#     # read the correct sheet \n",
    "#     df= pd.read_excel(datafilepath, sheet_name=sheet_names[ii],usecols=cols_used).dropna(subset=['Link ID'])\n",
    "    \n",
    "#     # dataframe for edge attributes\n",
    "#     df_edge_attr= np.column_stack((df['Link Length(miles)'],df['# of lanes-A'],df['Capacity-A (veh/h)'], df['Speed(mph)-A'],df['auto volume(2010)-A']))\n",
    "#     # edge_attr = torch.tensor(df_edge_attr, dtype=torch.float)\n",
    "\n",
    "#     df_edge_labels = df['AADT(2010)-A'].to_numpy()\n",
    "#     # edge_labels = torch.tensor(df['AADT(2010)-A'].to_numpy(),dtype=torch.float)\n",
    "#     # new_data = Data(x=x, edge_index=edge_index, edge_attr=edge_attr, y=edge_labels)\n",
    "    \n",
    "#     # sample = (df_edge_attr, df_edge_labels)\n",
    "#     dataset.append(sample)\n",
    "\n",
    "# X = [sample[0] for sample in dataset]\n",
    "# y = [sample[1] for sample in dataset]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af67dac6-7a10-4e01-a99a-fbce0e3a91d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalized data\n",
    "# Preparing the sample set\n",
    "dataset = np.zeros((len(sheet_names)*650,6))\n",
    "for ii in range(len(sheet_names)):\n",
    "    # read the correct sheet \n",
    "    df= pd.read_excel(datafilepath, sheet_name=sheet_names[ii],usecols=cols_used).dropna(subset=['Link ID'])\n",
    "    \n",
    "    # dataframe for edge attributes\n",
    "    df_edge_attr= np.column_stack((df['Link Length(miles)'],df['# of lanes-A'],df['Capacity-A (veh/h)'], df['Speed(mph)-A'],df['auto volume(2010)-A']))\n",
    " \n",
    "    df_edge_labels = df['AADT(2010)-A'].to_numpy()\n",
    "  \n",
    "    sample = np.column_stack((df_edge_attr, df_edge_labels))\n",
    "    dataset[ii*650:(ii+1)*650]= sample\n",
    "\n",
    "\n",
    "df_dataset = pd.DataFrame(dataset, columns=['Link Length(miles)','# of lanes-A','Capacity-A (veh/h)','Speed(mph)-A','auto volume(2010)-A','AADT(2010)-A'])\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "scaled_dataset = scaler.fit_transform(df_dataset)\n",
    "\n",
    "X = scaled_dataset[:,:5]\n",
    "y = scaled_dataset[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc57b89f-65ff-420b-b75f-3e6bd25c8e83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.03195832 0.14285714 0.19816724 0.51226189 0.05863944]\n",
      " [0.07325476 0.14285714 0.19816724 0.51226189 0.0962912 ]\n",
      " [0.04867793 0.14285714 0.19816724 0.5118514  0.10709455]\n",
      " ...\n",
      " [0.02856299 0.14285714 0.05211913 0.22260648 0.0511708 ]\n",
      " [0.00846723 0.         0.05211913 0.21372917 0.05832164]\n",
      " [0.02118534 0.14285714 0.05211913 0.23220691 0.0371458 ]]\n"
     ]
    }
   ],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "14c8e3f5-cc2a-4bfe-8b77-57859981e632",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.25)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X_train = torch.tensor(X_train, dtype=torch.float)\n",
    "y_train = torch.tensor(y_train,dtype=torch.float)\n",
    "X_val = torch.tensor(X_val,dtype=torch.float)\n",
    "y_val = torch.tensor(y_val,dtype=torch.float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ab2504ea-52fa-4646-8f14-0ceb9d98871b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Loss: 1.68e-03\n",
      "Epoch 2/50, Loss: 9.63e-04\n",
      "Epoch 3/50, Loss: 8.70e-04\n",
      "Epoch 4/50, Loss: 8.02e-04\n",
      "Epoch 5/50, Loss: 7.48e-04\n",
      "Epoch 6/50, Loss: 7.10e-04\n",
      "Epoch 7/50, Loss: 6.81e-04\n",
      "Epoch 8/50, Loss: 6.53e-04\n",
      "Epoch 9/50, Loss: 6.28e-04\n",
      "Epoch 10/50, Loss: 6.08e-04\n",
      "Epoch 11/50, Loss: 5.87e-04\n",
      "Epoch 12/50, Loss: 5.68e-04\n",
      "Epoch 13/50, Loss: 5.52e-04\n",
      "Epoch 14/50, Loss: 5.38e-04\n",
      "Epoch 15/50, Loss: 5.27e-04\n",
      "Epoch 16/50, Loss: 5.13e-04\n",
      "Epoch 17/50, Loss: 5.15e-04\n",
      "Epoch 18/50, Loss: 5.05e-04\n",
      "Epoch 19/50, Loss: 4.95e-04\n",
      "Epoch 20/50, Loss: 4.85e-04\n",
      "Epoch 21/50, Loss: 4.84e-04\n",
      "Epoch 22/50, Loss: 4.74e-04\n",
      "Epoch 23/50, Loss: 4.68e-04\n",
      "Epoch 24/50, Loss: 4.58e-04\n",
      "Epoch 25/50, Loss: 4.59e-04\n",
      "Epoch 26/50, Loss: 4.45e-04\n",
      "Epoch 27/50, Loss: 4.43e-04\n",
      "Epoch 28/50, Loss: 4.41e-04\n",
      "Epoch 29/50, Loss: 4.33e-04\n",
      "Epoch 30/50, Loss: 4.25e-04\n",
      "Epoch 31/50, Loss: 4.24e-04\n",
      "Epoch 32/50, Loss: 4.18e-04\n",
      "Epoch 33/50, Loss: 4.13e-04\n",
      "Epoch 34/50, Loss: 4.19e-04\n",
      "Epoch 35/50, Loss: 4.13e-04\n",
      "Epoch 36/50, Loss: 4.03e-04\n",
      "Epoch 37/50, Loss: 4.11e-04\n",
      "Epoch 38/50, Loss: 3.97e-04\n",
      "Epoch 39/50, Loss: 3.93e-04\n",
      "Epoch 40/50, Loss: 4.01e-04\n",
      "Epoch 41/50, Loss: 4.07e-04\n",
      "Epoch 42/50, Loss: 4.00e-04\n",
      "Epoch 43/50, Loss: 3.81e-04\n",
      "Epoch 44/50, Loss: 3.95e-04\n",
      "Epoch 45/50, Loss: 3.87e-04\n",
      "Epoch 46/50, Loss: 3.92e-04\n",
      "Epoch 47/50, Loss: 3.85e-04\n",
      "Epoch 48/50, Loss: 3.86e-04\n",
      "Epoch 49/50, Loss: 3.87e-04\n",
      "Epoch 50/50, Loss: 3.86e-04\n",
      "Absolute Validation Loss: 3.42e-04\n",
      "\n",
      " Relative Loss =  3.42e-04  /  4.80e-02  =  7.13e-03 \n",
      "\n",
      "Sample 1 Predictions:\n",
      " [ 48685.676  127004.58    38445.152   39498.547   22958.156  120406.37\n",
      "  83380.76    58123.45    43954.137    4042.8662  90916.46    44791.22\n",
      "  21559.795   24474.434   67478.34    21548.457   34970.156  101791.52\n",
      "   8751.83     5905.5776  67705.32    29214.434  140947.17    37055.44\n",
      "  82034.39    67903.17   212135.17    73562.875   31492.26    79745.71\n",
      "  29065.828  118466.81    82864.45    42110.613   18895.354    4577.1016\n",
      "  64606.125  263811.44    27480.906   38750.734   63457.727  163534.72\n",
      "  63089.812   28561.432  144756.62    33667.926   11499.08    20432.797\n",
      "  27635.754   46509.598   28065.457    6136.726   23505.658   40122.277\n",
      "  21860.955   13272.732   14835.443   23511.305   23713.586   43737.816\n",
      "  11490.856   64267.766   58521.797   27985.182   48575.56    28031.598\n",
      "  72744.805   69694.41    26537.32    48064.11   263314.8    284973.53\n",
      "  31009.04   145004.03    63621.902    9825.367   68265.57    40045.133\n",
      "  24668.07    77155.69    68128.77    47047.66    29699.621   22721.277\n",
      " 263059.9     27760.691    1132.7336  60918.098   27477.062  138153.97\n",
      "  61253.254   26435.012   39268.195   22369.602   43858.465   20300.564\n",
      "  43538.477   36021.594   22432.867  295700.88    38486.453   11628.407\n",
      "  35527.875   60118.74   108239.234   24643.684   83793.98    59534.414\n",
      " 151225.48     3838.3413  85192.164   47638.04    60024.805   25594.115\n",
      "  74699.61    72420.91     5823.0024  47171.7     40138.543   52459.35\n",
      "  40601.16    68557.14    64985.707   32644.312   93530.78    53934.06\n",
      "   6704.9185  74392.09    35916.48     9923.591   26891.133   77507.08\n",
      "  69496.7     30205.494   47417.64    32523.676    7561.653   29079.66\n",
      " 112983.19    90245.555   77975.92    31045.92    58316.535  102850.3\n",
      "  36010.043   12397.333   33846.805   66756.72    66697.83    13631.006\n",
      "  29356.854   14004.206   17697.338   26210.414   64584.36    32224.814\n",
      "  52738.81    61108.285   24780.285   70304.22   102239.664   59068.55\n",
      "  32617.482   40994.63    58283.348   68429.12    35961.23    63923.043\n",
      "  46089.523    1455.8674   3992.9614  81164.59     2932.0317  33821.86\n",
      "  32177.287   71800.234   91560.36    46785.727   24651.016   41960.727\n",
      "  24194.69    59093.105   93321.64    22594.791   30859.883   48861.125\n",
      "  54444.535   26341.453   32828.4     20551.438   31836.498  138800.56\n",
      "  55437.93    15137.697   85946.51    66153.51    50729.473   39467.684\n",
      "  45925.066   32269.816   13829.424   23496.06    19429.78    20283.432\n",
      "  32836.895   28468.611   14997.69    43376.613   60554.266   30142.969\n",
      "  41819.35    26149.85   115128.766   94265.6     40118.664  156606.31\n",
      "  77237.37    30251.781  160930.5     43352.516   35461.125   11482.235\n",
      "  64871.965   26565.123  144010.45    79953.63    32867.484   35496.85\n",
      "  61234.645   54204.51    60322.87    41654.125  103224.266   65445.44\n",
      "  69484.64    76931.95    57060.465   37924.375   33146.797   39727.395\n",
      "  63515.71    33079.625    7679.1606  11654.122  150164.9     40001.902\n",
      "  71944.13    28122.15    69395.91    44420.633   53582.117   47365.695\n",
      "  14153.079   68082.75    41256.027   25622.125   45669.83    60846.684\n",
      "  32337.023   25549.55    28603.818   34748.46    24759.621   89579.15\n",
      "  42415.8     22545.477   35816.402   46714.01     3818.1348  20696.639\n",
      "  78214.68   113233.96    39046.01    63423.906  179802.      26008.926\n",
      "  65715.35    15307.765   39376.715   56232.76    59564.246   78705.69\n",
      "  32416.18    43737.02    24773.633   50283.37    48516.68    55900.098\n",
      "   1786.6053  24262.566   32558.81    40939.723  112678.15    34444.25\n",
      "  29930.242   24109.77   145744.78    42432.066   51009.047   41746.887\n",
      "  12202.23    63201.26    27629.879   52452.87    44854.46    65903.36\n",
      "  31104.201   54862.82    91728.76    29406.578   15887.264   46159.73\n",
      "  40459.477   52850.324   56050.137   67877.56    25379.047   69759.79\n",
      "  33337.258   35887.887   34001.22     9573.917   30873.383   44923.254\n",
      "  27475.709   26308.275   28163.697    9838.025   68157.875   65761.47\n",
      " 142443.23    29341.678  140416.42    29766.78    47249.34    67474.79\n",
      "  37867.906   79136.32    70084.055   86678.2     75846.695   32112.21\n",
      "  58773.76    38310.18    76838.92    62197.297   46039.324   31938.93\n",
      "  62679.85    31208.781    3866.252   42738.59    91780.41    75655.35\n",
      "  73189.79    27821.459   67384.18    32108.99    66895.766   90147.06\n",
      "  69410.25    66294.57   129920.664   30448.518   33963.39    41796.22\n",
      "  24412.994   24896.531   43132.164   52651.23    41332.36    33636.59\n",
      " 109617.39    38462.344   72763.195   68252.12    64735.562   51039.46\n",
      "  58895.387  244241.73    66592.23     6841.657   21100.512   59560.605\n",
      "  37420.062  145804.97   127505.02   113076.87    38186.43    33394.35\n",
      "  76727.18    35530.516   18818.113   50494.043    9168.905    6726.8574\n",
      "  43041.184   29264.736   27969.61    13796.67   264547.94    40845.746\n",
      "  40349.32    28631.164   20143.234   64806.953   57994.273   23004.227\n",
      "  25405.629   77581.76    76161.01    48768.984   24776.975   39203.316\n",
      " 126624.38    43593.03    22585.475   33044.184   28555.053  123168.61\n",
      "  24862.021  113406.63    32971.566   41120.926   30168.37    38510.31\n",
      "  25745.836   34723.914   67733.73    11438.439   54999.29    45196.496\n",
      "  83285.26    26748.736   57550.82    21025.54    38201.773   84974.586\n",
      "  33141.02   133213.16   102788.2     49312.35    54202.688   51060.992\n",
      "  24475.688   33523.957   49636.484   72472.33   112070.39    25327.797\n",
      "   1878.8083  87766.805    6696.5635  28045.62    40621.33    26436.656\n",
      "  80041.445   23345.492   51978.566   66154.87    40969.43   124924.625\n",
      "  20080.934   67839.35    83626.35   128542.82    67132.15   140991.\n",
      "  69281.65     7062.7534  72762.35   127624.97    46063.336   30254.08\n",
      "  38502.73    22441.893   51791.21   298388.38    78011.65    28108.66\n",
      " 104237.75    17241.025   83868.375    6784.913   20189.69   105430.125\n",
      "  37005.117   18719.809   32013.82    68079.086   38223.44    48874.785\n",
      "  56714.332   21533.676   35957.44   126869.664   47648.473   34466.88\n",
      "  60597.152   33362.77    29154.82    35868.336  258055.27    66588.62\n",
      "  64571.53     7216.701   87162.28    23134.299   54292.387   60855.094\n",
      "  39312.133   25589.648   35940.758   32198.375   68240.61   146725.2\n",
      "  53043.094    2493.9058  25499.723   38972.23    27167.775   29623.873\n",
      "  66069.87    59627.434   23377.738   18978.025   44405.906   67317.625\n",
      "  23553.85    36369.5    120125.94    27382.9    107028.89    30548.871\n",
      "  26725.13    32129.91    62104.273   34653.348   22621.643  106891.68\n",
      "  64496.957    7457.9297  13893.51    35979.93    34929.76     8764.702\n",
      "  41671.566  139350.      38185.617   39506.855    9071.637   13985.001\n",
      "  18124.13     1881.6947  57276.9     34148.895   79593.43    32844.305\n",
      "  90837.016   56178.066   45560.26    42101.984   34427.08    35240.383\n",
      "  35208.926   20897.725   28625.918   35498.895    8616.111   13289.844\n",
      "  23190.904   38400.938   57443.984   18902.914   29322.521   11163.887\n",
      "  63282.832  123803.01    56762.707   63811.72    44793.65    63504.39\n",
      "  37975.12    33398.145   87481.336   64313.945   30519.844   44690.355\n",
      "  77736.48    29098.988    6149.3403  35092.395   20289.121  105208.51\n",
      " 163020.67    33006.914   77585.84    46090.754  113249.72    72473.67\n",
      "  47277.24    45408.805   29631.574   68891.12    63208.98    61799.312\n",
      "  26807.27     5386.05    31974.383   25974.113   28518.418   91261.6\n",
      "  37725.598    1037.6628  31878.826  134198.48    74002.836   24807.771\n",
      "  58816.543   91322.375   21609.045    9843.359   43540.21    70241.56\n",
      "  47901.47   112863.1     21584.387   60419.047   23757.287   14871.233\n",
      " 155077.66    31806.775   57950.477   19329.465   44748.266  162418.72\n",
      "  91396.2     33068.32    43843.11    39135.76    35379.574  242592.4\n",
      "  39411.758   17993.498   88965.06    26725.453   65472.26    79019.67\n",
      " 140976.14    43412.074 ]\n",
      "Sample 2 Predictions:\n",
      " [ 48685.676 127004.58   38445.152 ...  41604.438  55211.68   25737.885]\n"
     ]
    }
   ],
   "source": [
    " \n",
    "# Define the neural network model\n",
    "class RegressionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RegressionModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(5, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "model = RegressionModel()\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training the model\n",
    "num_epochs = 50\n",
    "batch_size = 1\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    for i in range(X_train.shape[0]):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_train[i])\n",
    "        loss = criterion(outputs.squeeze(), y_train[i])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {format(epoch_loss/X_train.shape[0],\".2e\")}')\n",
    "\n",
    "# Evaluate the model\n",
    "model.eval()\n",
    "val_loss = 0.0\n",
    "relative_val_loss = 0.0\n",
    "sum_edge_labels = 0.0\n",
    "with torch.no_grad():\n",
    "    for i in range(X_val.shape[0]):\n",
    "        outputs = model(X_val[i])\n",
    "        loss = criterion(outputs.squeeze(), y_val[i])\n",
    "        val_loss += loss.item()\n",
    "        \n",
    "        # Calculate Relative Validation Loss\n",
    "        edge_labels_tensor = criterion(torch.zeros(torch.Tensor.size(y_val[i])), y_val[i])\n",
    "        sum_edge_labels += edge_labels_tensor.item()\n",
    "        relative_loss = loss/edge_labels_tensor\n",
    "        relative_val_loss += relative_loss.item()\n",
    "        \n",
    "abs_val_loss = val_loss/X_val.shape[0]\n",
    "print(f'Absolute Validation Loss: {format(abs_val_loss,\".2e\")}')\n",
    "\n",
    "# calculate average edge_labels_tensor.item()\n",
    "ave_edge_labels = sum_edge_labels/X_val.shape[0]\n",
    "print(\"\\n Relative Loss = \", format(abs_val_loss, '.2e'), \" / \", format(ave_edge_labels,'.2e'), \" = \", format(abs_val_loss/ave_edge_labels,'.2e'), \"\\n\")\n",
    " \n",
    "\n",
    "# Make predictions\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for i in range(int(X_val.shape[0]/650)):\n",
    "        pred_scaled = model(X_val[:(i+1)*650,:]).squeeze()\n",
    "        # transform the prediction into unscaled values\n",
    "\n",
    "        pred_scaled = pred_scaled.numpy() # convert predicted label to np array\n",
    "        data_pred_scaled = np.hstack((X_val[:(i+1)*650,:], pred_scaled.reshape(-1,1)))\n",
    "     \n",
    "        unscaled_pred = scaler.inverse_transform(data_pred_scaled)\n",
    "\n",
    "        print(f'Sample {i+1} Predictions:\\n', unscaled_pred[:,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5f861d-d315-4ccf-8244-36e6d07ff6e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a757519-0f7e-4471-8d3b-e043ffb9db7e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
