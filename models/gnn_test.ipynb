{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "eb414927-0377-46d6-b0c7-86e6684568f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In order to push without output, use the following command:\n",
    "#git push --quiet -u --no-progress \n",
    "\n",
    "datadir = \"../data/\"\n",
    "datafilename = \"ITAprojectevaluation_C_ML_corrected.xlsx\"\n",
    "datafilepath = datadir + datafilename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8aa7cd45-6466-4389-96ed-f5ac4bcb787f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Link ID   ANODE   BNODE A X_COORD  A Y_COORD  B X_COORD  B Y_COORD   CapAB  \\\n",
      "1   6087.0  8921.0  8981.0  407624.2  4649654.5   408005.8  4649863.0  4000.0   \n",
      "2   6457.0  9137.0  8980.0  409288.6  4649934.5   408004.0  4649903.5  4000.0   \n",
      "3   6262.0  9027.0  9141.0    408444  4649861.0   409327.9  4649862.0  4000.0   \n",
      "4   6465.0  9141.0  9267.0  409327.9  4649862.0   410370.2  4649433.0  4000.0   \n",
      "5   6660.0  9264.0  9137.0  410362.6  4649530.5   409288.6  4649934.5  4000.0   \n",
      "\n",
      "   CapBA  \n",
      "1    0.0  \n",
      "2    0.0  \n",
      "3    0.0  \n",
      "4    0.0  \n",
      "5    0.0  \n",
      "\n",
      " nodes.head()\n",
      "   node_id   x_coord    y_coord\n",
      "1   8921.0  407624.2  4649654.5\n",
      "2   9137.0  409288.6  4649934.5\n",
      "3   9027.0  408444.0  4649861.0\n",
      "4   9141.0  409327.9  4649862.0\n",
      "5   9264.0  410362.6  4649530.5\n",
      "\n",
      " edges.head()\n",
      "   edge_id     src    dest  capacity  # of lanes   AADT(2010)-A\n",
      "0   6087.0  8921.0  8981.0    4000.0         3.5   65590.052876\n",
      "1   6457.0  9137.0  8980.0    4000.0         3.5   90151.850498\n",
      "2   6262.0  9027.0  9141.0    4000.0         3.5  111104.059618\n",
      "3   6465.0  9141.0  9267.0    4000.0         3.5  115923.227290\n",
      "4   6660.0  9264.0  9137.0    4000.0         3.5  119778.561429\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df1 = pd.read_excel(datafilepath, sheet_name='Link_info')\n",
    "df1_unique = df1.drop_duplicates(subset=['Link ID']).dropna(subset=['Link ID', 'A X_COORD', 'A Y_COORD', 'B X_COORD', 'B Y_COORD'])\n",
    "\n",
    "a_nodes = df1_unique[['ANODE', 'A X_COORD', 'A Y_COORD']].rename(columns={\"ANODE\": \"node_id\", \"A X_COORD\": \"x_coord\", \"A Y_COORD\": \"y_coord\"})\n",
    "b_nodes = df1_unique[['BNODE', 'B X_COORD', 'B Y_COORD']].rename(columns={\"BNODE\": \"node_id\", \"B X_COORD\": \"x_coord\", \"B Y_COORD\": \"y_coord\"})\n",
    "nodes = pd.concat([a_nodes, b_nodes]).drop_duplicates().apply(pd.to_numeric)\n",
    "# There are duplicate node_ids with different coordinates!\n",
    "# For an example:\n",
    "# print(nodes.loc[nodes['node_id'] == 11175])\n",
    "# TODO: Figure out why\n",
    "nodes = nodes.drop_duplicates(subset=['node_id'])\n",
    "\n",
    "a_edges = df1_unique[['Link ID', 'ANODE', 'BNODE', 'CapAB']].rename(columns={'Link ID': 'edge_id', \"ANODE\": \"src\", \"BNODE\": \"dest\", \"CapAB\": \"capacity\"})\n",
    "b_edges = df1_unique[['Link ID', 'BNODE', 'ANODE', 'CapBA']].rename(columns={'Link ID': 'edge_id', \"BNODE\": \"src\", \"ANODE\": \"dest\", \"CapBA\": \"capacity\"})\n",
    "edges = pd.concat([a_edges, b_edges]).drop_duplicates().apply(pd.to_numeric)\n",
    "# There are duplicate edge_ids with different src,dest and different capacities!\n",
    "# For an example:\n",
    "# print(edges.loc[edges['edge_id'] == 9197])\n",
    "# TODO: Figure out why\n",
    "edges = edges.drop_duplicates(subset=['edge_id'])\n",
    "\n",
    "# For now, we're only interested in two columns\n",
    "# AADT(2010)-B is the \"before\" treatment. We treat this as an additional edge feature\n",
    "# AADT(2010)-A is the \"after\" treatment. We treat this as the response variable\n",
    "df2 = pd.read_excel(datafilepath, sheet_name='AADT_Benefits_C').drop_duplicates(subset=['Link ID']).dropna(subset=['Link ID','AADT(2010)-A','AADT(2010)-B'])\n",
    "df2 = df2[['Link ID', '# of lanes', 'AADT(2010)-A']].rename(columns={'Link ID': 'edge_id'}).drop_duplicates().apply(pd.to_numeric)\n",
    "edges = edges.merge(right=df2, on=['edge_id'], how='left')\n",
    "\n",
    "print(df1_unique.head())\n",
    "print(\"\\n nodes.head()\")\n",
    "print(nodes.head())\n",
    "print(\"\\n edges.head()\")\n",
    "print(edges.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "aac212cf-93ff-41ad-92e0-9d8c4857f136",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAGsCAYAAADTx2yiAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/H5lhTAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAyeElEQVR4nO3dfVxUZd4/8M8wwIAIqCgIOIJPKIaYYhpqagtlxS+f2n61S7eulZVOCrprSE+upUC5dbumtynrQ/18YFc3lRIfuLX0VdqKEIlmPOQDpCG1yIyYDjhz/f6YnG0UkBnAM3Pxeb9e52Vz5jrXfK+G8+FwzZlzVEIIASIikpab0gUQEVHbYtATEUmOQU9EJDkGPRGR5Bj0RESSY9ATEUmOQU9EJDkGPRGR5Bj0RESSY9ATEUlOuqA/dOgQHn30UYSEhEClUmHHjh129yGEwF/+8hdERERAo9EgNDQUS5Ysaf1iiYjuAHelC2htV65cweDBg/H0009jypQpDvWRlJSEffv24S9/+QsGDRqE6upqVFdXt3KlRER3hkrmi5qpVCps374dkyZNsq4zGo145ZVXsGXLFtTU1CAqKgpvvfUWxo0bBwA4deoUoqOjceLECfTv31+ZwomIWpF0Uze38+KLL+LIkSPIysrC8ePH8fjjj+Ohhx5CaWkpAODjjz9G79698cknn6BXr14IDw/Hs88+yyN6InJZ7Sroy8vLsX79emzduhX33Xcf+vTpgz/96U8YPXo01q9fDwA4ffo0zp07h61bt+LDDz/Ehg0bkJ+fj9/+9rcKV09E5Bjp5uibUlRUBJPJhIiICJv1RqMRAQEBAACz2Qyj0YgPP/zQ2m7t2rWIiYlBcXExp3OIyOW0q6Cvra2FWq1Gfn4+1Gq1zXMdO3YEAAQHB8Pd3d3ml0FkZCQAy18EDHoicjXtKuiHDBkCk8mEqqoq3HfffQ22GTVqFK5fv47vvvsOffr0AQCUlJQAAMLCwu5YrURErUW6s25qa2tRVlYGwBLs7777Lu6//3506dIFPXv2xFNPPYUvvvgC77zzDoYMGYIff/wR+/fvR3R0NBISEmA2m3HPPfegY8eOWLZsGcxmM3Q6Hfz8/LBv3z6FR0dE5AAhmU8//VQAuGWZNm2aEEKIuro68frrr4vw8HDh4eEhgoODxeTJk8Xx48etfZw/f15MmTJFdOzYUQQFBYk//OEP4t///rdCIyIiahnpjuiJiMhWuzq9koioPWLQExFJTpqzbsxmMy5cuABfX1+oVCqlyyEialNCCFy+fBkhISFwc2v6mF2aoL9w4QK0Wq3SZRAR3VEVFRXo0aNHk22kCXpfX18AlkH7+fkpXA0RUdsyGAzQarXW7GuKNEF/Y7rGz8+PQU9E7UZzpqr5YSwRkeQY9EREkmPQExFJjkFPRCQ5Bj0RkeQY9EREkmPQExFJjkFPRCQ5Bj0RkeQY9EREkmPQExFJjkFPRCQ5Bj0RkeQY9EREkmPQExFJjkFPRCQ5Bj0RkeQY9EREkmPQExFJjkFPRCQ5Bj0RkeQY9EREkmPQExFJjkFPRCQ5Bj0RkeQY9EREkmPQExFJjkFP5CwKCgCdDrh8WelKSDLuShdARL94+WXg7FnA21vpSkgyDHoiZ/DZZ8DevcC2bYA7d0tqXS2ausnIyIBKpUJycnKjbcaNGweVSnXLkpCQYG0jhMDrr7+O4OBgeHt7Iz4+HqWlpS0pjci1nDoFjB4NTJmidCUkIYeDPi8vD6tXr0Z0dHST7T766CP88MMP1uXEiRNQq9V4/PHHrW3efvttLF++HO+//z7+9a9/wcfHB+PHj8e1a9ccLY/ItcycCRw8CKhUSldCEnIo6Gtra5GYmIjMzEx07ty5ybZdunRB9+7drUtubi46dOhgDXohBJYtW4ZXX30VEydORHR0ND788ENcuHABO3bscKQ8ItfkxnMjqG049JOl0+mQkJCA+Ph4u7ddu3YtnnzySfj4+AAAzpw5g8rKSpu+/P39MWLECBw5cqTRfoxGIwwGg81CRES3svtTn6ysLBQUFCAvL8/uFzt69ChOnDiBtWvXWtdVVlYCAIKCgmzaBgUFWZ9rSHp6OhYtWmR3DURE7Y1dR/QVFRVISkrCpk2b4OXlZfeLrV27FoMGDcLw4cPt3vZmqamp0Ov11qWioqLFfRIRyciuoM/Pz0dVVRWGDh0Kd3d3uLu74+DBg1i+fDnc3d1hMpka3fbKlSvIysrCM888Y7O+e/fuAICLFy/arL948aL1uYZoNBr4+fnZLEREdCu7gj4uLg5FRUUoLCy0LsOGDUNiYiIKCwuhVqsb3Xbr1q0wGo146qmnbNb36tUL3bt3x/79+63rDAYD/vWvfyE2NtbO4RAR0c3smqP39fVFVFSUzTofHx8EBARY10+dOhWhoaFIT0+3abd27VpMmjQJAQEBNutvnIe/ePFi9OvXD7169cJrr72GkJAQTJo0yYEhERHRr7X6V/DKy8vhdtNpYsXFxfj888+xb9++Brd56aWXcOXKFTz33HOoqanB6NGjsWfPHoc+ByAiIlsqIYRQuojWYDAY4O/vD71ez/l6IpKePZnHb2gQEUmOQU9EJDkGPRGR5Bj0RESSY9ATEUmOQU9EJDkGPRGR5Bj0RESSY9ATORO9HsjMBHh3NWpFDHoiZyIEMHkywMt/UCvi7eaJnEmnTkpXQBLiET0RkeQY9EREkmPQEzmrU6eUroAkwaAnckYFBcBddwH//KfSlZAEGPREzmjIEGDiRGDmTODHH5Wuhlwcg57IGalUwKpVgMkEvPii0tWQi2PQEzmr7t2BFSuA7GygrEzpasiFMeiJnNmTTwKlpUDfvkpXQi6MQU/kzFQqoEcPpasgF8egJyKSHIOeiEhyDHoiIskx6ImIJMegJyKSHIOeiEhyDHoiIskx6ImIJMegJyKSHIOeiEhyDHoiIskx6ImIJNeioM/IyIBKpUJycnKT7WpqaqDT6RAcHAyNRoOIiAjk5ORYnzeZTHjttdfQq1cveHt7o0+fPnjzzTchhGhJeUREBMDd0Q3z8vKwevVqREdHN9murq4ODzzwAAIDA7Ft2zaEhobi3Llz6NSpk7XNW2+9hVWrVuGDDz7AXXfdhWPHjmH69Onw9/fHnDlzHC2RiIjgYNDX1tYiMTERmZmZWLx4cZNt161bh+rqahw+fBgeHh4AgPDwcJs2hw8fxsSJE5GQkGB9fsuWLTh69Kgj5RER0a84NHWj0+mQkJCA+Pj427bNzs5GbGwsdDodgoKCEBUVhbS0NJhMJmubkSNHYv/+/SgpKQEAfP311/j888/x8MMPN9qv0WiEwWCwWYiI6FZ2H9FnZWWhoKAAeXl5zWp/+vRpHDhwAImJicjJyUFZWRlmzZqF+vp6LFy4EACwYMECGAwGDBgwAGq1GiaTCUuWLEFiYmKj/aanp2PRokX2lk9E1O7YFfQVFRVISkpCbm4uvLy8mrWN2WxGYGAg1qxZA7VajZiYGJw/fx5Lly61Bv0//vEPbNq0CZs3b8Zdd92FwsJCJCcnIyQkBNOmTWuw39TUVMybN8/62GAwQKvV2jMcIqJ2wa6gz8/PR1VVFYYOHWpdZzKZcOjQIaxYsQJGoxFqtdpmm+DgYHh4eNisj4yMRGVlJerq6uDp6Yn58+djwYIFePLJJwEAgwYNwrlz55Cent5o0Gs0Gmg0GnvKJyJql+wK+ri4OBQVFdmsmz59OgYMGICUlJRbQh4ARo0ahc2bN8NsNsPNzfKRQElJCYKDg+Hp6QkA+Pnnn63P3aBWq2E2m+0aDBER3cquD2N9fX0RFRVls/j4+CAgIABRUVEAgKlTpyI1NdW6zcyZM1FdXY2kpCSUlJRg165dSEtLg06ns7Z59NFHsWTJEuzatQtnz57F9u3b8e6772Ly5MmtNMxWNnMmUFGhdBVERM3i8Hn0jSkvL7c5Otdqtdi7dy/mzp2L6OhohIaGIikpCSkpKdY27733Hl577TXMmjULVVVVCAkJwfPPP4/XX3+9tctrHWfOANevK10FEVGzqIQkXz81GAzw9/eHXq+Hn5+f0uUQEbUpezKP17ohIpIcg56ISHIMeiIiyTHoiYgkx6AnIpIcg56ISHIMeiIiyTHoiYgkx6AnIpIcg56ISHIMeiIiyTHoiYgkx6AnIpIcg56ISHIMeiIiyTHoiYgkx6AnIpIcg56ISHIMeiIiyTHoiYgkx6AnIpIcg56ISHIMeiIiyTHoiYgkx6AnIpIcg56ISHIMeiIiyTHoiYgkx6AnIpIcg56ISHIMeiIiybUo6DMyMqBSqZCcnNxku5qaGuh0OgQHB0Oj0SAiIgI5OTk2bc6fP4+nnnoKAQEB8Pb2xqBBg3Ds2LGWlEdERADcHd0wLy8Pq1evRnR0dJPt6urq8MADDyAwMBDbtm1DaGgozp07h06dOlnbXLp0CaNGjcL999+P3bt3o1u3bigtLUXnzp0dLY+IiH7hUNDX1tYiMTERmZmZWLx4cZNt161bh+rqahw+fBgeHh4AgPDwcJs2b731FrRaLdavX29d16tXL0dKIyKimzg0daPT6ZCQkID4+Pjbts3OzkZsbCx0Oh2CgoIQFRWFtLQ0mEwmmzbDhg3D448/jsDAQAwZMgSZmZlN9ms0GmEwGGwWIiK6ld1Bn5WVhYKCAqSnpzer/enTp7Ft2zaYTCbk5OTgtddewzvvvGPzl8Dp06exatUq9OvXD3v37sXMmTMxZ84cfPDBB432m56eDn9/f+ui1WrtHQo5iz17gLo6pasgkpewQ3l5uQgMDBRff/21dd3YsWNFUlJSo9v069dPaLVacf36deu6d955R3Tv3t362MPDQ8TGxtpsN3v2bHHvvfc22u+1a9eEXq+3LhUVFQKA0Ov19gyJlHb2rBBubkKEhwuxdq0Q9fVKV0TkEvR6fbMzz64j+vz8fFRVVWHo0KFwd3eHu7s7Dh48iOXLl8Pd3d1mOuaG4OBgREREQK1WW9dFRkaisrISdb8cxQUHB2PgwIE220VGRqK8vLzRWjQaDfz8/GwWckFhYcDx40BMDPDMM0BkJLBxI3DmjNKVEUnDrqCPi4tDUVERCgsLrcuwYcOQmJiIwsJCmzC/YdSoUSgrK4PZbLauKykpQXBwMDw9Pa1tiouLbbYrKSlBWFiYI2MiV3PXXcC2bUBBgSXod+4EevcG1q5VujIiObT0z4ebp27+67/+SyxYsMD6uLy8XPj6+ooXX3xRFBcXi08++UQEBgaKxYsXW9scPXpUuLu7iyVLlojS0lKxadMm0aFDB7Fx48Zm12HPnzHk5C5dEmLnTiEqKpSuhMhp2ZN5Dp9H35jy8nK4uf3nDwWtVou9e/di7ty5iI6ORmhoKJKSkpCSkmJtc88992D79u1ITU3FG2+8gV69emHZsmVITExs7fLIFXTqBEyYoHQVRNJQCSGE0kW0BoPBAH9/f+j1es7XE5H07Mk8XuuGiEhyDHoiIskx6ImIJMegJ1KC2Qzwsh10hzDoiZSwfDkwaBBw8KDSlVA7wKAnUsKkSUB4OHD//cD8+cC1a0pXRBJj0BMpITwcOHAAePtty9H98OGWS0EQtQEGPZFS1GrgT38C8vIAlQoYNswS/A1cM4qoJRj0REqLjgaOHgXmzgUWLLBM55w9q3RVJBEGPZEz0GiAt94CPvsMKC8HDh1SuiKSSKtf64aIWmDMGODkSaBDB6UrIYkw6ImcjY+P0hWQZDh1Q9TaLl2yXEu/qkrpSogAMOiJWp+PD7B0KfC3v7X9a337bdu/Brk8Bj3Rzc6dAzIzHd/e0xPw9wdKS1uvpoaYzcCPP7bta5AUGPREv7ZjB/D888A77wBXrjjeT0RE258i6eYG3Hdf274GSYFBTwQAFRWWyxJMnmz5ItPhwy37UHTKFOCTT4B9+4CrV1utTCJHMOiJAOD774Fjx4CtWy0B3aVLy/rr0sVyxD1pEnD9equUSOQonl5JBACxscB331m+uNQaPvgAWL3a8peCr2/r9EnkIB7RE93QWiEPACUlgLs7EBDQen0SOYhBT9QWzp4FwsKUroIIAIOeqPUZjcD585ajeiInwKAnam1mMzB6NLB3L28XSE6BQU/U2ry9gS1bLHeNWrJE6WqIGPREbaJHD8udozIzgYULLUf5RArh6ZVEbWXGDOCnn4BXXwUKCoD/9/+ATp2UroraIR7RE7UVlQp4+WVg1y7g888t94X95hulq6J2iEFP1NYefthyX1iNBhgxAvjoI6UronaGQU90J/TtCxw5Ygn9xx6zTOfwJuB0h3COnuhO6dgR+PvfgWHDgNRUy7z9pk1A585KV0aS4xE90Z2kUgEvvQTs3m35QlV1NfDvf1t+ARC1kRYFfUZGBlQqFZKTk5tsV1NTA51Oh+DgYGg0GkRERCAnJ6dFfRK5tAcftNwdqk8fYONG4MkngfnzOZ1DbcLhqZu8vDysXr0a0dHRTbarq6vDAw88gMDAQGzbtg2hoaE4d+4cOjVwmllz+ySSgvsvu9+cOZZ/580DvvoKSEuznKFD1EocOqKvra1FYmIiMjMz0fk284vr1q1DdXU1duzYgVGjRiE8PBxjx47F4MGDHe6TSCoqFZCUBOTkAF98YTnCJ2pFDgW9TqdDQkIC4uPjb9s2OzsbsbGx0Ol0CAoKQlRUFNLS0mC66U9Ue/oEAKPRCIPBYLMQubTx4y3Xr//rX5WuhCRj99RNVlYWCgoKkJeX16z2p0+fxoEDB5CYmIicnByUlZVh1qxZqK+vx8KFCx3qEwDS09OxaNEie8sncm5du9o+PnoUGDIE8PBQph6Sgl1H9BUVFUhKSsKmTZvg5eXVrG3MZjMCAwOxZs0axMTE4IknnsArr7yC999/3+E+ASA1NRV6vd66VFRU2DMUItdw7RpQV6d0FeTiVEII0dzGO3bswOTJk6FWq63rTCYTVCoV3NzcYDQabZ4DgLFjx8LDwwP/+7//a123e/duPPLIIzAajcjJybG7z4YYDAb4+/tDr9fDz8+vuUMiInJJ9mSeXVM3cXFxKCoqslk3ffp0DBgwACkpKQ0G8qhRo7B582aYzWa4uVn+gCgpKUFwcDA8PT0d6pOIiJrPrqD39fVFVFSUzTofHx8EBARY10+dOhWhoaFIT08HAMycORMrVqxAUlISZs+ejdLSUqSlpWHOL6eUNadPIiJyXKtfAqG8vNx65A4AWq0We/fuxdy5cxEdHY3Q0FAkJSUhJSWltV+aiIgaYNccvTPjHD0RtSf2ZB6vdUNEJDkGPRGR5Bj0RESSY9ATEUmOQU9EJDkGPRGR5Bj0RESSY9ATEUmOQU9EJDkGPRGR5Bj0RESSY9ATEUmOQU9EJDkGPRGR5Bj0RESSY9ATEUmu1e8wReQ0jh4FCguBixeBykrLv6mpQEyM0pUR3VEMenJuV64AXl6AIzeJX78eWLMG6NYNCAoCuncH6upav0YiJ8epG2pbp08DK1c2/vyyZU1vf+ECUF3t2Gu/844l2Csrga+/BvbuBWJjHeuLyIXxiJ7altFomTJpjI9P09uHhwOO3ta4QwfHtiOSDG8OTkTkgnhzcCIismLQExFJjkFPRCQ5Bj0RkeQY9EREkmPQExFJjkFPRCQ5Bj0RkeQY9EREkmPQExFJjkFPRCS5FgV9RkYGVCoVkpOTm2xXU1MDnU6H4OBgaDQaREREICcnx/p8eno67rnnHvj6+iIwMBCTJk1CcXFxS0ojIqJfOBz0eXl5WL16NaKjo5tsV1dXhwceeABnz57Ftm3bUFxcjMzMTISGhlrbHDx4EDqdDl9++SVyc3NRX1+PBx98EFeuXHG0PKI77/vvgR07eM17cjoOXaa4trYWiYmJyMzMxOLFi5tsu27dOlRXV+Pw4cPw8PAAAISHh9u02bNnj83jDRs2IDAwEPn5+RgzZowjJRLdeT16WBYiJ+PQEb1Op0NCQgLi4+Nv2zY7OxuxsbHQ6XQICgpCVFQU0tLSYDKZGt1Gr9cDALp06dJoG6PRCIPBYLMQEdGt7D6iz8rKQkFBAfLy8prV/vTp0zhw4AASExORk5ODsrIyzJo1C/X19Vi4cOEt7c1mM5KTkzFq1ChERUU12m96ejoWLVpkb/lERO2OXUFfUVGBpKQk5ObmwsvLq1nbmM1mBAYGYs2aNVCr1YiJicH58+exdOnSBoNep9PhxIkT+Pzzz5vsNzU1FfPmzbM+NhgM0Gq19gyHiKhdsCvo8/PzUVVVhaFDh1rXmUwmHDp0CCtWrIDRaIT6pps4BwcHw8PDw2Z9ZGQkKisrUVdXB09PT+v6F198EZ988gkOHTqEHreZ69RoNNBoNPaUT0TULtkV9HFxcSgqKrJZN336dAwYMAApKSm3hDwAjBo1Cps3b4bZbIabm+UjgZKSEgQHB1tDXgiB2bNnY/v27fjss8/Qq1cvR8dDREQ3sSvofX19b5k39/HxQUBAgHX91KlTERoaivT0dADAzJkzsWLFCiQlJWH27NkoLS1FWloa5syZY+1Dp9Nh8+bN2LlzJ3x9fVFZWQkA8Pf3h7e3d4sGSETU3jl0emVTysvLrUfuAKDVarF3717MnTsX0dHRCA0NRVJSElJSUqxtVq1aBQAYN26cTV/r16/HH/7wh9YukYioXVEJIYTSRbQGe+6ITkTk6uzJPF7rhohIcgx6IiLJMeiJiCTHoCcikhyDnohIcgx6IiLJMeiJiCTHoCeixsnxNZt2j0FPRI1bswY4cULpKqiFGPRE1LjYWKCJ+0KQa2DQE7WFL74Ahg4FamuVrqRlbnNPaHINDHqithASAnz1FbBrl9KVEDHoidpEr15ATAywdavSlRAx6InazIwZQM+eSldB1PrXoyeiXzz/vNIVEAHgET0RkfQY9EREkmPQExFJjkFPRCQ5Bj0RkeQY9EREkmPQExFJjkFPRCQ5Bj0RtUx5OfDUU8B33yldCTWC34wlopbp2RPYuFHpKqgJPKInIpIcg56ISHKcuiFqa3v3Au7uQFyc0pW0rdOngbNngW7dgA4dALUacHMD/PyATp2Urq5d4xE9UVsSAnjvPeC3vwWKi5Wupm1lZVl+mT31FNC3r+Wa/GFhwJ//rHRl7Z5KCDlu824wGODv7w+9Xg8/Pz+lyyH6D70eGDkSqKsDvvwSCAhQuqLmM5stR+XN8fPPwIULQHU1cOUKYDJZlp49gcjItq2zHbIn8zh1Q9TW/P2Bjz8GRoywHNnv3Qt4eipd1e1t3Wo5Gt+/H+je/fbtO3SwHMmT02nR1E1GRgZUKhWSk5ObbFdTUwOdTofg4GBoNBpEREQgJyfHps3KlSsRHh4OLy8vjBgxAkePHm1JaUTOpXdv4KOPLDcNnzXLMqXjjK5dAw4dAmpqAJ3OMvWiUildFbWQw0Gfl5eH1atXI/o2d4mvq6vDAw88gLNnz2Lbtm0oLi5GZmYmQkNDrW3+/ve/Y968eVi4cCEKCgowePBgjB8/HlVVVY6WR+R87rsPyMwE1q4F3n1X6Wr+w2CwzK//3/9r+SB17FjLFMyPPwLPPAMEBSldIbWUcMDly5dFv379RG5urhg7dqxISkpqtO2qVatE7969RV1dXaNthg8fLnQ6nfWxyWQSISEhIj09vdk16fV6AUDo9fpmb0OkiJQUIVQqIbKzlavh6lUhMjOFeOQRITw9hQCEGDZMiCVLhPjmGyF+/NGybvt25WqkJtmTeQ4d0et0OiQkJCA+Pv62bbOzsxEbGwudToegoCBERUUhLS0NJpMJgOWIPz8/36YvNzc3xMfH48iRI432azQaYTAYbBYil5CWBkycCPz+98DJk8rUoFYDCxZYPjRduhQ4dw7IywNeftnywanZbGnX3A9iCaivV7qCRtn9YWxWVhYKCgqQl5fXrPanT5/GgQMHkJiYiJycHJSVlWHWrFmor6/HwoUL8dNPP8FkMiHopj8Pg4KC8O233zbab3p6OhYtWmRv+UTKc3OzXDJg0SLLHLgSPDws16jp0KHh5298hsD5+eZbuRL46ivggw+UruQWdgV9RUUFkpKSkJubCy8vr2ZtYzabERgYiDVr1kCtViMmJgbnz5/H0qVLsXDhQoeKBoDU1FTMmzfP+thgMECr1TrcH9Ed5eMDvP22sjU0FvIAj+gdcZuTUpRkV9Dn5+ejqqoKQ4cOta4zmUw4dOgQVqxYAaPRCLVabbNNcHAwPDw8bNZHRkaisrISdXV16Nq1K9RqNS5evGiz3cWLF9G9iVO6NBoNNBqNPeUTUXPdOKJn0EvBrncxLi4ORUVFKCwstC7Dhg1DYmIiCgsLbwl5ABg1ahTKyspgvnGEAKCkpATBwcHw9PSEp6cnYmJisH//fuvzZrMZ+/fvR2xsbAuGRkQOu7G/cupGCnYd0fv6+iIqKspmnY+PDwICAqzrp06ditDQUKSnpwMAZs6ciRUrViApKQmzZ89GaWkp0tLSMGfOHGsf8+bNw7Rp0zBs2DAMHz4cy5Ytw5UrVzB9+vSWjo+IHOHtDfzud0BwsNKVUCto9W/GlpeXw+1Xf+5ptVrs3bsXc+fORXR0NEJDQ5GUlISUlBRrmyeeeAI//vgjXn/9dVRWVuLuu+/Gnj17bvmAlojukIAAYPNmpaugVsJr3RARuSB7Mo+ftBDZ49tvLd8YJXIhDHoie3z6qSXsiVwIr15JZI/nnuOZKORyGPRE9mjgFGIiZ8epGyIiyTHoiYgkx6AnOZ0+bblDEhEx6ElSGzcCM2YAV68qXQmR4vhhLMnp1VeB0aMtX+Unaud4RE9ycnMDfvMbpasgcgoMeiIiyTHoiYgkx6AnIpIcg56ISHIMeiIiyTHoiYgkx6AnxyQnA198oXQVRNQMDHpyzPLlwMmTSldBRM3AoCf7mc2AEIA7v1hN5AoY9GQ/kwno2xfo1EnpSoioGXhIRvbz8ABKS5WugoiaiUf0RESSY9ATEUmOQU9EJDkGPRFRS2zdCqxcqXQVTWLQExG1xO7dwObNSlfRJAY92U8IYNcu4Pvvla6EiJqBQU/2u34d+D//B9i/X+lKiJQnhNIV3BaDnuxnMln+5TdjiVwC91Syn0YDnD8P+PsrXQmR8saMASIilK6iSQx6sp9KBYSEKF0FkXOYPl3pCm6LUzdERJJrUdBnZGRApVIhOTm50TYbNmyASqWyWby8vGza1NbW4sUXX0SPHj3g7e2NgQMH4v33329JaURE9AuHp27y8vKwevVqREdH37atn58fiouLrY9VKpXN8/PmzcOBAwewceNGhIeHY9++fZg1axZCQkIwYcIER0skIiI4eERfW1uLxMREZGZmonPnzrdtr1Kp0L17d+sSFBRk8/zhw4cxbdo0jBs3DuHh4XjuuecwePBgHD161JHyiIjoVxwKep1Oh4SEBMTHxzerfW1tLcLCwqDVajFx4kScvOnORCNHjkR2djbOnz8PIQQ+/fRTlJSU4MEHH2y0T6PRCIPBYLMQEdGt7A76rKwsFBQUID09vVnt+/fvj3Xr1mHnzp3YuHEjzGYzRo4cie9/9a3K9957DwMHDkSPHj3g6emJhx56CCtXrsSYMWMa7Tc9PR3+/v7WRavV2jsUIqJ2wa45+oqKCiQlJSE3N/eWD1QbExsbi9jYWOvjkSNHIjIyEqtXr8abb74JwBL0X375JbKzsxEWFoZDhw5Bp9MhJCSk0b8aUlNTMW/ePOtjg8HAsCciaoBKiOZ/f3fHjh2YPHky1Gq1dZ3JZIJKpYKbmxuMRqPNc415/PHH4e7uji1btuDq1avw9/fH9u3bkZCQYG3z7LPP4vvvv8eePXuaVZvBYIC/vz/0ej38/PyaOyQiIpdkT+bZdUQfFxeHoqIim3XTp0/HgAEDkJKS0qyQN5lMKCoqwiOPPAIAqK+vR319PdzcbGeR1Go1zGazPeUREVED7Ap6X19fREVF2azz8fFBQECAdf3UqVMRGhpqncN/4403cO+996Jv376oqanB0qVLce7cOTz77LMALKdejh07FvPnz4e3tzfCwsJw8OBBfPjhh3j33XdbY4xERO1aq18Coby83Obo/NKlS5gxYwYqKyvRuXNnxMTE4PDhwxg4cKC1TVZWFlJTU5GYmIjq6mqEhYVhyZIleOGFF1q7PCKidseuOXpnxjl6ImpP7Mk8XuuGiEhyDHoiIskx6ImIJMegJyKSHIOeiEhyDHoiIskx6ImIJMegJyKSHIOeiEhyDHpyXdeuAWfOKF0FkdNj0JPrSksDhgwBeHcxoiYx6Mk1XboE7NsHzJ0L8NpG5CzOnQOeegrIz1e6EhutfvVKojti82bg2DFgxw6lKyH6j/p64KefgJ9/VroSGwx6cj1CAJmZwKOPAt27K10N0X/07Qs08654dxKnbsj1FBQAX38N/HLzGiJqGoOeXM/f/gaEhgLjxytdCZFLYNCTa7l2zTI/P3064M6ZR6Lm4J5CrqWkBDh4EAgKUroSIpfBI3pyHUePAsOHA19+CQQHK10Nkctg0JNruHABmDQJGDrUMm1DRM3GoCfnd+0aMHky4OYGfPQRoNEoXZHzMRqBf/8bMJuVroScEIOenJsQwPPPA8ePW74cxfPmG7ZnD9C1q+XLOkQ34Yex5Nz++7+BDz8ENm0Chg1TuhrnVV9v+dfDQ9k6yCnxiJ6c1969wPz5wEsvAb//vdLVOLcbQe/pqWwd5JQY9OScSkuBJ58EHnrIcpVKalpdneVfHtFTAxj05Hz0emDCBMu58ps3A2q10hU5P07dUBM4R0/OxWQCEhOBH36wnDfv7690Ra6hvt7yTWGVSulKyAkx6Mm5vPoqsHs3sGsXEBGhdDWuo76eR/PUKAY9OY8tW4CMDGDpUsvcPDVfXR2DnhrFOXpyDvn5wNNPW+7O88c/Kl2N6+ERPTWBQU/Kq6y0XN5g0CBgzRrOMzuivp6nVlKjWhT0GRkZUKlUSE5ObrTNhg0boFKpbBYvL69b2p06dQoTJkyAv78/fHx8cM8996C8vLwl5ZErMBqBxx6zfAi7fTvg7a10Ra6JR/TUBIfn6PPy8rB69WpER0fftq2fnx+Ki4utj1U3HbF99913GD16NJ555hksWrQIfn5+OHnyZIO/EEgiQgA6neXerwcPWm4mQo5h0FMTHAr62tpaJCYmIjMzE4sXL75te5VKhe5NXKPklVdewSOPPIK3337buq5Pnz6OlEauZM0aYO1aYMMG4N57la7GtfHDWGqCQ1M3Op0OCQkJiI+Pb1b72tpahIWFQavVYuLEiTh58qT1ObPZjF27diEiIgLjx49HYGAgRowYgR07djTZp9FohMFgsFnIxTz8MLBsGTBtmtKVuD7O0VMT7A76rKwsFBQUID09vVnt+/fvj3Xr1mHnzp3YuHEjzGYzRo4cie+//x4AUFVVhdraWmRkZOChhx7Cvn37MHnyZEyZMgUHDx5stN/09HT4+/tbF61Wa+9QSGk9ewJJSUpXIQdO3VATVEII0dzGFRUVGDZsGHJzc61z8+PGjcPdd9+NZcuWNauP+vp6REZG4ne/+x3efPNNXLhwAaGhofjd736HzZs3W9tNmDABPj4+2LJlS4P9GI1GGI1G62ODwQCtVgu9Xg8/P7/mDolIDs88A3zzDXDkiNKV0B1iMBjg7+/frMyza44+Pz8fVVVVGDp0qHWdyWTCoUOHsGLFChiNRqhvc10SDw8PDBkyBGVlZQCArl27wt3dHQMHDrRpFxkZic8//7zRfjQaDTS8AQWRRV0dp26oUXYFfVxcHIqKimzWTZ8+HQMGDEBKSsptQx6w/GIoKirCI488AgDw9PTEPffcY3NWDgCUlJQgLCzMnvKI2i9O3VAT7Ap6X19fREVF2azz8fFBQECAdf3UqVMRGhpqncN/4403cO+996Jv376oqanB0qVLce7cOTz77LPWPubPn48nnngCY8aMwf333489e/bg448/xmeffdbC4RG1E088AVy9qnQV5KRa/Vo35eXlcHP7z2e8ly5dwowZM1BZWYnOnTsjJiYGhw8ftpmqmTx5Mt5//32kp6djzpw56N+/P/75z39i9OjRrV0ekZwmT1a6AnJidn0Y68zs+WCCiMjV2ZN5vNYNEZHkGPRERJJj0BMRSY5BT0QkOQY9EZHkGPRERJJj0BMRSY5BT0QkOQY9EZHkGPRERJJj0BMRSY5BT0QkOQY9EZHkGPRERJJj0BMRSY5BT0QkOQY9EZHkGPRERJJj0BMRSY5BT0QkOQY9EZHkGPRERJJj0BMRSY5BT0QkOQY9EZHkGPRERJJj0BMRSc5d6QJaixACAGAwGBSuhIio7d3IuhvZ1xRpgv7y5csAAK1Wq3AlRER3zuXLl+Hv799kG5Vozq8DF2A2m3HhwgX4+vpCpVK12esYDAZotVpUVFTAz8+vzV7nTuKYXAPH5Bru1JiEELh8+TJCQkLg5tb0LLw0R/Rubm7o0aPHHXs9Pz8/aX4wb+CYXAPH5BruxJhudyR/Az+MJSKSHIOeiEhyDHo7aTQaLFy4EBqNRulSWg3H5Bo4JtfgjGOS5sNYIiJqGI/oiYgkx6AnIpIcg56ISHIMeiIiyUkV9KtWrUJ0dLT1iwqxsbHYvXu39flx48ZBpVLZLC+88IJNH+Xl5UhISECHDh0QGBiI+fPn4/r16zZtPvvsMwwdOhQajQZ9+/bFhg0bbqll5cqVCA8Ph5eXF0aMGIGjR4/aPH/t2jXodDoEBASgY8eOeOyxx3Dx4kW7x3X27NlbxnRj2bp1q7WPhp7PyspSdFw3ZGRkQKVSITk52a5+nPG9amxM1dXVmD17Nvr37w9vb2/07NkTc+bMgV6vt9nO1d4nV92nGhuTDPtTg4REsrOzxa5du0RJSYkoLi4WL7/8svDw8BAnTpwQQggxduxYMWPGDPHDDz9YF71eb93++vXrIioqSsTHx4uvvvpK5OTkiK5du4rU1FRrm9OnT4sOHTqIefPmiW+++Ua89957Qq1Wiz179ljbZGVlCU9PT7Fu3Tpx8uRJMWPGDNGpUydx8eJFa5sXXnhBaLVasX//fnHs2DFx7733ipEjR9o9ruvXr9uM54cffhCLFi0SHTt2FJcvX7b2AUCsX7/ept3Vq1cVHZcQQhw9elSEh4eL6OhokZSU1Ox+nPW9amxMRUVFYsqUKSI7O1uUlZWJ/fv3i379+onHHnvMZltXe59cdZ9qbEyuvj81Rqqgb0jnzp3F3/72NyGE5Yfy1z+kN8vJyRFubm6isrLSum7VqlXCz89PGI1GIYQQL730krjrrrtstnviiSfE+PHjrY+HDx8udDqd9bHJZBIhISEiPT1dCCFETU2N8PDwEFu3brW2OXXqlAAgjhw5Yve4bnb33XeLp59+2mYdALF9+/ZG+1NiXJcvXxb9+vUTubm5Nu9Nc/px1veqsTE15B//+Ifw9PQU9fX11nWu9D4J4br7lD3vk6vsT02Raurm10wmE7KysnDlyhXExsZa12/atAldu3ZFVFQUUlNT8fPPP1ufO3LkCAYNGoSgoCDruvHjx8NgMODkyZPWNvHx8TavNX78eBw5cgQAUFdXh/z8fJs2bm5uiI+Pt7bJz89HfX29TZsBAwagZ8+e1jb2juuG/Px8FBYW4plnnrnlOZ1Oh65du2L48OFYt26dzeVNlRiXTqdDQkLCLa/bnH6c9b1qbEwN0ev18PPzg7u77SWnXOV9usEV96nmvk+utD81RZqLmt1QVFSE2NhYXLt2DR07dsT27dsxcOBAAMDvf/97hIWFISQkBMePH0dKSgqKi4vx0UcfAQAqKyttfiABWB9XVlY22cZgMODq1au4dOkSTCZTg22+/fZbax+enp7o1KnTLW1uvI494/q1tWvXIjIyEiNHjrRZ/8Ybb+A3v/kNOnTogH379mHWrFmora3FnDlzFBlXVlYWCgoKkJeXd8sYmtOPM75XTY3pZj/99BPefPNNPPfcczbrXel9Alxzn7LnfXKV/el2pAv6/v37o7CwEHq9Htu2bcO0adNw8OBBDBw40GanGjRoEIKDgxEXF4fvvvsOffr0UbDq22tqXDdcvXoVmzdvxmuvvXbL9r9eN2TIEFy5cgVLly61/mDeSRUVFUhKSkJubi68vLzu+Ou3BXvGZDAYkJCQgIEDB+LPf/6zzXOu9j652j5lz/vkKvtTc0g3dePp6Ym+ffsiJiYG6enpGDx4MP7617822HbEiBEAgLKyMgBA9+7db/lE+8bj7t27N9nGz88P3t7e6Nq1K9RqdYNtft1HXV0dampqGm3jyLi2bduGn3/+GVOnTm2wj5vH/v3338NoNN7xceXn56OqqgpDhw6Fu7s73N3dcfDgQSxfvhzu7u4ICgq6bT/O9l7dbkwmkwmA5SYRDz30EHx9fbF9+3Z4eHigKc78Pt0Y0831As67T9kzJlfZn5pDuqC/mdlstv7Pv1lhYSEAIDg4GAAQGxuLoqIiVFVVWdvk5ubCz8/PeuQcGxuL/fv32/STm5trnS/39PRETEyMTRuz2Yz9+/db28TExMDDw8OmTXFxMcrLyxucd2/uuNauXYsJEyagW7dut92+sLAQnTt3tl546U6OKy4uDkVFRSgsLLQuw4YNQ2JiovW/b9ePs71XtxuTWq2GwWDAgw8+CE9PT2RnZzfrrxlnfp/UanWD9QLOu0/ZMyZX2Z+axa6Pbp3cggULxMGDB8WZM2fE8ePHxYIFC4RKpRL79u0TZWVl4o033hDHjh0TZ86cETt37hS9e/cWY8aMsW5/41SwBx98UBQWFoo9e/aIbt26NXgq2Pz588WpU6fEypUrGzxtSqPRiA0bNohvvvlGPPfcc6JTp042Zx688MILomfPnuLAgQPi2LFjIjY2VsTGxto9rhtKS0uFSqUSu3fvvmX77OxskZmZKYqKikRpaan4n//5H9GhQwfx+uuvKzquX7v5zIfb9eOs71VjY9Lr9WLEiBFi0KBBoqyszOa0vOvXr7vk++TK+1RjY7rB1fenm0kV9E8//bQICwsTnp6eolu3biIuLs4ahuXl5WLMmDGiS5cuQqPRiL59+4r58+fbnPMrhBBnz54VDz/8sPD29hZdu3YVf/zjH21OfxNCiE8//VTcfffdwtPTU/Tu3VusX7/+llree+890bNnT+Hp6SmGDx8uvvzyS5vnr169KmbNmiU6d+4sOnToICZPnix++OEHu8d1Q2pqqtBqtcJkMt2y/e7du8Xdd98tOnbsKHx8fMTgwYPF+++/f0vbOz2uX7t5Z2tOP874XjU2pk8//VQAaHA5c+aMEML13idX3qcaG9MNrr4/3YyXKSYikpz0c/RERO0dg56ISHIMeiIiyTHoiYgkx6AnIpIcg56ISHIMeiIiyTHoiYgkx6AnIpIcg56ISHIMeiIiyTHoiYgk9/8Bn1XkMPar7AgAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize the dataset\n",
    "from matplotlib import collections  as mc\n",
    "import pylab as pl\n",
    "\n",
    "viz_subset = df1_unique.sample(n=50)\n",
    "srcs  = zip(viz_subset['A X_COORD'], viz_subset['A Y_COORD'])\n",
    "dests  = zip(viz_subset['B X_COORD'], viz_subset['B Y_COORD'])\n",
    "lines = [list(a) for a in zip(srcs, dests)]\n",
    "\n",
    "# Taken from https://stackoverflow.com/questions/21352580/plotting-numerous-disconnected-line-segments-with-different-colors\n",
    "linecollection = mc.LineCollection(lines, colors=(1,0,0,1), linewidths=1)\n",
    "fig, ax = pl.subplots()\n",
    "ax.add_collection(linecollection)\n",
    "ax.autoscale()\n",
    "ax.set_aspect('equal', 'box')\n",
    "ax.margins(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "cf225d72-2255-40a0-aee9-a2fce31a04bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "edges_augmented\n",
      "     capacity  # of lanes  src_x_coord  src_y_coord  dest_x_coord  \\\n",
      "0      4000.0         3.5     407624.2    4649654.5      408005.8   \n",
      "1      4000.0         3.5     409288.6    4649934.5      408004.0   \n",
      "2      4000.0         3.5     408444.0    4649861.0      409327.9   \n",
      "3      4000.0         3.5     409327.9    4649862.0      410370.2   \n",
      "4      4000.0         3.5     410362.6    4649530.5      409288.6   \n",
      "..        ...         ...          ...          ...           ...   \n",
      "563    1660.0         3.5     418256.4    4648479.6      418818.6   \n",
      "564    2520.0         3.5     418818.6    4648483.5      419087.3   \n",
      "565    2160.0         3.5     419087.3    4648470.0      419615.7   \n",
      "566    1440.0         3.5     419615.7    4648447.5      420538.3   \n",
      "567    1620.0         3.5     420538.3    4648300.5      422144.6   \n",
      "\n",
      "     dest_y_coord  \n",
      "0       4649863.0  \n",
      "1       4649903.5  \n",
      "2       4649862.0  \n",
      "3       4649433.0  \n",
      "4       4649934.5  \n",
      "..            ...  \n",
      "563     4648483.5  \n",
      "564     4648470.0  \n",
      "565     4648447.5  \n",
      "566     4648300.5  \n",
      "567     4647647.5  \n",
      "\n",
      "[568 rows x 6 columns]\n",
      "\n",
      "nodes_tensor\n",
      "tensor([[4.0000e+03, 3.5000e+00, 4.0762e+05, 4.6497e+06, 4.0801e+05, 4.6499e+06],\n",
      "        [4.0000e+03, 3.5000e+00, 4.0929e+05, 4.6499e+06, 4.0800e+05, 4.6499e+06],\n",
      "        [4.0000e+03, 3.5000e+00, 4.0844e+05, 4.6499e+06, 4.0933e+05, 4.6499e+06],\n",
      "        ...,\n",
      "        [2.1600e+03, 3.5000e+00, 4.1909e+05, 4.6485e+06, 4.1962e+05, 4.6484e+06],\n",
      "        [1.4400e+03, 3.5000e+00, 4.1962e+05, 4.6484e+06, 4.2054e+05, 4.6483e+06],\n",
      "        [1.6200e+03, 3.5000e+00, 4.2054e+05, 4.6483e+06, 4.2214e+05, 4.6476e+06]])\n",
      "\n",
      "edges_tensor\n",
      "tensor([[  0,   1,   2,  ..., 395, 496, 497],\n",
      "        [  0,   1,   2,  ..., 396, 501, 500]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from itertools import chain, combinations\n",
    "\n",
    "# This is the target of the prediction (\"ground truth\")\n",
    "y_tensor = torch.tensor(edges['AADT(2010)-A'].values)\n",
    "y_tensor = y_tensor.to(torch.float32)\n",
    "\n",
    "\"\"\"# Build a map from node_id to index (basically enumerate the n unique node_ids from 0 to n-1)\n",
    "unique_node_ids = nodes['node_id'].unique()\n",
    "node_id_to_idx  = dict(zip(unique_node_ids, range(len(unique_node_ids))))\n",
    "\n",
    "nodes_tensor = torch.tensor(nodes[['x_coord','y_coord']].values)\n",
    "# print(nodes_tensor)\n",
    "\n",
    "edges_tensor = torch.tensor(edges[['src', 'dest']].apply(lambda x: [node_id_to_idx[elem] for elem in x]).values).T\n",
    "edge_features_tensor = torch.tensor(edges[['capacity','AADT(2010)-B']].values).squeeze(1)\n",
    "# print(edges_tensor)\n",
    "# print(edge_features_tensor)\"\"\"\n",
    "\n",
    "# The above code should be used eventually, but for now, I'm \"inverting\" the problem formulation to make it easier to plug into\n",
    "# existing GNNs. Namely, I will be using the Line Graph (https://en.wikipedia.org/wiki/Line_graph) instead of the original graph\n",
    "\n",
    "# Build a map from edge_id to index (basically enumerate the n unique node_ids from 0 to n-1)\n",
    "unique_edge_ids = edges['edge_id'].unique()\n",
    "edge_id_to_idx  = dict(zip(unique_edge_ids, range(len(unique_edge_ids))))\n",
    "\n",
    "# Build a map from node_id to a list of edge_ids that are incident to that node\n",
    "# For each edge, create tuples (src, edge_id), (dest, edge_id)\n",
    "entries = zip(pd.concat([edges['src'],edges['dest']]), chain(range(len(unique_edge_ids)), range(len(unique_edge_ids))))\n",
    "node_to_edges = {}\n",
    "for node_id, edge_id in entries:\n",
    "    if node_id not in node_to_edges:\n",
    "        node_to_edges[node_id] = [edge_id]\n",
    "    else:\n",
    "        node_to_edges[node_id].append(edge_id)\n",
    "\n",
    "# Actually, we want to cram as much information as we can into each edge because these will be the features\n",
    "# Namely, we will put the x,y coordinates of src/dest into each edge as well\n",
    "edges_augmented = edges.merge(nodes, how='inner', left_on='src', right_on='node_id').rename(columns={\"x_coord\": \"src_x_coord\", \"y_coord\": \"src_y_coord\"})\n",
    "edges_augmented = edges_augmented.drop(columns='node_id')\n",
    "edges_augmented = edges_augmented.merge(nodes, how='inner', left_on='dest', right_on='node_id').rename(columns={\"x_coord\": \"dest_x_coord\", \"y_coord\": \"dest_y_coord\"})\n",
    "edges_augmented = edges_augmented.drop(columns=['edge_id','AADT(2010)-A','src','dest','node_id'])\n",
    "print(\"\\nedges_augmented\")\n",
    "print(edges_augmented)\n",
    "\n",
    "# Finalizing our input tensors\n",
    "nodes_tensor = torch.tensor(edges_augmented.values)\n",
    "nodes_tensor = nodes_tensor.to(torch.float32)\n",
    "print(\"\\nnodes_tensor\")\n",
    "print(nodes_tensor)\n",
    "\n",
    "# Edges in the original graph become nodes\n",
    "# First, every edge should be connected to itself\n",
    "edges_tensor = torch.cat((torch.unsqueeze(torch.arange(0, len(unique_edge_ids)),0),torch.unsqueeze(torch.arange(0, len(unique_edge_ids)),0)), 0)\n",
    "\n",
    "# Then for every node in the original graph, we generate (degree choose 2) edges in the line graph\n",
    "for node_id, edge_list in node_to_edges.items():\n",
    "    # We actually don't even need the node id here! just do all choose 2\n",
    "    if len(edge_list) == 0:\n",
    "        continue\n",
    "    for comb in combinations(edge_list, 2):\n",
    "        new_thing = torch.tensor([comb[0],comb[1]]).unsqueeze(1)\n",
    "        edges_tensor = torch.cat([edges_tensor, torch.tensor([comb[0],comb[1]]).unsqueeze(1)], 1)\n",
    "\n",
    "print(\"\\nedges_tensor\")\n",
    "print(edges_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c1388243-452b-445b-b1cc-b234d3f143bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple GCN\n",
    "# Taken from https://www.datacamp.com/tutorial/comprehensive-introduction-graph-neural-networks-gnns-tutorial\n",
    "# Eventually, this should be moved to the src/ directory\n",
    "\n",
    "from torch_geometric.nn import GCNConv\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(42)\n",
    "        # The first number (6) is num_features. todo: this shouldn't be hardcoded\n",
    "        self.conv1 = GCNConv(in_channels=6, out_channels=hidden_channels)\n",
    "        self.conv2 = GCNConv(in_channels=hidden_channels, out_channels=1)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "d9cfb3bb-5489-4ba7-8368-d27febf75f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Create train/val/test split\n",
    "num_datapoints = len(edges.index)\n",
    "shuffled_indices = list(range(num_datapoints))\n",
    "random.shuffle(shuffled_indices)\n",
    "train_frac = .7\n",
    "val_frac   = .15\n",
    "test_frac  = .15\n",
    "train_indices = shuffled_indices[:int(num_datapoints*train_frac)]\n",
    "val_indices   = shuffled_indices[int(num_datapoints*train_frac):int(num_datapoints*(train_frac+val_frac))]\n",
    "test_indices  = shuffled_indices[int(num_datapoints*(train_frac+val_frac)):]\n",
    "\n",
    "train_mask = torch.zeros(num_datapoints)\n",
    "val_mask   = torch.zeros(num_datapoints)\n",
    "test_mask  = torch.zeros(num_datapoints)\n",
    "\n",
    "train_mask[train_indices] = 1\n",
    "val_mask[val_indices]     = 1\n",
    "test_mask[test_indices]   = 1\n",
    "\n",
    "train_mask = train_mask.to(torch.bool)\n",
    "val_mask   = val_mask.to(torch.bool)\n",
    "test_mask  = test_mask.to(torch.bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1c14c2d8-ee5d-41aa-a994-1506304222b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Loss: 918859415552.0000, Val_loss: 598977085440.0000\n",
      "New lowest validation loss. Saved model\n",
      "Epoch: 002, Loss: 711258734592.0000, Val_loss: 413089824768.0000\n",
      "New lowest validation loss. Saved model\n",
      "Epoch: 003, Loss: 469169438720.0000, Val_loss: 268802506752.0000\n",
      "New lowest validation loss. Saved model\n",
      "Epoch: 004, Loss: 321676541952.0000, Val_loss: 161457963008.0000\n",
      "New lowest validation loss. Saved model\n",
      "Epoch: 005, Loss: 228040736768.0000, Val_loss: 85879013376.0000\n",
      "New lowest validation loss. Saved model\n",
      "Epoch: 006, Loss: 138395041792.0000, Val_loss: 38555160576.0000\n",
      "New lowest validation loss. Saved model\n",
      "Epoch: 007, Loss: 69672304640.0000, Val_loss: 17569536000.0000\n",
      "New lowest validation loss. Saved model\n",
      "Epoch: 008, Loss: 49266835456.0000, Val_loss: 9290528768.0000\n",
      "New lowest validation loss. Saved model\n",
      "Epoch: 009, Loss: 42880274432.0000, Val_loss: 5621113856.0000\n",
      "New lowest validation loss. Saved model\n",
      "Epoch: 010, Loss: 39819513856.0000, Val_loss: 5029455360.0000\n",
      "New lowest validation loss. Saved model\n",
      "Epoch: 011, Loss: 44710461440.0000, Val_loss: 6204887040.0000\n",
      "Epoch: 012, Loss: 42804936704.0000, Val_loss: 8084080128.0000\n",
      "Epoch: 013, Loss: 43914465280.0000, Val_loss: 9862533120.0000\n",
      "Epoch: 014, Loss: 44928487424.0000, Val_loss: 11008245760.0000\n",
      "Epoch: 015, Loss: 54460313600.0000, Val_loss: 11270982656.0000\n",
      "Epoch: 016, Loss: 48716951552.0000, Val_loss: 10802769920.0000\n",
      "Epoch: 017, Loss: 44010463232.0000, Val_loss: 8255551488.0000\n",
      "Epoch: 018, Loss: 32650364928.0000, Val_loss: 5976193024.0000\n",
      "Epoch: 019, Loss: 32894861312.0000, Val_loss: 5060712960.0000\n",
      "Epoch: 020, Loss: 30612164608.0000, Val_loss: 5010177536.0000\n",
      "New lowest validation loss. Saved model\n",
      "Epoch: 021, Loss: 26693486592.0000, Val_loss: 5002738176.0000\n",
      "New lowest validation loss. Saved model\n",
      "Epoch: 022, Loss: 21435758592.0000, Val_loss: 5008410624.0000\n",
      "Epoch: 023, Loss: 18915424256.0000, Val_loss: 5040611840.0000\n",
      "Epoch: 024, Loss: 16879975424.0000, Val_loss: 5200179200.0000\n",
      "Epoch: 025, Loss: 13726510080.0000, Val_loss: 5080912384.0000\n",
      "Epoch: 026, Loss: 13894772736.0000, Val_loss: 5022014464.0000\n",
      "Epoch: 027, Loss: 11986472960.0000, Val_loss: 5083576320.0000\n",
      "Epoch: 028, Loss: 10981834752.0000, Val_loss: 5186507264.0000\n",
      "Epoch: 029, Loss: 9533673472.0000, Val_loss: 5274133504.0000\n",
      "Epoch: 030, Loss: 10473292800.0000, Val_loss: 5284371456.0000\n",
      "Epoch: 031, Loss: 8994180096.0000, Val_loss: 5189144576.0000\n",
      "Epoch: 032, Loss: 8714776576.0000, Val_loss: 5103483904.0000\n",
      "Epoch: 033, Loss: 6466848768.0000, Val_loss: 5069575680.0000\n",
      "Epoch: 034, Loss: 9388867584.0000, Val_loss: 5156748800.0000\n",
      "Epoch: 035, Loss: 7843597312.0000, Val_loss: 5418824704.0000\n",
      "Epoch: 036, Loss: 7431470080.0000, Val_loss: 5692179456.0000\n",
      "Epoch: 037, Loss: 8149868544.0000, Val_loss: 5801176064.0000\n",
      "Epoch: 038, Loss: 8811703296.0000, Val_loss: 5720630784.0000\n",
      "Epoch: 039, Loss: 6983853568.0000, Val_loss: 5513867264.0000\n",
      "Epoch: 040, Loss: 7421794304.0000, Val_loss: 5320582144.0000\n",
      "Epoch: 041, Loss: 7792933888.0000, Val_loss: 5237487104.0000\n",
      "Epoch: 042, Loss: 7426370560.0000, Val_loss: 5232212480.0000\n",
      "Epoch: 043, Loss: 7860025344.0000, Val_loss: 5367606784.0000\n",
      "Epoch: 044, Loss: 8056574976.0000, Val_loss: 5622753792.0000\n",
      "Epoch: 045, Loss: 7228349440.0000, Val_loss: 5858336768.0000\n",
      "Epoch: 046, Loss: 7316274688.0000, Val_loss: 5962319872.0000\n",
      "Epoch: 047, Loss: 6821619712.0000, Val_loss: 5869152256.0000\n",
      "Epoch: 048, Loss: 7092257792.0000, Val_loss: 5716216320.0000\n",
      "Epoch: 049, Loss: 7020587008.0000, Val_loss: 5500509184.0000\n",
      "Epoch: 050, Loss: 6850045440.0000, Val_loss: 5335828480.0000\n",
      "Epoch: 051, Loss: 7599290368.0000, Val_loss: 5255661568.0000\n",
      "Epoch: 052, Loss: 7108305920.0000, Val_loss: 5229271040.0000\n",
      "Epoch: 053, Loss: 6921582080.0000, Val_loss: 5258363392.0000\n",
      "Epoch: 054, Loss: 7322835968.0000, Val_loss: 5395042816.0000\n",
      "Epoch: 055, Loss: 7119964672.0000, Val_loss: 5564408320.0000\n",
      "Epoch: 056, Loss: 7789488128.0000, Val_loss: 5756030464.0000\n",
      "Epoch: 057, Loss: 7295734272.0000, Val_loss: 5924542464.0000\n",
      "Epoch: 058, Loss: 7322709504.0000, Val_loss: 5927499264.0000\n",
      "Epoch: 059, Loss: 7280180224.0000, Val_loss: 5845993984.0000\n",
      "Epoch: 060, Loss: 7385793536.0000, Val_loss: 5684231168.0000\n",
      "Epoch: 061, Loss: 6391958016.0000, Val_loss: 5459373056.0000\n",
      "Epoch: 062, Loss: 7147314688.0000, Val_loss: 5349490688.0000\n",
      "Epoch: 063, Loss: 7759533568.0000, Val_loss: 5362415104.0000\n",
      "Epoch: 064, Loss: 5763065344.0000, Val_loss: 5145299456.0000\n",
      "Epoch: 065, Loss: 6363347968.0000, Val_loss: 5067042816.0000\n",
      "Epoch: 066, Loss: 6188440576.0000, Val_loss: 5072522752.0000\n",
      "Epoch: 067, Loss: 6773829120.0000, Val_loss: 5100167168.0000\n",
      "Epoch: 068, Loss: 6152996352.0000, Val_loss: 5278490112.0000\n",
      "Epoch: 069, Loss: 6034805760.0000, Val_loss: 5332501504.0000\n",
      "Epoch: 070, Loss: 5610929664.0000, Val_loss: 5266397184.0000\n",
      "Epoch: 071, Loss: 6145819648.0000, Val_loss: 5135890432.0000\n",
      "Epoch: 072, Loss: 5716163584.0000, Val_loss: 5089228800.0000\n",
      "Epoch: 073, Loss: 6033610752.0000, Val_loss: 5093101056.0000\n",
      "Epoch: 074, Loss: 6444696064.0000, Val_loss: 5115182080.0000\n",
      "Epoch: 075, Loss: 7556577280.0000, Val_loss: 5331426304.0000\n",
      "Epoch: 076, Loss: 5928314368.0000, Val_loss: 5587609088.0000\n",
      "Epoch: 077, Loss: 6268278784.0000, Val_loss: 5749053952.0000\n",
      "Epoch: 078, Loss: 6397264896.0000, Val_loss: 5675100672.0000\n",
      "Epoch: 079, Loss: 6671424000.0000, Val_loss: 5461903872.0000\n",
      "Epoch: 080, Loss: 6140244480.0000, Val_loss: 5247431680.0000\n",
      "Epoch: 081, Loss: 6264902144.0000, Val_loss: 5111057408.0000\n",
      "Epoch: 082, Loss: 5707294208.0000, Val_loss: 5097482752.0000\n",
      "Epoch: 083, Loss: 6009148928.0000, Val_loss: 5110206464.0000\n",
      "Epoch: 084, Loss: 5399466496.0000, Val_loss: 5105782784.0000\n",
      "Epoch: 085, Loss: 5740905472.0000, Val_loss: 5091103232.0000\n",
      "Epoch: 086, Loss: 5771788800.0000, Val_loss: 5136388096.0000\n",
      "Epoch: 087, Loss: 5863657984.0000, Val_loss: 5228447744.0000\n",
      "Epoch: 088, Loss: 5931958272.0000, Val_loss: 5265426944.0000\n",
      "Epoch: 089, Loss: 5842998272.0000, Val_loss: 5192923136.0000\n",
      "Epoch: 090, Loss: 5842920448.0000, Val_loss: 5098360320.0000\n",
      "Epoch: 091, Loss: 5580785664.0000, Val_loss: 5123759104.0000\n",
      "Epoch: 092, Loss: 5470146560.0000, Val_loss: 5233973248.0000\n",
      "Epoch: 093, Loss: 5970208256.0000, Val_loss: 5091996672.0000\n",
      "Epoch: 094, Loss: 6602413056.0000, Val_loss: 5347382784.0000\n",
      "Epoch: 095, Loss: 6030750720.0000, Val_loss: 5486062080.0000\n",
      "Epoch: 096, Loss: 6283878400.0000, Val_loss: 5362834944.0000\n",
      "Epoch: 097, Loss: 5635025408.0000, Val_loss: 5099242496.0000\n",
      "Epoch: 098, Loss: 5697001472.0000, Val_loss: 5292719104.0000\n",
      "Epoch: 099, Loss: 4956277248.0000, Val_loss: 5484728320.0000\n",
      "Epoch: 100, Loss: 6206562816.0000, Val_loss: 5111081984.0000\n",
      "Epoch: 101, Loss: 5913012736.0000, Val_loss: 5241249792.0000\n",
      "Epoch: 102, Loss: 5846305792.0000, Val_loss: 5500364800.0000\n",
      "Epoch: 103, Loss: 5628677632.0000, Val_loss: 5518168064.0000\n",
      "Epoch: 104, Loss: 5825339392.0000, Val_loss: 5285186048.0000\n",
      "Epoch: 105, Loss: 5811461120.0000, Val_loss: 5097938432.0000\n",
      "Epoch: 106, Loss: 4952550400.0000, Val_loss: 5154732032.0000\n",
      "Epoch: 107, Loss: 5515789312.0000, Val_loss: 5228978176.0000\n",
      "Epoch: 108, Loss: 6183032832.0000, Val_loss: 5106231296.0000\n",
      "Epoch: 109, Loss: 5250207232.0000, Val_loss: 5098770944.0000\n",
      "Epoch: 110, Loss: 5634342400.0000, Val_loss: 5209792512.0000\n",
      "Epoch: 111, Loss: 5869593088.0000, Val_loss: 5272872448.0000\n",
      "Epoch: 112, Loss: 5336197120.0000, Val_loss: 5182600192.0000\n",
      "Epoch: 113, Loss: 5722674176.0000, Val_loss: 5093804032.0000\n",
      "Epoch: 114, Loss: 5501144576.0000, Val_loss: 5097795584.0000\n",
      "Epoch: 115, Loss: 5944527872.0000, Val_loss: 5182195712.0000\n",
      "Epoch: 116, Loss: 5900182528.0000, Val_loss: 5132773888.0000\n",
      "Epoch: 117, Loss: 5491670016.0000, Val_loss: 5079295488.0000\n",
      "Epoch: 118, Loss: 5339703808.0000, Val_loss: 5103352832.0000\n",
      "Epoch: 119, Loss: 5341242880.0000, Val_loss: 5130484224.0000\n",
      "Epoch: 120, Loss: 5553449472.0000, Val_loss: 5145573888.0000\n",
      "Epoch: 121, Loss: 5434868224.0000, Val_loss: 5111658496.0000\n",
      "Epoch: 122, Loss: 4955643904.0000, Val_loss: 5079698432.0000\n",
      "Epoch: 123, Loss: 5461055488.0000, Val_loss: 5082615808.0000\n",
      "Epoch: 124, Loss: 5304078848.0000, Val_loss: 5096038400.0000\n",
      "Epoch: 125, Loss: 5811039232.0000, Val_loss: 5076698112.0000\n",
      "Epoch: 126, Loss: 5403752448.0000, Val_loss: 5081080832.0000\n",
      "Epoch: 127, Loss: 5643360256.0000, Val_loss: 5157218816.0000\n",
      "Epoch: 128, Loss: 5793557504.0000, Val_loss: 5230731264.0000\n",
      "Epoch: 129, Loss: 5464608256.0000, Val_loss: 5168725504.0000\n",
      "Epoch: 130, Loss: 5926533632.0000, Val_loss: 5106216448.0000\n",
      "Epoch: 131, Loss: 5133810176.0000, Val_loss: 5069478912.0000\n",
      "Epoch: 132, Loss: 5245609984.0000, Val_loss: 5077468160.0000\n",
      "Epoch: 133, Loss: 5483212288.0000, Val_loss: 5070651392.0000\n",
      "Epoch: 134, Loss: 5561997824.0000, Val_loss: 5074469376.0000\n",
      "Epoch: 135, Loss: 5459652096.0000, Val_loss: 5123870720.0000\n",
      "Epoch: 136, Loss: 5352456192.0000, Val_loss: 5164707840.0000\n",
      "Epoch: 137, Loss: 5686254592.0000, Val_loss: 5130784256.0000\n",
      "Epoch: 138, Loss: 5063265792.0000, Val_loss: 5075208704.0000\n",
      "Epoch: 139, Loss: 5315590144.0000, Val_loss: 5062397440.0000\n",
      "Epoch: 140, Loss: 5045509120.0000, Val_loss: 5083634688.0000\n",
      "Epoch: 141, Loss: 5791474688.0000, Val_loss: 5066201600.0000\n",
      "Epoch: 142, Loss: 5131913728.0000, Val_loss: 5060565504.0000\n",
      "Epoch: 143, Loss: 5132854272.0000, Val_loss: 5103437312.0000\n",
      "Epoch: 144, Loss: 5574067712.0000, Val_loss: 5193197056.0000\n",
      "Epoch: 145, Loss: 5504715264.0000, Val_loss: 5205565440.0000\n",
      "Epoch: 146, Loss: 5455293952.0000, Val_loss: 5134179840.0000\n",
      "Epoch: 147, Loss: 4904424448.0000, Val_loss: 5072525824.0000\n",
      "Epoch: 148, Loss: 5486522368.0000, Val_loss: 5054592512.0000\n",
      "Epoch: 149, Loss: 5359579136.0000, Val_loss: 5081395200.0000\n",
      "Epoch: 150, Loss: 5373895168.0000, Val_loss: 5089196032.0000\n",
      "Epoch: 151, Loss: 4992295424.0000, Val_loss: 5051074048.0000\n",
      "Epoch: 152, Loss: 5554379264.0000, Val_loss: 5063243264.0000\n",
      "Epoch: 153, Loss: 5068465664.0000, Val_loss: 5107364864.0000\n",
      "Epoch: 154, Loss: 5414297600.0000, Val_loss: 5125346816.0000\n",
      "Epoch: 155, Loss: 5766768640.0000, Val_loss: 5098303488.0000\n",
      "Epoch: 156, Loss: 5375649792.0000, Val_loss: 5067697664.0000\n",
      "Epoch: 157, Loss: 5636212224.0000, Val_loss: 5043796480.0000\n",
      "Epoch: 158, Loss: 5146301952.0000, Val_loss: 5063106560.0000\n",
      "Epoch: 159, Loss: 5399429632.0000, Val_loss: 5046074368.0000\n",
      "Epoch: 160, Loss: 5353081344.0000, Val_loss: 5053119488.0000\n",
      "Epoch: 161, Loss: 5329453056.0000, Val_loss: 5139615232.0000\n",
      "Epoch: 162, Loss: 5664697344.0000, Val_loss: 5191881216.0000\n",
      "Epoch: 163, Loss: 5373045248.0000, Val_loss: 5135198208.0000\n",
      "Epoch: 164, Loss: 5572760064.0000, Val_loss: 5076408320.0000\n",
      "Epoch: 165, Loss: 5070013440.0000, Val_loss: 5034316800.0000\n",
      "Epoch: 166, Loss: 4709431296.0000, Val_loss: 5050279936.0000\n",
      "Epoch: 167, Loss: 5449914368.0000, Val_loss: 5054771712.0000\n",
      "Epoch: 168, Loss: 5398378496.0000, Val_loss: 5031248896.0000\n",
      "Epoch: 169, Loss: 5477498368.0000, Val_loss: 5047095296.0000\n",
      "Epoch: 170, Loss: 5262046208.0000, Val_loss: 5126675968.0000\n",
      "Epoch: 171, Loss: 4778685952.0000, Val_loss: 5136328192.0000\n",
      "Epoch: 172, Loss: 4948798464.0000, Val_loss: 5087081984.0000\n",
      "Epoch: 173, Loss: 5569252352.0000, Val_loss: 5032335872.0000\n",
      "Epoch: 174, Loss: 5521669632.0000, Val_loss: 5025524224.0000\n",
      "Epoch: 175, Loss: 5206885888.0000, Val_loss: 5038903808.0000\n",
      "Epoch: 176, Loss: 5556568576.0000, Val_loss: 5029699072.0000\n",
      "Epoch: 177, Loss: 5201379328.0000, Val_loss: 5020226048.0000\n",
      "Epoch: 178, Loss: 5235581440.0000, Val_loss: 5025998848.0000\n",
      "Epoch: 179, Loss: 5321779200.0000, Val_loss: 5066236928.0000\n",
      "Epoch: 180, Loss: 5233540608.0000, Val_loss: 5100425728.0000\n",
      "Epoch: 181, Loss: 5327097344.0000, Val_loss: 5076639232.0000\n",
      "Epoch: 182, Loss: 5192002048.0000, Val_loss: 5021643264.0000\n",
      "Epoch: 183, Loss: 5274318848.0000, Val_loss: 5012966400.0000\n",
      "Epoch: 184, Loss: 5709141504.0000, Val_loss: 5013264384.0000\n",
      "Epoch: 185, Loss: 5057780224.0000, Val_loss: 5008495104.0000\n",
      "Epoch: 186, Loss: 5046702080.0000, Val_loss: 5014013440.0000\n",
      "Epoch: 187, Loss: 5226702848.0000, Val_loss: 5022107136.0000\n",
      "Epoch: 188, Loss: 5078481920.0000, Val_loss: 5016764416.0000\n",
      "Epoch: 189, Loss: 5403501568.0000, Val_loss: 5008484864.0000\n",
      "Epoch: 190, Loss: 5442820096.0000, Val_loss: 5004252672.0000\n",
      "Epoch: 191, Loss: 5242738176.0000, Val_loss: 5006176768.0000\n",
      "Epoch: 192, Loss: 5075576320.0000, Val_loss: 5004669952.0000\n",
      "Epoch: 193, Loss: 5444450816.0000, Val_loss: 4999313408.0000\n",
      "New lowest validation loss. Saved model\n",
      "Epoch: 194, Loss: 5309801472.0000, Val_loss: 4994622464.0000\n",
      "New lowest validation loss. Saved model\n",
      "Epoch: 195, Loss: 4949774848.0000, Val_loss: 5003220480.0000\n",
      "Epoch: 196, Loss: 5190963712.0000, Val_loss: 5017573376.0000\n",
      "Epoch: 197, Loss: 5144640512.0000, Val_loss: 5013263872.0000\n",
      "Epoch: 198, Loss: 5024864256.0000, Val_loss: 4993957888.0000\n",
      "New lowest validation loss. Saved model\n",
      "Epoch: 199, Loss: 5991252480.0000, Val_loss: 5011156480.0000\n",
      "Epoch: 200, Loss: 5150741504.0000, Val_loss: 5015418368.0000\n",
      "Epoch: 201, Loss: 5127190016.0000, Val_loss: 4984817664.0000\n",
      "New lowest validation loss. Saved model\n",
      "Epoch: 202, Loss: 5643350016.0000, Val_loss: 4977529856.0000\n",
      "New lowest validation loss. Saved model\n",
      "Epoch: 203, Loss: 5501219328.0000, Val_loss: 4976092160.0000\n",
      "New lowest validation loss. Saved model\n",
      "Epoch: 204, Loss: 4892765696.0000, Val_loss: 4972599296.0000\n",
      "New lowest validation loss. Saved model\n",
      "Epoch: 205, Loss: 5161446400.0000, Val_loss: 4970481664.0000\n",
      "New lowest validation loss. Saved model\n",
      "Epoch: 206, Loss: 5223877632.0000, Val_loss: 4973656064.0000\n",
      "Epoch: 207, Loss: 5242843136.0000, Val_loss: 4980913152.0000\n",
      "Epoch: 208, Loss: 5386618368.0000, Val_loss: 5009654272.0000\n",
      "Epoch: 209, Loss: 5508035584.0000, Val_loss: 5038405120.0000\n",
      "Epoch: 210, Loss: 5294424064.0000, Val_loss: 4995128320.0000\n",
      "Epoch: 211, Loss: 5418545664.0000, Val_loss: 4959322624.0000\n",
      "New lowest validation loss. Saved model\n",
      "Epoch: 212, Loss: 5072433152.0000, Val_loss: 4953748480.0000\n",
      "New lowest validation loss. Saved model\n",
      "Epoch: 213, Loss: 5208316928.0000, Val_loss: 4954923008.0000\n",
      "Epoch: 214, Loss: 5503565824.0000, Val_loss: 4995955200.0000\n",
      "Epoch: 215, Loss: 5285501440.0000, Val_loss: 5034210816.0000\n",
      "Epoch: 216, Loss: 4907488768.0000, Val_loss: 4959505920.0000\n",
      "Epoch: 217, Loss: 5216349696.0000, Val_loss: 4940331008.0000\n",
      "New lowest validation loss. Saved model\n",
      "Epoch: 218, Loss: 5478952448.0000, Val_loss: 4949520384.0000\n",
      "Epoch: 219, Loss: 5239980032.0000, Val_loss: 4938281472.0000\n",
      "New lowest validation loss. Saved model\n",
      "Epoch: 220, Loss: 5142524928.0000, Val_loss: 4939606016.0000\n",
      "Epoch: 221, Loss: 5619378176.0000, Val_loss: 5012229632.0000\n",
      "Epoch: 222, Loss: 5599058432.0000, Val_loss: 5028543488.0000\n",
      "Epoch: 223, Loss: 5824823808.0000, Val_loss: 4998422016.0000\n",
      "Epoch: 224, Loss: 5413099520.0000, Val_loss: 4938668032.0000\n",
      "Epoch: 225, Loss: 5292770304.0000, Val_loss: 4921204736.0000\n",
      "New lowest validation loss. Saved model\n",
      "Epoch: 226, Loss: 5371650560.0000, Val_loss: 4918904832.0000\n",
      "New lowest validation loss. Saved model\n",
      "Epoch: 227, Loss: 5241743360.0000, Val_loss: 4924709888.0000\n",
      "Epoch: 228, Loss: 5111780864.0000, Val_loss: 4943263744.0000\n",
      "Epoch: 229, Loss: 5140072960.0000, Val_loss: 4998317056.0000\n",
      "Epoch: 230, Loss: 5155551232.0000, Val_loss: 4954184704.0000\n",
      "Epoch: 231, Loss: 5304358400.0000, Val_loss: 4904664576.0000\n",
      "New lowest validation loss. Saved model\n",
      "Epoch: 232, Loss: 4874036736.0000, Val_loss: 4954088448.0000\n",
      "Epoch: 233, Loss: 5384613888.0000, Val_loss: 4913989632.0000\n",
      "Epoch: 234, Loss: 5211452416.0000, Val_loss: 4940899840.0000\n",
      "Epoch: 235, Loss: 5256568832.0000, Val_loss: 5050498048.0000\n",
      "Epoch: 236, Loss: 5192029184.0000, Val_loss: 4994276352.0000\n",
      "Epoch: 237, Loss: 4949199360.0000, Val_loss: 4890106880.0000\n",
      "New lowest validation loss. Saved model\n",
      "Epoch: 238, Loss: 4943893504.0000, Val_loss: 4942178816.0000\n",
      "Epoch: 239, Loss: 5430921216.0000, Val_loss: 4896181248.0000\n",
      "Epoch: 240, Loss: 5198957568.0000, Val_loss: 4899064320.0000\n",
      "Epoch: 241, Loss: 5543542784.0000, Val_loss: 4940819968.0000\n",
      "Epoch: 242, Loss: 5177675264.0000, Val_loss: 4966644736.0000\n",
      "Epoch: 243, Loss: 5333917184.0000, Val_loss: 4899572224.0000\n",
      "Epoch: 244, Loss: 5117345280.0000, Val_loss: 4882070528.0000\n",
      "New lowest validation loss. Saved model\n",
      "Epoch: 245, Loss: 5234469376.0000, Val_loss: 4901752832.0000\n",
      "Epoch: 246, Loss: 5257403904.0000, Val_loss: 4871691776.0000\n",
      "New lowest validation loss. Saved model\n",
      "Epoch: 247, Loss: 5047833600.0000, Val_loss: 4886019584.0000\n",
      "Epoch: 248, Loss: 5027724288.0000, Val_loss: 4946478592.0000\n",
      "Epoch: 249, Loss: 5287417344.0000, Val_loss: 4897967104.0000\n",
      "Epoch: 250, Loss: 5484521984.0000, Val_loss: 4858328576.0000\n",
      "New lowest validation loss. Saved model\n",
      "Epoch: 251, Loss: 4926633472.0000, Val_loss: 4863493632.0000\n",
      "Epoch: 252, Loss: 5095733248.0000, Val_loss: 4873364992.0000\n",
      "Epoch: 253, Loss: 4489302016.0000, Val_loss: 4859900928.0000\n",
      "Epoch: 254, Loss: 5446254592.0000, Val_loss: 4850739712.0000\n",
      "New lowest validation loss. Saved model\n",
      "Epoch: 255, Loss: 5426940928.0000, Val_loss: 4862670336.0000\n",
      "Epoch: 256, Loss: 4920877568.0000, Val_loss: 4843706368.0000\n",
      "New lowest validation loss. Saved model\n",
      "Epoch: 257, Loss: 4790233600.0000, Val_loss: 4844768256.0000\n",
      "Epoch: 258, Loss: 5310928896.0000, Val_loss: 4852305408.0000\n",
      "Epoch: 259, Loss: 5492610048.0000, Val_loss: 4886316544.0000\n",
      "Epoch: 260, Loss: 4890043392.0000, Val_loss: 4848654336.0000\n",
      "Epoch: 261, Loss: 5331021312.0000, Val_loss: 4846929408.0000\n",
      "Epoch: 262, Loss: 5281999360.0000, Val_loss: 4841886208.0000\n",
      "New lowest validation loss. Saved model\n",
      "Epoch: 263, Loss: 5233981440.0000, Val_loss: 4826944000.0000\n",
      "New lowest validation loss. Saved model\n",
      "Epoch: 264, Loss: 5292897792.0000, Val_loss: 4822814208.0000\n",
      "New lowest validation loss. Saved model\n",
      "Epoch: 265, Loss: 5253588992.0000, Val_loss: 4823770112.0000\n",
      "Epoch: 266, Loss: 5223044096.0000, Val_loss: 4838543872.0000\n",
      "Epoch: 267, Loss: 5143938048.0000, Val_loss: 4841013248.0000\n",
      "Epoch: 268, Loss: 5719076352.0000, Val_loss: 4831487488.0000\n",
      "Epoch: 269, Loss: 5348012544.0000, Val_loss: 4827274752.0000\n",
      "Epoch: 270, Loss: 4986472960.0000, Val_loss: 4806563328.0000\n",
      "New lowest validation loss. Saved model\n",
      "Epoch: 271, Loss: 5171101696.0000, Val_loss: 4799141376.0000\n",
      "New lowest validation loss. Saved model\n",
      "Epoch: 272, Loss: 5069679616.0000, Val_loss: 4796964352.0000\n",
      "New lowest validation loss. Saved model\n",
      "Epoch: 273, Loss: 4730691072.0000, Val_loss: 4802777088.0000\n",
      "Epoch: 274, Loss: 5447178240.0000, Val_loss: 4863038976.0000\n",
      "Epoch: 275, Loss: 4956599296.0000, Val_loss: 4809410048.0000\n",
      "Epoch: 276, Loss: 5427831296.0000, Val_loss: 4789903872.0000\n",
      "New lowest validation loss. Saved model\n",
      "Epoch: 277, Loss: 5428930560.0000, Val_loss: 4790571520.0000\n",
      "Epoch: 278, Loss: 5086861312.0000, Val_loss: 4795015168.0000\n",
      "Epoch: 279, Loss: 5141531648.0000, Val_loss: 4781371392.0000\n",
      "New lowest validation loss. Saved model\n",
      "Epoch: 280, Loss: 5194740736.0000, Val_loss: 4909472768.0000\n",
      "Epoch: 281, Loss: 5763388416.0000, Val_loss: 4929790464.0000\n",
      "Epoch: 282, Loss: 5551846912.0000, Val_loss: 4788440064.0000\n",
      "Epoch: 283, Loss: 5128986112.0000, Val_loss: 4772502016.0000\n",
      "New lowest validation loss. Saved model\n",
      "Epoch: 284, Loss: 5221770240.0000, Val_loss: 4761007616.0000\n",
      "New lowest validation loss. Saved model\n",
      "Epoch: 285, Loss: 4725013504.0000, Val_loss: 4807608320.0000\n",
      "Epoch: 286, Loss: 5202235904.0000, Val_loss: 4843154944.0000\n",
      "Epoch: 287, Loss: 5088283648.0000, Val_loss: 4770390016.0000\n",
      "Epoch: 288, Loss: 4972400128.0000, Val_loss: 4763550720.0000\n",
      "Epoch: 289, Loss: 4676196864.0000, Val_loss: 4805736960.0000\n",
      "Epoch: 290, Loss: 4798348800.0000, Val_loss: 4753880576.0000\n",
      "New lowest validation loss. Saved model\n",
      "Epoch: 291, Loss: 5538947072.0000, Val_loss: 4792316928.0000\n",
      "Epoch: 292, Loss: 4894194688.0000, Val_loss: 4850380800.0000\n",
      "Epoch: 293, Loss: 5150692352.0000, Val_loss: 4763981312.0000\n",
      "Epoch: 294, Loss: 4660864512.0000, Val_loss: 4757923840.0000\n",
      "Epoch: 295, Loss: 4972923392.0000, Val_loss: 4781938176.0000\n",
      "Epoch: 296, Loss: 5255733760.0000, Val_loss: 4735262208.0000\n",
      "New lowest validation loss. Saved model\n",
      "Epoch: 297, Loss: 5315704320.0000, Val_loss: 4834416640.0000\n",
      "Epoch: 298, Loss: 5240172544.0000, Val_loss: 4832952832.0000\n",
      "Epoch: 299, Loss: 5479519232.0000, Val_loss: 4724397568.0000\n",
      "New lowest validation loss. Saved model\n",
      "Epoch: 300, Loss: 5192491008.0000, Val_loss: 4724656640.0000\n",
      "Epoch: 301, Loss: 5342495232.0000, Val_loss: 4753050624.0000\n",
      "Epoch: 302, Loss: 5379771904.0000, Val_loss: 4871704064.0000\n",
      "Epoch: 303, Loss: 5444370944.0000, Val_loss: 4734371328.0000\n",
      "Epoch: 304, Loss: 4888039936.0000, Val_loss: 4742470144.0000\n",
      "Epoch: 305, Loss: 4899070976.0000, Val_loss: 4719015936.0000\n",
      "New lowest validation loss. Saved model\n",
      "Epoch: 306, Loss: 5640833024.0000, Val_loss: 4714315776.0000\n",
      "New lowest validation loss. Saved model\n",
      "Epoch: 307, Loss: 4795722240.0000, Val_loss: 4738655232.0000\n",
      "Epoch: 308, Loss: 5166263808.0000, Val_loss: 4768692224.0000\n",
      "Epoch: 309, Loss: 5406589952.0000, Val_loss: 4732560384.0000\n",
      "Epoch: 310, Loss: 4852409344.0000, Val_loss: 4683084800.0000\n",
      "New lowest validation loss. Saved model\n",
      "Epoch: 311, Loss: 5062003712.0000, Val_loss: 4687601664.0000\n",
      "Epoch: 312, Loss: 5090719744.0000, Val_loss: 4689293312.0000\n",
      "Epoch: 313, Loss: 4293816576.0000, Val_loss: 4673370624.0000\n",
      "New lowest validation loss. Saved model\n",
      "Epoch: 314, Loss: 5236491264.0000, Val_loss: 4669698048.0000\n",
      "New lowest validation loss. Saved model\n",
      "Epoch: 315, Loss: 4586953728.0000, Val_loss: 4666627584.0000\n",
      "New lowest validation loss. Saved model\n",
      "Epoch: 316, Loss: 5055602688.0000, Val_loss: 4702204928.0000\n",
      "Epoch: 317, Loss: 5156907520.0000, Val_loss: 4741021696.0000\n",
      "Epoch: 318, Loss: 5321449472.0000, Val_loss: 4667216896.0000\n",
      "Epoch: 319, Loss: 5121640960.0000, Val_loss: 4666738176.0000\n",
      "Epoch: 320, Loss: 4876465152.0000, Val_loss: 4657264640.0000\n",
      "New lowest validation loss. Saved model\n",
      "Epoch: 321, Loss: 4855664640.0000, Val_loss: 4672347648.0000\n",
      "Epoch: 322, Loss: 5030837248.0000, Val_loss: 4679787520.0000\n",
      "Epoch: 323, Loss: 5347020800.0000, Val_loss: 4639497728.0000\n",
      "New lowest validation loss. Saved model\n",
      "Epoch: 324, Loss: 4938651136.0000, Val_loss: 4648538624.0000\n",
      "Epoch: 325, Loss: 5362856960.0000, Val_loss: 4637705216.0000\n",
      "New lowest validation loss. Saved model\n",
      "Epoch: 326, Loss: 5353415168.0000, Val_loss: 4634597376.0000\n",
      "New lowest validation loss. Saved model\n",
      "Epoch: 327, Loss: 4940661760.0000, Val_loss: 4665743872.0000\n",
      "Epoch: 328, Loss: 5107457024.0000, Val_loss: 4689509888.0000\n",
      "Epoch: 329, Loss: 4987289600.0000, Val_loss: 4619790848.0000\n",
      "New lowest validation loss. Saved model\n",
      "Epoch: 330, Loss: 4709867520.0000, Val_loss: 4623199232.0000\n",
      "Epoch: 331, Loss: 5112717824.0000, Val_loss: 4614988288.0000\n",
      "New lowest validation loss. Saved model\n",
      "Epoch: 332, Loss: 4910085632.0000, Val_loss: 4606727680.0000\n",
      "New lowest validation loss. Saved model\n",
      "Epoch: 333, Loss: 4924090368.0000, Val_loss: 4603624960.0000\n",
      "New lowest validation loss. Saved model\n",
      "Epoch: 334, Loss: 5327541760.0000, Val_loss: 4627553792.0000\n",
      "Epoch: 335, Loss: 5097872384.0000, Val_loss: 4708760576.0000\n",
      "Epoch: 336, Loss: 4818055680.0000, Val_loss: 4595625472.0000\n",
      "New lowest validation loss. Saved model\n",
      "Epoch: 337, Loss: 4992517120.0000, Val_loss: 4600514048.0000\n",
      "Epoch: 338, Loss: 5119297536.0000, Val_loss: 4613234176.0000\n",
      "Epoch: 339, Loss: 4883977216.0000, Val_loss: 4593794560.0000\n",
      "New lowest validation loss. Saved model\n",
      "Epoch: 340, Loss: 4768122880.0000, Val_loss: 4697890304.0000\n",
      "Epoch: 341, Loss: 5042821120.0000, Val_loss: 4587769344.0000\n",
      "New lowest validation loss. Saved model\n",
      "Epoch: 342, Loss: 4986560512.0000, Val_loss: 4631193088.0000\n",
      "Epoch: 343, Loss: 4597584896.0000, Val_loss: 4596173824.0000\n",
      "Epoch: 344, Loss: 5183632384.0000, Val_loss: 4594529280.0000\n",
      "Epoch: 345, Loss: 5043139584.0000, Val_loss: 4783373824.0000\n",
      "Epoch: 346, Loss: 5125303808.0000, Val_loss: 4632913920.0000\n",
      "Epoch: 347, Loss: 5323946496.0000, Val_loss: 4602163200.0000\n",
      "Epoch: 348, Loss: 5075345408.0000, Val_loss: 4588901888.0000\n",
      "Epoch: 349, Loss: 5446527488.0000, Val_loss: 4611952128.0000\n",
      "Epoch: 350, Loss: 5064022016.0000, Val_loss: 4615190528.0000\n",
      "Epoch: 351, Loss: 4852263936.0000, Val_loss: 4578744832.0000\n",
      "New lowest validation loss. Saved model\n",
      "Epoch: 352, Loss: 5013639680.0000, Val_loss: 4548297728.0000\n",
      "New lowest validation loss. Saved model\n",
      "Epoch: 353, Loss: 5155623936.0000, Val_loss: 4538968064.0000\n",
      "New lowest validation loss. Saved model\n",
      "Epoch: 354, Loss: 5360869376.0000, Val_loss: 4645110784.0000\n",
      "Epoch: 355, Loss: 5118130176.0000, Val_loss: 4536745472.0000\n",
      "New lowest validation loss. Saved model\n",
      "Epoch: 356, Loss: 5014309376.0000, Val_loss: 4625466368.0000\n",
      "Epoch: 357, Loss: 5173243392.0000, Val_loss: 4540884480.0000\n",
      "Epoch: 358, Loss: 4642598400.0000, Val_loss: 4649832960.0000\n",
      "Epoch: 359, Loss: 5403595264.0000, Val_loss: 4619517952.0000\n",
      "Epoch: 360, Loss: 4631398912.0000, Val_loss: 4550373376.0000\n",
      "Epoch: 361, Loss: 4706526208.0000, Val_loss: 4565839872.0000\n",
      "Epoch: 362, Loss: 5360828928.0000, Val_loss: 4518499328.0000\n",
      "New lowest validation loss. Saved model\n",
      "Epoch: 363, Loss: 4969661952.0000, Val_loss: 4533469696.0000\n",
      "Epoch: 364, Loss: 4944858112.0000, Val_loss: 4530419712.0000\n",
      "Epoch: 365, Loss: 5273048576.0000, Val_loss: 4529881088.0000\n",
      "Epoch: 366, Loss: 4590516736.0000, Val_loss: 4529117184.0000\n",
      "Epoch: 367, Loss: 5579172352.0000, Val_loss: 4511916544.0000\n",
      "New lowest validation loss. Saved model\n",
      "Epoch: 368, Loss: 4982417920.0000, Val_loss: 4534156800.0000\n",
      "Epoch: 369, Loss: 4770067456.0000, Val_loss: 4534602240.0000\n",
      "Epoch: 370, Loss: 4711057408.0000, Val_loss: 4516229120.0000\n",
      "Epoch: 371, Loss: 5246632448.0000, Val_loss: 4498865664.0000\n",
      "New lowest validation loss. Saved model\n",
      "Epoch: 372, Loss: 4886522880.0000, Val_loss: 4552129024.0000\n",
      "Epoch: 373, Loss: 5314798080.0000, Val_loss: 4663521280.0000\n",
      "Epoch: 374, Loss: 5125813760.0000, Val_loss: 4486367744.0000\n",
      "New lowest validation loss. Saved model\n",
      "Epoch: 375, Loss: 4932291072.0000, Val_loss: 4498348544.0000\n",
      "Epoch: 376, Loss: 4921901056.0000, Val_loss: 4480559616.0000\n",
      "New lowest validation loss. Saved model\n",
      "Epoch: 377, Loss: 4678290432.0000, Val_loss: 4475825664.0000\n",
      "New lowest validation loss. Saved model\n",
      "Epoch: 378, Loss: 5435110400.0000, Val_loss: 4485655552.0000\n",
      "Epoch: 379, Loss: 5086488064.0000, Val_loss: 4541236224.0000\n",
      "Epoch: 380, Loss: 4691993600.0000, Val_loss: 4470297088.0000\n",
      "New lowest validation loss. Saved model\n",
      "Epoch: 381, Loss: 4997188608.0000, Val_loss: 4479529472.0000\n",
      "Epoch: 382, Loss: 4830938112.0000, Val_loss: 4474872320.0000\n",
      "Epoch: 383, Loss: 4865307648.0000, Val_loss: 4539881984.0000\n",
      "Epoch: 384, Loss: 5141128704.0000, Val_loss: 4459479552.0000\n",
      "New lowest validation loss. Saved model\n",
      "Epoch: 385, Loss: 5020150784.0000, Val_loss: 4460429312.0000\n",
      "Epoch: 386, Loss: 4644852224.0000, Val_loss: 4462586880.0000\n",
      "Epoch: 387, Loss: 4777025024.0000, Val_loss: 4527870464.0000\n",
      "Epoch: 388, Loss: 5420502528.0000, Val_loss: 4497585664.0000\n",
      "Epoch: 389, Loss: 4790339072.0000, Val_loss: 4483154944.0000\n",
      "Epoch: 390, Loss: 4529188864.0000, Val_loss: 4602104832.0000\n",
      "Epoch: 391, Loss: 5213360128.0000, Val_loss: 4441430528.0000\n",
      "New lowest validation loss. Saved model\n",
      "Epoch: 392, Loss: 5021829632.0000, Val_loss: 4626316800.0000\n",
      "Epoch: 393, Loss: 5081831424.0000, Val_loss: 4440640000.0000\n",
      "New lowest validation loss. Saved model\n",
      "Epoch: 394, Loss: 5492745728.0000, Val_loss: 4473116672.0000\n",
      "Epoch: 395, Loss: 5360648704.0000, Val_loss: 4433544192.0000\n",
      "New lowest validation loss. Saved model\n",
      "Epoch: 396, Loss: 4888025600.0000, Val_loss: 4472729088.0000\n",
      "Epoch: 397, Loss: 5468658176.0000, Val_loss: 4467593216.0000\n",
      "Epoch: 398, Loss: 5228344832.0000, Val_loss: 4436695552.0000\n",
      "Epoch: 399, Loss: 4967863296.0000, Val_loss: 4455906816.0000\n",
      "Epoch: 400, Loss: 4777096192.0000, Val_loss: 4425604096.0000\n",
      "New lowest validation loss. Saved model\n",
      "Epoch: 401, Loss: 4966693888.0000, Val_loss: 4424948224.0000\n",
      "New lowest validation loss. Saved model\n",
      "Epoch: 402, Loss: 5076673536.0000, Val_loss: 4429686784.0000\n",
      "Epoch: 403, Loss: 5287639040.0000, Val_loss: 4463206912.0000\n",
      "Epoch: 404, Loss: 4688495104.0000, Val_loss: 4435491840.0000\n",
      "Epoch: 405, Loss: 5385942016.0000, Val_loss: 4454761472.0000\n",
      "Epoch: 406, Loss: 5100448768.0000, Val_loss: 4433839104.0000\n",
      "Epoch: 407, Loss: 4502964224.0000, Val_loss: 4437914624.0000\n",
      "Epoch: 408, Loss: 4865099264.0000, Val_loss: 4452508672.0000\n",
      "Epoch: 409, Loss: 5222339072.0000, Val_loss: 4424887296.0000\n",
      "New lowest validation loss. Saved model\n",
      "Epoch: 410, Loss: 4436019200.0000, Val_loss: 4420489216.0000\n",
      "New lowest validation loss. Saved model\n",
      "Epoch: 411, Loss: 4862018560.0000, Val_loss: 4414295040.0000\n",
      "New lowest validation loss. Saved model\n",
      "Epoch: 412, Loss: 5077026816.0000, Val_loss: 4424924160.0000\n",
      "Epoch: 413, Loss: 4775960064.0000, Val_loss: 4414669312.0000\n",
      "Epoch: 414, Loss: 4338016256.0000, Val_loss: 4419440640.0000\n",
      "Epoch: 415, Loss: 5421153280.0000, Val_loss: 4410827776.0000\n",
      "New lowest validation loss. Saved model\n",
      "Epoch: 416, Loss: 4706008576.0000, Val_loss: 4416963584.0000\n",
      "Epoch: 417, Loss: 4866702848.0000, Val_loss: 4405664768.0000\n",
      "New lowest validation loss. Saved model\n",
      "Epoch: 418, Loss: 4764912640.0000, Val_loss: 4420059136.0000\n",
      "Epoch: 419, Loss: 4555997184.0000, Val_loss: 4399705088.0000\n",
      "New lowest validation loss. Saved model\n",
      "Epoch: 420, Loss: 5800975872.0000, Val_loss: 4707145216.0000\n",
      "Epoch: 421, Loss: 5295284224.0000, Val_loss: 4419258880.0000\n",
      "Epoch: 422, Loss: 5247748096.0000, Val_loss: 4535188480.0000\n",
      "Epoch: 423, Loss: 5126923776.0000, Val_loss: 4418169344.0000\n",
      "Epoch: 424, Loss: 4973236736.0000, Val_loss: 4632934400.0000\n",
      "Epoch: 425, Loss: 4830732288.0000, Val_loss: 4491620864.0000\n",
      "Epoch: 426, Loss: 5238409728.0000, Val_loss: 4411879424.0000\n",
      "Epoch: 427, Loss: 5013717504.0000, Val_loss: 4424746496.0000\n",
      "Epoch: 428, Loss: 5042650112.0000, Val_loss: 4428403200.0000\n",
      "Epoch: 429, Loss: 5204181504.0000, Val_loss: 4465256960.0000\n",
      "Epoch: 430, Loss: 4689040384.0000, Val_loss: 4385024000.0000\n",
      "New lowest validation loss. Saved model\n",
      "Epoch: 431, Loss: 4531691008.0000, Val_loss: 4428678144.0000\n",
      "Epoch: 432, Loss: 4265417728.0000, Val_loss: 4380076032.0000\n",
      "New lowest validation loss. Saved model\n",
      "Epoch: 433, Loss: 4317267456.0000, Val_loss: 4387857408.0000\n",
      "Epoch: 434, Loss: 5124003840.0000, Val_loss: 4485140992.0000\n",
      "Epoch: 435, Loss: 5280486912.0000, Val_loss: 4376057856.0000\n",
      "New lowest validation loss. Saved model\n",
      "Epoch: 436, Loss: 5069602816.0000, Val_loss: 4387389952.0000\n",
      "Epoch: 437, Loss: 4970843648.0000, Val_loss: 4371038208.0000\n",
      "New lowest validation loss. Saved model\n",
      "Epoch: 438, Loss: 4633732096.0000, Val_loss: 4378598400.0000\n",
      "Epoch: 439, Loss: 4613285888.0000, Val_loss: 4369902080.0000\n",
      "New lowest validation loss. Saved model\n",
      "Epoch: 440, Loss: 4935880192.0000, Val_loss: 4372400640.0000\n",
      "Epoch: 441, Loss: 4718459392.0000, Val_loss: 4378936832.0000\n",
      "Epoch: 442, Loss: 5030051328.0000, Val_loss: 4362014720.0000\n",
      "New lowest validation loss. Saved model\n",
      "Epoch: 443, Loss: 4947163136.0000, Val_loss: 4371160576.0000\n",
      "Epoch: 444, Loss: 4664732672.0000, Val_loss: 4354708480.0000\n",
      "New lowest validation loss. Saved model\n",
      "Epoch: 445, Loss: 4465219584.0000, Val_loss: 4343068672.0000\n",
      "New lowest validation loss. Saved model\n",
      "Epoch: 446, Loss: 4840336896.0000, Val_loss: 4399267840.0000\n",
      "Epoch: 447, Loss: 4587045376.0000, Val_loss: 4345140736.0000\n",
      "Epoch: 448, Loss: 4622020096.0000, Val_loss: 4443496448.0000\n",
      "Epoch: 449, Loss: 4695719936.0000, Val_loss: 4343611392.0000\n",
      "Epoch: 450, Loss: 4839439360.0000, Val_loss: 4608764928.0000\n",
      "Epoch: 451, Loss: 5685506048.0000, Val_loss: 4377544704.0000\n",
      "Epoch: 452, Loss: 5340197376.0000, Val_loss: 4603051008.0000\n",
      "Epoch: 453, Loss: 5165409792.0000, Val_loss: 4337165312.0000\n",
      "New lowest validation loss. Saved model\n",
      "Epoch: 454, Loss: 4878092288.0000, Val_loss: 4603524096.0000\n",
      "Epoch: 455, Loss: 5611159552.0000, Val_loss: 4375239680.0000\n",
      "Epoch: 456, Loss: 4818084864.0000, Val_loss: 4426340864.0000\n",
      "Epoch: 457, Loss: 5179581952.0000, Val_loss: 4362979328.0000\n",
      "Epoch: 458, Loss: 4798841856.0000, Val_loss: 4365910528.0000\n",
      "Epoch: 459, Loss: 5196914688.0000, Val_loss: 4401840128.0000\n",
      "Epoch: 460, Loss: 5209988096.0000, Val_loss: 4342372864.0000\n",
      "Epoch: 461, Loss: 4447111680.0000, Val_loss: 4386715648.0000\n",
      "Epoch: 462, Loss: 4797070336.0000, Val_loss: 4331525632.0000\n",
      "New lowest validation loss. Saved model\n",
      "Epoch: 463, Loss: 4704393728.0000, Val_loss: 4360681472.0000\n",
      "Epoch: 464, Loss: 4765199360.0000, Val_loss: 4334779392.0000\n",
      "Epoch: 465, Loss: 5070283776.0000, Val_loss: 4334931456.0000\n",
      "Epoch: 466, Loss: 4849293312.0000, Val_loss: 4334701568.0000\n",
      "Epoch: 467, Loss: 5247154688.0000, Val_loss: 4354563584.0000\n",
      "Epoch: 468, Loss: 4642206208.0000, Val_loss: 4331191808.0000\n",
      "New lowest validation loss. Saved model\n",
      "Epoch: 469, Loss: 4950005760.0000, Val_loss: 4321089024.0000\n",
      "New lowest validation loss. Saved model\n",
      "Epoch: 470, Loss: 4708135424.0000, Val_loss: 4317099008.0000\n",
      "New lowest validation loss. Saved model\n",
      "Epoch: 471, Loss: 5566060032.0000, Val_loss: 4317554176.0000\n",
      "Epoch: 472, Loss: 4840088064.0000, Val_loss: 4322134016.0000\n",
      "Epoch: 473, Loss: 4497116160.0000, Val_loss: 4327040000.0000\n",
      "Epoch: 474, Loss: 4958725120.0000, Val_loss: 4343369728.0000\n",
      "Epoch: 475, Loss: 4837066752.0000, Val_loss: 4326041088.0000\n",
      "Epoch: 476, Loss: 4756041728.0000, Val_loss: 4324606976.0000\n",
      "Epoch: 477, Loss: 4499625984.0000, Val_loss: 4322294272.0000\n",
      "Epoch: 478, Loss: 5044404224.0000, Val_loss: 4383817728.0000\n",
      "Epoch: 479, Loss: 5042744832.0000, Val_loss: 4379642368.0000\n",
      "Epoch: 480, Loss: 4792517632.0000, Val_loss: 4329641472.0000\n",
      "Epoch: 481, Loss: 4857326592.0000, Val_loss: 4334342656.0000\n",
      "Epoch: 482, Loss: 4835090944.0000, Val_loss: 4402270720.0000\n",
      "Epoch: 483, Loss: 5146690560.0000, Val_loss: 4476647936.0000\n",
      "Epoch: 484, Loss: 4634102784.0000, Val_loss: 4301957120.0000\n",
      "New lowest validation loss. Saved model\n",
      "Epoch: 485, Loss: 4420474368.0000, Val_loss: 4385928704.0000\n",
      "Epoch: 486, Loss: 5199307264.0000, Val_loss: 4303789056.0000\n",
      "Epoch: 487, Loss: 5364037120.0000, Val_loss: 4388785152.0000\n",
      "Epoch: 488, Loss: 5303599616.0000, Val_loss: 4328403456.0000\n",
      "Epoch: 489, Loss: 4714182656.0000, Val_loss: 4372880896.0000\n",
      "Epoch: 490, Loss: 4574882816.0000, Val_loss: 4376087040.0000\n",
      "Epoch: 491, Loss: 4669656064.0000, Val_loss: 4310711808.0000\n",
      "Epoch: 492, Loss: 4437819392.0000, Val_loss: 4304516608.0000\n",
      "Epoch: 493, Loss: 5360949760.0000, Val_loss: 4306337792.0000\n",
      "Epoch: 494, Loss: 5317950976.0000, Val_loss: 4316093952.0000\n",
      "Epoch: 495, Loss: 4807448064.0000, Val_loss: 4315810816.0000\n",
      "Epoch: 496, Loss: 4587243520.0000, Val_loss: 4341671936.0000\n",
      "Epoch: 497, Loss: 5118543360.0000, Val_loss: 4297362432.0000\n",
      "New lowest validation loss. Saved model\n",
      "Epoch: 498, Loss: 4338989568.0000, Val_loss: 4322435584.0000\n",
      "Epoch: 499, Loss: 4688905216.0000, Val_loss: 4322308096.0000\n",
      "Epoch: 500, Loss: 5160025088.0000, Val_loss: 4304614400.0000\n",
      "\n",
      "Ground truth (AADT(2010)-A values that were assigned to the test part):\n",
      "tensor([115923.2266, 108502.9844,  81307.8828, 116829.2969, 109539.9062,\n",
      "        157256.4219,  95713.0234,  92675.0859,  72290.0547, 209065.8281,\n",
      "          1260.6698,  18712.5605,   2852.4504,  34440.3047,  51579.4180,\n",
      "        108860.2500, 106953.3125,  42198.9609, 254556.4219, 220044.2656,\n",
      "        243317.9531, 197124.1562, 197073.0938, 189823.0625, 161814.7656,\n",
      "        163215.3438, 143309.9375, 102174.2266,  55774.6797,  22155.4004,\n",
      "         22050.0801,  22050.0801, 143954.4375, 163157.7031, 102515.6250,\n",
      "         65117.6094,  62625.0469,  65301.9570,  91188.6094,  99110.5078,\n",
      "         95109.7188,  76038.0078, 210352.3281, 262154.0312, 239057.2344,\n",
      "        259341.4531, 157422.6875,  89374.9766,  75923.9297,  51174.8906,\n",
      "         40343.1250,  67234.7422,  59402.5000,  35195.9180,  82215.9922,\n",
      "        145937.7812, 121931.3438,  78650.2500,  83159.8438,  74451.6953,\n",
      "         72008.1641,  25877.2695,  64401.9062,   5194.3027,  31387.7695,\n",
      "         21170.4199,  25073.6270,  81178.8750,  62859.6562,  77929.5469,\n",
      "        100773.3125, 112093.5703, 102517.7969,  38911.0312,  62059.3984,\n",
      "         82491.9297, 150195.7969, 101816.0391, 126035.6406, 124855.5469,\n",
      "        101064.2500,  88923.3438,  58323.0039,  94064.6328, 134998.8281,\n",
      "        126075.3906])\n",
      "\n",
      "Model predictions (estimated AADT(2010)-A values of the test part, i.e., estimated by the GNN):\n",
      "tensor([ 77307.7891,  98699.8125,  70614.7969,  88357.5781, 101297.9922,\n",
      "        149054.7500,  78337.4922,  93202.9297,  86969.9062, 105989.9688,\n",
      "        113433.9688, 136359.5938, 124918.6406,  91712.9453,  93192.8672,\n",
      "         98487.0781, 103864.0234, 117395.2891, 103573.2891,  93860.1641,\n",
      "         93939.7266,  88038.8984,  97136.8516,  88137.3750, 104207.1797,\n",
      "        105877.4688, 103827.6797, 101605.8594, 114390.6953,  97964.2109,\n",
      "        103583.4531,  97076.4922, 105559.7109,  98419.1719,  73516.7578,\n",
      "         81766.8516,  73234.5391,  73982.7656,  76664.5312,  85356.5938,\n",
      "         79815.6797,  95446.5547, 121515.3984, 121472.7734, 121294.4766,\n",
      "        126574.4609, 140172.1094,  73415.5078,  96794.0000,  73810.1797,\n",
      "         73157.9844, 106992.5938, 101266.8984, 119378.3516, 115668.5938,\n",
      "        122176.6094, 160018.9375, 121840.7969, 115404.9531, 115272.5469,\n",
      "        117302.7578, 112639.9297,  81828.6641, 123246.0000,  69050.7031,\n",
      "         51978.7695,  52037.8320,  65890.1328,  65934.4453,  73805.1172,\n",
      "         85629.3594,  85725.7109,  86193.0625,  86366.0703,  71478.0391,\n",
      "         86443.2656,  85204.9062,  72351.9375,  57249.4609,  62169.5039,\n",
      "         38421.3477,  47080.1992,  83138.1328,  75919.5156,  60742.8281,\n",
      "         86359.1172])\n",
      "\n",
      "Relative Test loss (squared error between the above two vectors / abs(y_tensor)\n",
      "\n",
      " =  3.89e+09  /  1.41e+10  =  2.77e-01 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print(nodes_tensor.shape)\n",
    "# print(nodes_tensor.dtype)\n",
    "# print(edges_tensor.shape)\n",
    "# print(edges_tensor.dtype)\n",
    "# print(y_tensor.shape)\n",
    "# print(y_tensor.dtype)\n",
    "# print(train_mask.shape)\n",
    "# print(train_mask.dtype)\n",
    "# print(train_mask)\n",
    "\n",
    "model = GCN(hidden_channels=16)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "def train():\n",
    "      model.train()\n",
    "      optimizer.zero_grad()\n",
    "      out = model(nodes_tensor, edges_tensor)\n",
    "      loss = criterion(out[train_mask].squeeze(1), y_tensor[train_mask])\n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "      return loss\n",
    "\n",
    "\n",
    "def test(mask):\n",
    "      model.eval()\n",
    "      out = model(nodes_tensor, edges_tensor)\n",
    "      return criterion(out[mask].squeeze(1), y_tensor[mask])\n",
    "\n",
    "# Todo: There should probably be some logic here to keep going until we see N iterations without\n",
    "#       an improvement in validation loss\n",
    "val_losses = []\n",
    "lowest_val_loss       = float(\"inf\")\n",
    "lowest_val_loss_epoch = -1\n",
    "\n",
    "for epoch in range(1, 501):\n",
    "    loss = train()\n",
    "    val_loss = test(val_mask)\n",
    "    val_losses.append(val_loss)\n",
    "    print(f'Epoch: {epoch:03d}, Loss: {loss:.4f}, Val_loss: {val_loss:.4f}')\n",
    "    if val_loss < lowest_val_loss:\n",
    "        lowest_val_loss       = val_loss\n",
    "        lowest_val_loss_epoch = epoch\n",
    "        torch.save(model.state_dict(), \"checkpoint.pth\")\n",
    "        print(\"New lowest validation loss. Saved model\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    model.load_state_dict(torch.load(\"checkpoint.pth\"))\n",
    "    model.eval()\n",
    "    test_out = model(nodes_tensor, edges_tensor)\n",
    "    test_loss = criterion(test_out[test_mask].squeeze(1), y_tensor[test_mask])\n",
    "    abs_y_tensor = criterion(torch.zeros(torch.Tensor.size(y_tensor[test_mask])), y_tensor[test_mask])\n",
    "    #test_loss_relative = test_loss/criterion(torch.zeros(torch.Tensor.size(y_tensor[test_mask])), y_tensor[test_mask]) #relative test error\n",
    "    test_loss_relative = test_loss/abs_y_tensor\n",
    "\n",
    "print(\"\\nGround truth (AADT(2010)-A values that were assigned to the test part):\")\n",
    "print(y_tensor[test_mask])\n",
    "print(\"\\nModel predictions (estimated AADT(2010)-A values of the test part, i.e., estimated by the GNN):\")\n",
    "print(test_out[test_mask].squeeze(1))\n",
    "\n",
    "print(\"\\nRelative Test loss (squared error between the above two vectors / abs(y_tensor)\")\n",
    "print(\"\\n = \", format(test_loss.item(), '.2e'), \" / \", format(abs_y_tensor.item(),'.2e'), \" = \", format(test_loss_relative.item(),'.2e'), \"\\n\")\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "39f2d295-6260-409f-b50c-9dd70afe0916",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below are some links that I found helpful\n",
    "\n",
    "# https://arxiv.org/pdf/1609.02907.pdf (note: this is the net that GCNConv implements)\n",
    "#         (SEMI-SUPERVISED CLASSIFICATION WITH GRAPH CONVOLUTIONAL NETWORKS)\n",
    "# But, the issue with the above is that we need a GNN to predict links/edges. GCNConv only does nodes\n",
    "# That's why I had to use the link graph trick\n",
    "\n",
    "# https://rish-16.github.io/posts/gnn-math/ (Math Behind Graph Neural Networks)  \n",
    "\n",
    "# https://blog.dataiku.com/graph-neural-networks-link-prediction-part-two\n",
    "\n",
    "# https://arxiv.org/pdf/1901.00596.pdf (A Comprehensive Survey on Graph Neural Networks)\n",
    "# https://arxiv.org/pdf/1609.02907.pdf (SEMI-SUPERVISED CLASSIFICATION WITH GRAPH CONVOLUTIONAL NETWORKS)\n",
    "# https://arxiv.org/pdf/2302.05631.pdf (A Survey on Spectral Graph Neural Networks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "45ea4de7-c1d0-4e6d-b277-9ac9a8a2dfab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below is some code that I copied over (with very minor modifications) to view the \"quickstart.xlsx\" file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "005bab46-c552-4e86-be8d-d261fdf6bba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# datadir = \"../data/\"\n",
    "# datafilename = \"quickstart.xlsx\"\n",
    "# datafilepath = datadir + datafilename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d9a16d10-22e5-4810-9461-5362e3b6a1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "\n",
    "# df1 = pd.read_excel(datafilepath, sheet_name='Network connectivity')\n",
    "# df1_unique = df1.drop_duplicates(subset=['Link ID']).dropna(subset=['Link ID', 'A X_COORD', 'A Y_COORD', 'B X_COORD', 'B Y_COORD'])\n",
    "\n",
    "# a_nodes = df1_unique[['ANODE', 'A X_COORD', 'A Y_COORD']].rename(columns={\"ANODE\": \"node_id\", \"A X_COORD\": \"x_coord\", \"A Y_COORD\": \"y_coord\"})\n",
    "# b_nodes = df1_unique[['BNODE', 'B X_COORD', 'B Y_COORD']].rename(columns={\"BNODE\": \"node_id\", \"B X_COORD\": \"x_coord\", \"B Y_COORD\": \"y_coord\"})\n",
    "# nodes = pd.concat([a_nodes, b_nodes]).drop_duplicates().apply(pd.to_numeric)\n",
    "# nodes = nodes.drop_duplicates(subset=['node_id'])\n",
    "\n",
    "# a_edges = df1_unique[['Link ID', 'ANODE', 'BNODE', 'CapAB']].rename(columns={'Link ID': 'edge_id', \"ANODE\": \"src\", \"BNODE\": \"dest\", \"CapAB\": \"capacity\"})\n",
    "# b_edges = df1_unique[['Link ID', 'BNODE', 'ANODE', 'CapBA']].rename(columns={'Link ID': 'edge_id', \"BNODE\": \"src\", \"ANODE\": \"dest\", \"CapBA\": \"capacity\"})\n",
    "# edges = pd.concat([a_edges, b_edges]).drop_duplicates()\n",
    "# edges[['src','dest','capacity']] = edges[['src','dest','capacity']].apply(pd.to_numeric)\n",
    "# edges = edges.drop_duplicates(subset=['edge_id'])\n",
    "\n",
    "# print(df1_unique.head())\n",
    "# print(nodes.head())\n",
    "# print(edges.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2ffa20e5-27f3-4251-85de-700faa55a959",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Visualize the dataset\n",
    "# from matplotlib import collections  as mc\n",
    "# import pylab as pl\n",
    "\n",
    "# viz_subset = df1_unique\n",
    "# srcs  = zip(viz_subset['A X_COORD'], viz_subset['A Y_COORD'])\n",
    "# dests  = zip(viz_subset['B X_COORD'], viz_subset['B Y_COORD'])\n",
    "# lines = [list(a) for a in zip(srcs, dests)]\n",
    "\n",
    "# # Taken from https://stackoverflow.com/questions/21352580/plotting-numerous-disconnected-line-segments-with-different-colors\n",
    "# linecollection = mc.LineCollection(lines, colors=(1,0,0,1), linewidths=1)\n",
    "# fig, ax = pl.subplots()\n",
    "# ax.add_collection(linecollection)\n",
    "# ax.autoscale()\n",
    "# ax.set_aspect('equal', 'box')\n",
    "# ax.margins(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d0d551-250f-4daa-84ce-12644cd20f97",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
