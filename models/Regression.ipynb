{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6841782c-b123-4a13-85c4-69a7a3dab6ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "datadir = \"../data/\"\n",
    "datafilename = \"Certainty.xlsx\"\n",
    "datafilepath = datadir + datafilename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e42f6051-df94-493e-ac8a-b6983f804b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# Read in network information from sheet P0\n",
    "# For now we only use the folloiwng columns in each sheet\n",
    "cols_used = ['Link ID','ANODE','BNODE','A X_COORD','A Y_COORD','B X_COORD','B Y_COORD','Link Length(miles)','# of lanes-A','Capacity-A (veh/h)',\n",
    "            'auto volume(2010)-A','AADT(2010)-A','Speed(mph)-A','VMT-A']\n",
    "# # For now we assume all occurrences of the same link in the same sheet share the same attribute values\n",
    "df_p0= pd.read_excel(datafilepath, sheet_name='P0',usecols=cols_used).dropna(subset=['Link ID'])\n",
    "# df_p0_unique = df_p0.drop_duplicates(subset=['Link ID']).dropna(subset=['Link ID', 'A X_COORD', 'A Y_COORD', 'B X_COORD', 'B Y_COORD'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "624bc0f8-4dfb-4350-9220-16c60e9ff507",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Only Read in Project 3, 4 and 5\n",
    "integers = [ 3, 4, 5]\n",
    "def generate_combinations(integers):\n",
    "    all_combinations = []\n",
    "    # Loop through lengths from 1 to 6\n",
    "    for length in range(1, 1+len(integers)):\n",
    "        # Generate combinations of the current length\n",
    "        comb = combinations(integers, length)\n",
    "        # Convert each combination to a string, add \"p\", and add to the list\n",
    "        all_combinations.extend(['P' + ''.join(map(str, c)) for c in comb])\n",
    "    return all_combinations\n",
    "# Generate all combinations\n",
    "sheet_names = generate_combinations(integers)\n",
    "\n",
    "# Insert \"P0\" at the beginning of the list\n",
    "sheet_names.insert(0, \"P0\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6600ef0f-dc1c-41ab-962a-8dd2a513eb3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Read in all 2^6 sheets of data ###\n",
    "\n",
    "\n",
    "# ## Create a list of sheet names ##\n",
    "\n",
    "# # List of the first 6 integers\n",
    "# integers = [1, 2, 3, 4, 5, 6]\n",
    "\n",
    "# # Function to generate combinations\n",
    "# def generate_combinations(integers):\n",
    "#     all_combinations = []\n",
    "#     # Loop through lengths from 1 to 6\n",
    "#     for length in range(1, 7):\n",
    "#         # Generate combinations of the current length\n",
    "#         comb = combinations(integers, length)\n",
    "#         # Convert each combination to a string, add \"p\", and add to the list\n",
    "#         all_combinations.extend(['P' + ''.join(map(str, c)) for c in comb])\n",
    "#     return all_combinations\n",
    "\n",
    "# # Generate all combinations\n",
    "# sheet_names = generate_combinations(integers)\n",
    "\n",
    "# # Insert \"P0\" at the beginning of the list\n",
    "# sheet_names.insert(0, \"P0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7b5dde9-a937-428d-9b0e-4a68b9cb8dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing the sample set\n",
    "dataset = []\n",
    "for ii in range(len(sheet_names)):\n",
    "    # read the correct sheet \n",
    "    df= pd.read_excel(datafilepath, sheet_name=sheet_names[ii],usecols=cols_used).dropna(subset=['Link ID'])\n",
    "    \n",
    "    # dataframe for edge attributes\n",
    "    df_edge_attr= np.column_stack((df['Link Length(miles)'],df['# of lanes-A'],df['Capacity-A (veh/h)'], df['Speed(mph)-A'],df['auto volume(2010)-A']))\n",
    "    # edge_attr = torch.tensor(df_edge_attr, dtype=torch.float)\n",
    "\n",
    "    df_edge_labels = df['AADT(2010)-A'].to_numpy()\n",
    "    # edge_labels = torch.tensor(df['AADT(2010)-A'].to_numpy(),dtype=torch.float)\n",
    "    # new_data = Data(x=x, edge_index=edge_index, edge_attr=edge_attr, y=edge_labels)\n",
    "    \n",
    "    sample = (df_edge_attr, df_edge_labels)\n",
    "    dataset.append(sample)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14c8e3f5-cc2a-4bfe-8b77-57859981e632",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_258328/3189767075.py:11: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:274.)\n",
      "  X_train = torch.tensor(X_train, dtype=torch.float)\n"
     ]
    }
   ],
   "source": [
    "X = [sample[0] for sample in dataset]\n",
    "y = [sample[1] for sample in dataset]\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.25)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Convert data to PyTorch tensors\n",
    "X_train = torch.tensor(X_train, dtype=torch.float)\n",
    "y_train = torch.tensor(y_train,dtype=torch.float)\n",
    "X_val = torch.tensor(X_val,dtype=torch.float)\n",
    "y_val = torch.tensor(y_val,dtype=torch.float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab2504ea-52fa-4646-8f14-0ceb9d98871b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/duantu/soft/transportation_venv/lib/python3.12/site-packages/torch/cuda/__init__.py:619: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Loss: 4.69e+09\n",
      "Epoch 2/100, Loss: 4.26e+09\n",
      "Epoch 3/100, Loss: 3.84e+09\n",
      "Epoch 4/100, Loss: 3.38e+09\n",
      "Epoch 5/100, Loss: 2.88e+09\n",
      "Epoch 6/100, Loss: 2.37e+09\n",
      "Epoch 7/100, Loss: 1.85e+09\n",
      "Epoch 8/100, Loss: 1.34e+09\n",
      "Epoch 9/100, Loss: 8.84e+08\n",
      "Epoch 10/100, Loss: 5.15e+08\n",
      "Epoch 11/100, Loss: 2.67e+08\n",
      "Epoch 12/100, Loss: 1.45e+08\n",
      "Epoch 13/100, Loss: 1.18e+08\n",
      "Epoch 14/100, Loss: 1.27e+08\n",
      "Epoch 15/100, Loss: 1.29e+08\n",
      "Epoch 16/100, Loss: 1.22e+08\n",
      "Epoch 17/100, Loss: 1.15e+08\n",
      "Epoch 18/100, Loss: 1.13e+08\n",
      "Epoch 19/100, Loss: 1.13e+08\n",
      "Epoch 20/100, Loss: 1.13e+08\n",
      "Epoch 21/100, Loss: 1.13e+08\n",
      "Epoch 22/100, Loss: 1.13e+08\n",
      "Epoch 23/100, Loss: 1.12e+08\n",
      "Epoch 24/100, Loss: 1.12e+08\n",
      "Epoch 25/100, Loss: 1.12e+08\n",
      "Epoch 26/100, Loss: 1.11e+08\n",
      "Epoch 27/100, Loss: 1.11e+08\n",
      "Epoch 28/100, Loss: 1.11e+08\n",
      "Epoch 29/100, Loss: 1.11e+08\n",
      "Epoch 30/100, Loss: 1.10e+08\n",
      "Epoch 31/100, Loss: 1.10e+08\n",
      "Epoch 32/100, Loss: 1.10e+08\n",
      "Epoch 33/100, Loss: 1.10e+08\n",
      "Epoch 34/100, Loss: 1.10e+08\n",
      "Epoch 35/100, Loss: 1.10e+08\n",
      "Epoch 36/100, Loss: 1.10e+08\n",
      "Epoch 37/100, Loss: 1.10e+08\n",
      "Epoch 38/100, Loss: 1.10e+08\n",
      "Epoch 39/100, Loss: 1.09e+08\n",
      "Epoch 40/100, Loss: 1.09e+08\n",
      "Epoch 41/100, Loss: 1.09e+08\n",
      "Epoch 42/100, Loss: 1.09e+08\n",
      "Epoch 43/100, Loss: 1.09e+08\n",
      "Epoch 44/100, Loss: 1.09e+08\n",
      "Epoch 45/100, Loss: 1.09e+08\n",
      "Epoch 46/100, Loss: 1.09e+08\n",
      "Epoch 47/100, Loss: 1.09e+08\n",
      "Epoch 48/100, Loss: 1.09e+08\n",
      "Epoch 49/100, Loss: 1.09e+08\n",
      "Epoch 50/100, Loss: 1.09e+08\n",
      "Epoch 51/100, Loss: 1.09e+08\n",
      "Epoch 52/100, Loss: 1.09e+08\n",
      "Epoch 53/100, Loss: 1.09e+08\n",
      "Epoch 54/100, Loss: 1.09e+08\n",
      "Epoch 55/100, Loss: 1.08e+08\n",
      "Epoch 56/100, Loss: 1.08e+08\n",
      "Epoch 57/100, Loss: 1.08e+08\n",
      "Epoch 58/100, Loss: 1.08e+08\n",
      "Epoch 59/100, Loss: 1.08e+08\n",
      "Epoch 60/100, Loss: 1.08e+08\n",
      "Epoch 61/100, Loss: 1.08e+08\n",
      "Epoch 62/100, Loss: 1.08e+08\n",
      "Epoch 63/100, Loss: 1.08e+08\n",
      "Epoch 64/100, Loss: 1.08e+08\n",
      "Epoch 65/100, Loss: 1.08e+08\n",
      "Epoch 66/100, Loss: 1.08e+08\n",
      "Epoch 67/100, Loss: 1.08e+08\n",
      "Epoch 68/100, Loss: 1.08e+08\n",
      "Epoch 69/100, Loss: 1.08e+08\n",
      "Epoch 70/100, Loss: 1.08e+08\n",
      "Epoch 71/100, Loss: 1.08e+08\n",
      "Epoch 72/100, Loss: 1.08e+08\n",
      "Epoch 73/100, Loss: 1.08e+08\n",
      "Epoch 74/100, Loss: 1.08e+08\n",
      "Epoch 75/100, Loss: 1.08e+08\n",
      "Epoch 76/100, Loss: 1.08e+08\n",
      "Epoch 77/100, Loss: 1.08e+08\n",
      "Epoch 78/100, Loss: 1.08e+08\n",
      "Epoch 79/100, Loss: 1.08e+08\n",
      "Epoch 80/100, Loss: 1.07e+08\n",
      "Epoch 81/100, Loss: 1.07e+08\n",
      "Epoch 82/100, Loss: 1.07e+08\n",
      "Epoch 83/100, Loss: 1.07e+08\n",
      "Epoch 84/100, Loss: 1.07e+08\n",
      "Epoch 85/100, Loss: 1.07e+08\n",
      "Epoch 86/100, Loss: 1.07e+08\n",
      "Epoch 87/100, Loss: 1.07e+08\n",
      "Epoch 88/100, Loss: 1.07e+08\n",
      "Epoch 89/100, Loss: 1.07e+08\n",
      "Epoch 90/100, Loss: 1.07e+08\n",
      "Epoch 91/100, Loss: 1.07e+08\n",
      "Epoch 92/100, Loss: 1.07e+08\n",
      "Epoch 93/100, Loss: 1.07e+08\n",
      "Epoch 94/100, Loss: 1.07e+08\n",
      "Epoch 95/100, Loss: 1.07e+08\n",
      "Epoch 96/100, Loss: 1.07e+08\n",
      "Epoch 97/100, Loss: 1.07e+08\n",
      "Epoch 98/100, Loss: 1.07e+08\n",
      "Epoch 99/100, Loss: 1.07e+08\n",
      "Epoch 100/100, Loss: 1.07e+08\n",
      "Absolute Validation Loss: 1.26e+08\n",
      "\n",
      " Relative Loss =  1.26e+08  /  5.24e+09  =  3.34e-02 \n",
      "\n",
      "Sample 1 Predictions:\n",
      " tensor([2.3410e+04, 3.5078e+04, 3.8206e+04, 3.9745e+04, 4.4562e+04, 3.3164e+04,\n",
      "        3.2377e+04, 4.2343e+04, 4.2151e+04, 4.0090e+04, 4.0173e+04, 4.1799e+04,\n",
      "        4.1787e+04, 3.7094e+04, 3.7643e+04, 3.0646e+04, 3.7400e+04, 3.4429e+04,\n",
      "        4.5509e+04, 3.6774e+04, 4.6372e+04, 4.6925e+04, 3.4988e+04, 4.4175e+04,\n",
      "        4.3883e+04, 5.8032e+04, 5.5913e+04, 5.5919e+04, 5.5923e+04, 6.6785e+04,\n",
      "        7.0643e+04, 5.8284e+04, 4.3014e+04, 5.5693e+04, 5.7268e+04, 4.6653e+04,\n",
      "        6.3549e+04, 6.8735e+04, 6.1717e+04, 7.7726e+04, 5.0821e+04, 7.0886e+04,\n",
      "        5.8033e+04, 4.6002e+04, 5.1448e+04, 7.4886e+04, 6.6295e+04, 3.5262e+04,\n",
      "        4.2229e+04, 4.0177e+04, 4.2167e+04, 2.5191e+04, 4.2639e+04, 3.4420e+04,\n",
      "        2.8963e+04, 3.4250e+04, 4.0589e+04, 3.5299e+04, 6.5029e+04, 6.8933e+04,\n",
      "        5.5806e+04, 4.6278e+04, 4.4482e+04, 4.6609e+04, 4.9137e+04, 5.4659e+04,\n",
      "        3.5496e+04, 4.9666e+04, 4.9213e+04, 4.9803e+04, 4.3096e+04, 1.7508e+04,\n",
      "        4.2332e+04, 3.9733e+04, 2.8277e+04, 9.5391e+04, 9.5803e+04, 8.9495e+04,\n",
      "        9.4716e+04, 1.0135e+05, 9.1997e+04, 1.1956e+04, 5.6520e+02, 9.0614e+03,\n",
      "        8.6348e+03, 1.2487e+04, 7.6432e+03, 1.3882e+03, 1.4198e+04, 1.6016e+04,\n",
      "        1.4192e+04, 2.3608e+04, 2.7960e+04, 1.5656e+04, 1.4378e+04, 3.9858e+04,\n",
      "        4.2233e+04, 2.0584e+04, 1.4017e+04, 1.0275e+04, 8.8432e+03, 5.8290e+04,\n",
      "        4.5424e+04, 4.8927e+03, 1.6847e+04, 9.5228e+02, 8.5484e+03, 7.7496e+03,\n",
      "        7.7468e+03, 1.6180e+04, 1.8074e+04, 1.2466e+04, 7.1250e+04, 3.1976e+04,\n",
      "        4.3927e+04, 1.2537e+04, 8.9712e+04, 9.5741e+04, 1.0704e+05, 8.8366e+04,\n",
      "        8.1718e+04, 7.3379e+04, 6.7338e+04, 8.6291e+04, 7.8483e+04, 7.5117e+04,\n",
      "        6.9233e+04, 8.1759e+04, 8.8431e+04, 8.3101e+04, 7.6650e+04, 6.8063e+04,\n",
      "        6.3664e+04, 7.7703e+04, 7.4699e+04, 7.0942e+04, 6.8478e+04, 8.3101e+04,\n",
      "        8.1316e+04, 7.4142e+04, 7.8611e+04, 7.5112e+04, 8.2577e+04, 7.3021e+04,\n",
      "        7.3538e+04, 7.5827e+04, 7.3774e+04, 7.6410e+04, 7.3424e+04, 7.5849e+04,\n",
      "        7.3745e+04, 7.2397e+04, 7.1606e+04, 6.6994e+04, 3.3825e+04, 1.9301e+04,\n",
      "        1.8003e+04, 2.4753e+04, 3.7218e+04, 3.7382e+04, 3.6237e+04, 2.9613e+04,\n",
      "        3.7675e+04, 3.6753e+04, 2.9555e+04, 3.4891e+04, 4.1443e+04, 3.7853e+04,\n",
      "        3.8004e+04, 3.5304e+04, 4.1438e+04, 3.7863e+04, 3.7533e+04, 2.0897e+04,\n",
      "        2.2243e+04, 2.5890e+04, 2.3509e+04, 2.5902e+04, 2.0572e+04, 2.2766e+04,\n",
      "        1.9890e+04, 2.1871e+04, 1.9082e+04, 5.2223e+04, 2.0333e+04, 4.7236e+04,\n",
      "        5.1114e+04, 4.6661e+04, 1.1190e+05, 7.0777e+04, 7.1038e+04, 6.8316e+04,\n",
      "        9.7601e+04, 7.7257e+04, 8.6329e+04, 6.9650e+04, 8.3678e+04, 9.8720e+04,\n",
      "        6.7569e+04, 1.5493e+05, 7.0027e+04, 1.4053e+05, 6.4868e+04, 1.4651e+05,\n",
      "        8.5937e+04, 1.3343e+05, 7.1030e+04, 1.4482e+05, 7.4689e+04, 1.0953e+05,\n",
      "        1.1678e+05, 1.1578e+05, 1.0173e+05, 1.2334e+05, 9.5445e+04, 1.0932e+05,\n",
      "        8.9114e+04, 1.1651e+05, 9.5212e+04, 7.9543e+04, 6.8326e+04, 9.7169e+04,\n",
      "        8.8597e+04, 1.0059e+05, 7.2564e+04, 9.5037e+04, 7.3638e+04, 6.6061e+04,\n",
      "        1.0587e+05, 1.0583e+05, 8.1951e+04, 7.4370e+04, 1.0466e+05, 9.8014e+04,\n",
      "        1.0434e+05, 6.1106e+04, 5.3225e+04, 6.1135e+04, 5.2550e+04, 5.8954e+04,\n",
      "        6.9446e+04, 5.8944e+04, 5.9663e+04, 2.4776e+04, 5.7973e+04, 2.4588e+04,\n",
      "        2.0623e+04, 1.9955e+05, 2.3624e+04, 1.5195e+05, 1.7223e+05, 1.2427e+05,\n",
      "        1.9582e+05, 2.3835e+05, 1.9415e+05, 2.4440e+05, 1.8898e+05, 1.9877e+05,\n",
      "        2.4503e+05, 2.5270e+05, 2.3520e+05, 2.4008e+05, 2.1853e+05, 2.1829e+05,\n",
      "        2.6190e+05, 2.4824e+05, 2.5082e+05, 2.2496e+05, 2.2222e+05, 2.1122e+05,\n",
      "        2.5144e+05, 2.3484e+05, 2.7827e+05, 2.8074e+05, 2.4116e+05, 2.7372e+05,\n",
      "        2.6901e+05, 2.5838e+05, 2.3404e+05, 2.6670e+05, 2.5590e+05, 2.3691e+05,\n",
      "        2.3220e+05, 2.3760e+05, 2.2100e+05, 1.7756e+05, 1.5256e+05, 1.7808e+05,\n",
      "        1.4703e+05, 1.2685e+05, 1.1673e+05, 8.5992e+04, 9.7338e+04, 1.1260e+05,\n",
      "        9.8489e+04, 9.4705e+04, 2.4961e+04, 1.1034e+05, 1.0408e+05, 9.2272e+04,\n",
      "        8.6174e+04, 1.0080e+05, 9.5715e+04, 9.6549e+04, 9.3067e+04, 8.8682e+04,\n",
      "        4.6149e+04, 8.9146e+04, 8.5203e+04, 5.3864e+04, 5.8395e+04, 5.9987e+04,\n",
      "        4.8550e+04, 4.2944e+04, 5.1916e+04, 4.7262e+04, 4.2296e+04, 4.2589e+04,\n",
      "        4.3168e+04, 4.3614e+04, 3.9542e+04, 3.9145e+04, 4.1316e+04, 3.9326e+04,\n",
      "        7.7599e+04, 7.5323e+04, 8.1000e+04, 7.5555e+04, 6.5212e+04, 7.2853e+04,\n",
      "        7.9999e+04, 7.1253e+04, 5.9714e+04, 5.4355e+04, 7.4048e+04, 7.4706e+04,\n",
      "        6.3458e+04, 7.1517e+04, 5.7771e+04, 6.5294e+04, 5.5897e+04, 5.0894e+04,\n",
      "        6.6095e+04, 5.9124e+04, 5.6630e+04, 5.2129e+04, 6.7463e+04, 6.4927e+04,\n",
      "        5.2757e+04, 5.0915e+04, 6.6736e+04, 6.6795e+04, 4.8573e+04, 4.9917e+04,\n",
      "        6.1204e+04, 6.3427e+04, 3.9469e+04, 3.9738e+04, 4.7386e+04, 4.7674e+04,\n",
      "        4.1697e+04, 9.3994e+04, 1.0290e+05, 1.6647e+05, 1.3501e+05, 1.3356e+05,\n",
      "        1.5342e+05, 1.5501e+05, 1.3030e+05, 1.3105e+05, 1.5540e+05, 1.4625e+05,\n",
      "        8.6572e+04, 8.4078e+04, 8.6590e+04, 8.4078e+04, 8.9392e+04, 8.7968e+04,\n",
      "        8.9402e+04, 8.7944e+04, 8.9425e+04, 8.7925e+04, 8.9427e+04, 8.7899e+04,\n",
      "        6.5062e+04, 6.4682e+04, 7.5353e+04, 7.6204e+04, 6.3087e+04, 6.2316e+04,\n",
      "        7.4253e+04, 7.3332e+04, 6.4567e+04, 6.7186e+04, 4.7475e+04, 9.0345e+04,\n",
      "        1.1094e+05, 6.7464e+04, 3.0878e+04, 7.3975e+04, 7.9648e+04, 7.9592e+04,\n",
      "        8.1842e+04, 1.2634e+05, 1.2572e+05, 1.2173e+02, 4.7092e+04, 7.8857e+03,\n",
      "        8.5158e+02, 5.8240e+04, 1.1335e+05, 5.0575e+04, 5.2905e+04, 3.2900e+04,\n",
      "        2.9967e+04, 3.2930e+04, 2.8461e+04, 2.8621e+04, 2.7246e+04, 2.3400e+04,\n",
      "        2.3268e+04, 2.0779e+04, 2.2064e+04, 2.2350e+04, 2.2158e+04, 2.6478e+04,\n",
      "        5.5216e+04, 4.3679e+04, 3.4730e+04, 3.4088e+04, 7.3744e+04, 7.0004e+04,\n",
      "        6.3901e+04, 5.7345e+04, 9.2867e+04, 5.3510e+04, 7.5362e+04, 7.2846e+04,\n",
      "        1.0391e+05, 8.8513e+04, 1.0594e+05, 8.9737e+04, 1.0746e+05, 9.2115e+04,\n",
      "        8.5042e+04, 8.0346e+04, 8.8671e+04, 8.8671e+04, 7.8252e+04, 7.4020e+04,\n",
      "        9.3774e+04, 7.7984e+04, 7.0455e+04, 6.0675e+04, 1.0384e+05, 9.4755e+04,\n",
      "        8.6552e+04, 7.8949e+04, 9.2648e+04, 8.4819e+04, 6.7956e+04, 5.8774e+04,\n",
      "        7.0771e+04, 6.2834e+04, 8.2438e+04, 1.1095e+05, 1.0326e+05, 1.3813e+05,\n",
      "        1.1383e+05, 1.4324e+05, 1.2332e+05, 1.2145e+05, 1.0165e+05, 1.6867e+05,\n",
      "        1.5389e+05, 9.9094e+04, 9.9303e+04, 1.1421e+05, 1.1257e+05, 6.0970e+04,\n",
      "        6.7266e+04, 8.7018e+04, 8.9614e+04, 8.8160e+04, 8.9438e+04, 8.7047e+04,\n",
      "        8.9601e+04, 4.8388e+04, 5.6987e+04, 5.9657e+04, 7.5039e+04, 4.6149e+04,\n",
      "        7.4857e+04, 5.3529e+04, 5.3308e+04, 7.1421e+04, 6.6259e+04, 6.9596e+04,\n",
      "        6.6470e+04, 2.1390e+04, 4.1090e+04, 4.8430e+04, 2.9360e+04, 4.1855e+04,\n",
      "        3.9299e+04, 3.8043e+04, 3.4883e+04, 4.1878e+04, 3.9283e+04, 4.9937e+04,\n",
      "        8.6553e+04, 8.7076e+04, 6.8314e+04, 7.1715e+04, 7.4900e+04, 7.9142e+04,\n",
      "        5.8185e+04, 5.8458e+04, 6.9555e+04, 6.8732e+04, 6.9444e+04, 1.0192e+05,\n",
      "        7.5801e+03, 6.7131e+03, 8.1219e+03, 7.5801e+03, 9.1187e+04, 1.8816e+04,\n",
      "        1.8189e+04, 1.3833e+04, 4.3286e+04, 5.8685e+04, 5.3722e+04, 6.7258e+04,\n",
      "        7.1100e+04, 6.7231e+04, 7.1088e+04, 5.6304e+04, 5.6300e+04, 5.6294e+04,\n",
      "        5.8431e+04, 4.4162e+04, 4.4457e+04, 3.5245e+04, 4.6673e+04, 4.7230e+04,\n",
      "        3.7046e+04, 4.5802e+04, 4.6716e+04, 5.2116e+04, 4.6310e+04, 4.6154e+04,\n",
      "        5.1486e+04, 5.7153e+04, 4.4222e+04, 4.5872e+04, 4.2236e+04, 4.2354e+04,\n",
      "        3.9676e+04, 5.3957e+04, 5.2456e+04, 6.5789e+04, 6.2999e+04, 5.9718e+04,\n",
      "        5.0857e+04, 6.2164e+04, 5.2887e+04, 5.3634e+04, 4.6981e+04, 5.6457e+04,\n",
      "        4.8265e+04, 5.3556e+04, 4.5922e+04, 5.2848e+04, 4.4687e+04, 4.4579e+04,\n",
      "        5.1783e+04, 4.5040e+04, 4.2552e+04, 4.5209e+04, 4.5260e+04, 4.4606e+04,\n",
      "        4.5288e+04, 3.6007e+04, 3.6138e+04, 3.7874e+04, 3.6424e+04, 3.6094e+04,\n",
      "        3.7854e+04, 3.2253e+04, 3.1551e+04, 2.5809e+04, 2.5887e+04, 1.5176e+04,\n",
      "        1.8831e+04, 2.3582e+04, 3.5314e+04, 3.8468e+04, 4.0020e+04, 4.4877e+04,\n",
      "        3.3383e+04, 3.2590e+04, 4.2640e+04, 4.2447e+04, 4.0360e+04, 4.0444e+04,\n",
      "        4.2084e+04, 4.2071e+04, 3.7383e+04, 3.7936e+04, 3.0880e+04, 3.7691e+04,\n",
      "        3.4695e+04, 3.7741e+04, 3.4420e+04, 4.7405e+04, 5.1261e+04, 4.4944e+04,\n",
      "        5.3443e+04, 4.8012e+04, 4.3527e+04, 4.6374e+04, 4.5918e+04, 4.7591e+04,\n",
      "        4.7802e+04, 4.6618e+04, 3.2466e+04, 1.7631e+04, 4.2651e+04, 4.0031e+04,\n",
      "        2.8486e+04, 5.6073e+04, 5.7687e+04, 4.6995e+04, 6.4021e+04, 6.9238e+04,\n",
      "        6.2174e+04, 7.8305e+04, 4.0326e+04, 5.8661e+04, 4.5756e+04, 4.9245e+03,\n",
      "        1.6967e+04, 9.6117e+02, 8.6115e+03, 7.8059e+03, 7.8031e+03, 1.6295e+04,\n",
      "        1.8205e+04, 1.2549e+04])\n",
      "Sample 2 Predictions:\n",
      " tensor([2.3481e+04, 3.5176e+04, 3.8314e+04, 3.9859e+04, 4.4693e+04, 3.3255e+04,\n",
      "        3.2465e+04, 4.2466e+04, 4.2274e+04, 4.0201e+04, 4.0285e+04, 4.1917e+04,\n",
      "        4.1905e+04, 3.7214e+04, 3.7764e+04, 3.0743e+04, 3.7521e+04, 3.4539e+04,\n",
      "        4.5630e+04, 3.6886e+04, 4.6497e+04, 4.7051e+04, 3.5095e+04, 4.4291e+04,\n",
      "        4.3999e+04, 5.8197e+04, 5.6071e+04, 5.6077e+04, 5.6081e+04, 6.6981e+04,\n",
      "        7.0827e+04, 5.8450e+04, 4.3127e+04, 5.5856e+04, 5.7441e+04, 4.6794e+04,\n",
      "        6.3744e+04, 6.8943e+04, 6.1906e+04, 7.7966e+04, 5.0961e+04, 7.1071e+04,\n",
      "        5.8198e+04, 4.6136e+04, 5.1601e+04, 7.5085e+04, 6.6500e+04, 3.5369e+04,\n",
      "        4.2339e+04, 4.0301e+04, 4.2277e+04, 2.5263e+04, 4.2750e+04, 3.4524e+04,\n",
      "        2.9048e+04, 3.4354e+04, 4.0715e+04, 3.5406e+04, 6.5229e+04, 6.9140e+04,\n",
      "        5.5964e+04, 4.6418e+04, 4.4616e+04, 4.6751e+04, 4.9287e+04, 5.4829e+04,\n",
      "        3.5592e+04, 4.9813e+04, 4.9359e+04, 4.9950e+04, 4.3225e+04, 1.7559e+04,\n",
      "        4.2464e+04, 3.9856e+04, 2.8364e+04, 9.5649e+04, 9.6063e+04, 8.9733e+04,\n",
      "        9.4972e+04, 1.0163e+05, 9.2243e+04, 1.1988e+04, 5.6733e+02, 9.0893e+03,\n",
      "        8.6612e+03, 1.2521e+04, 7.6661e+03, 1.3938e+03, 1.4238e+04, 1.6065e+04,\n",
      "        1.4233e+04, 2.3680e+04, 2.8043e+04, 1.5703e+04, 1.4420e+04, 3.9976e+04,\n",
      "        4.2359e+04, 2.0647e+04, 1.4057e+04, 1.0303e+04, 8.8703e+03, 5.8444e+04,\n",
      "        4.5561e+04, 4.9059e+03, 1.6896e+04, 9.5597e+02, 8.5745e+03, 7.7729e+03,\n",
      "        7.7701e+03, 1.6228e+04, 1.8129e+04, 1.2501e+04, 7.1462e+04, 3.2033e+04,\n",
      "        4.4004e+04, 1.2563e+04, 8.9879e+04, 9.5913e+04, 1.0724e+05, 8.8538e+04,\n",
      "        8.1875e+04, 7.3525e+04, 6.7470e+04, 8.6467e+04, 7.8641e+04, 7.5267e+04,\n",
      "        6.9369e+04, 8.1924e+04, 8.8611e+04, 8.3269e+04, 7.6804e+04, 6.8196e+04,\n",
      "        6.3787e+04, 7.7859e+04, 7.4847e+04, 7.1082e+04, 6.8612e+04, 8.3269e+04,\n",
      "        8.1479e+04, 7.4289e+04, 7.8769e+04, 7.5261e+04, 8.2744e+04, 7.3165e+04,\n",
      "        7.3684e+04, 7.5978e+04, 7.3921e+04, 7.6561e+04, 7.3570e+04, 7.6000e+04,\n",
      "        7.3892e+04, 7.2540e+04, 7.1747e+04, 6.6857e+04, 3.3755e+04, 1.9263e+04,\n",
      "        1.7967e+04, 2.4705e+04, 3.7139e+04, 3.7303e+04, 3.6161e+04, 2.9552e+04,\n",
      "        3.7595e+04, 3.6675e+04, 2.9495e+04, 3.4817e+04, 4.1357e+04, 3.7776e+04,\n",
      "        3.7933e+04, 3.5239e+04, 4.1352e+04, 3.7786e+04, 3.7453e+04, 2.0858e+04,\n",
      "        2.2200e+04, 2.5838e+04, 2.3463e+04, 2.5851e+04, 2.0534e+04, 2.2722e+04,\n",
      "        1.9853e+04, 2.1830e+04, 1.9047e+04, 5.2123e+04, 2.0291e+04, 4.7148e+04,\n",
      "        5.1016e+04, 4.6574e+04, 3.7108e+04, 2.5044e+04, 2.3865e+04, 2.4451e+04,\n",
      "        3.2870e+04, 2.8067e+04, 2.9412e+04, 2.5304e+04, 3.1395e+04, 3.3237e+04,\n",
      "        2.6291e+04, 5.0487e+04, 2.7887e+04, 4.6144e+04, 2.6942e+04, 4.7992e+04,\n",
      "        4.5059e+04, 4.3956e+04, 4.2264e+04, 4.9990e+04, 4.9712e+04, 3.8497e+04,\n",
      "        4.1104e+04, 4.0760e+04, 3.4037e+04, 4.0496e+04, 3.8932e+04, 3.6204e+04,\n",
      "        3.4196e+04, 3.8382e+04, 3.6013e+04, 2.6747e+04, 2.5075e+04, 3.2142e+04,\n",
      "        3.1694e+04, 3.2319e+04, 2.5471e+04, 3.2020e+04, 2.4424e+04, 2.2634e+04,\n",
      "        3.6231e+04, 3.8447e+04, 2.7642e+04, 2.7524e+04, 3.5887e+04, 3.7228e+04,\n",
      "        3.5784e+04, 2.1040e+04, 2.2596e+04, 2.1059e+04, 2.2420e+04, 2.6104e+04,\n",
      "        2.3699e+04, 2.6117e+04, 2.0731e+04, 2.2949e+04, 2.0042e+04, 2.2045e+04,\n",
      "        1.9226e+04, 5.2639e+04, 2.0508e+04, 4.7602e+04, 5.1518e+04, 4.7021e+04,\n",
      "        6.7872e+04, 8.4420e+04, 6.7408e+04, 8.2370e+04, 6.8047e+04, 6.9258e+04,\n",
      "        8.3092e+04, 8.3603e+04, 8.0076e+04, 7.9878e+04, 7.4933e+04, 7.3360e+04,\n",
      "        8.8231e+04, 8.2642e+04, 8.4798e+04, 7.5593e+04, 7.5968e+04, 7.1407e+04,\n",
      "        8.5480e+04, 7.9107e+04, 9.3271e+04, 9.2912e+04, 8.0802e+04, 9.1862e+04,\n",
      "        8.9339e+04, 8.7118e+04, 7.8674e+04, 8.9661e+04, 8.5408e+04, 8.0473e+04,\n",
      "        7.8105e+04, 8.0729e+04, 7.4722e+04, 6.2443e+04, 5.4161e+04, 6.0113e+04,\n",
      "        5.0224e+04, 4.3726e+04, 4.1976e+04, 3.1195e+04, 3.5953e+04, 3.9501e+04,\n",
      "        3.6268e+04, 3.3976e+04, 8.7254e+03, 3.8808e+04, 3.7715e+04, 3.3297e+04,\n",
      "        3.2111e+04, 3.6266e+04, 3.5225e+04, 3.5068e+04, 3.4448e+04, 3.2268e+04,\n",
      "        1.7885e+04, 3.2409e+04, 3.1701e+04, 2.0430e+04, 2.1990e+04, 2.2580e+04,\n",
      "        1.8345e+04, 1.6315e+04, 1.9547e+04, 1.7832e+04, 1.6029e+04, 1.6124e+04,\n",
      "        1.6314e+04, 1.6459e+04, 1.4984e+04, 1.4849e+04, 1.5592e+04, 1.4872e+04,\n",
      "        2.3283e+04, 2.8080e+04, 2.4394e+04, 2.8136e+04, 2.4069e+04, 2.1534e+04,\n",
      "        2.8071e+04, 2.6294e+04, 2.2052e+04, 1.9061e+04, 2.6341e+04, 2.7149e+04,\n",
      "        2.2537e+04, 2.5563e+04, 2.0913e+04, 2.3631e+04, 2.0028e+04, 1.8307e+04,\n",
      "        2.3756e+04, 2.1307e+04, 2.0225e+04, 1.8746e+04, 2.4197e+04, 2.3537e+04,\n",
      "        1.9010e+04, 1.8590e+04, 2.4624e+04, 2.4905e+04, 1.7985e+04, 1.8730e+04,\n",
      "        2.2685e+04, 2.3716e+04, 1.4620e+04, 1.4979e+04, 1.7550e+04, 1.7872e+04,\n",
      "        1.5449e+04, 3.4578e+04, 3.6743e+04, 5.8607e+04, 4.8993e+04, 4.8424e+04,\n",
      "        5.4609e+04, 5.4995e+04, 4.7388e+04, 4.7723e+04, 5.5199e+04, 5.2261e+04,\n",
      "        3.2302e+04, 3.1230e+04, 3.2307e+04, 3.1229e+04, 3.2226e+04, 3.1634e+04,\n",
      "        3.2233e+04, 3.1627e+04, 3.2237e+04, 3.1623e+04, 3.2237e+04, 3.1612e+04,\n",
      "        2.4453e+04, 2.4171e+04, 2.8275e+04, 2.8508e+04, 2.3711e+04, 2.3399e+04,\n",
      "        2.7788e+04, 2.7448e+04, 2.5811e+04, 2.6441e+04, 1.7902e+04, 3.2612e+04,\n",
      "        3.9007e+04, 2.5188e+04, 1.1438e+04, 2.5763e+04, 2.9408e+04, 2.9376e+04,\n",
      "        2.4470e+04, 4.0161e+04, 3.9962e+04, 1.1149e+02, 1.5681e+04, 5.0044e+03,\n",
      "        2.3543e+02, 2.1945e+04, 3.9969e+04, 1.9861e+04, 2.0722e+04, 3.2639e+04,\n",
      "        2.9741e+04, 3.2669e+04, 2.8238e+04, 2.8397e+04, 2.7033e+04, 2.3225e+04,\n",
      "        2.3095e+04, 2.0623e+04, 2.1899e+04, 2.2185e+04, 2.1995e+04, 2.6279e+04,\n",
      "        5.4827e+04, 4.3384e+04, 3.4507e+04, 3.3871e+04, 7.3206e+04, 6.9496e+04,\n",
      "        6.3443e+04, 5.6940e+04, 9.2175e+04, 5.3135e+04, 7.4811e+04, 7.2316e+04,\n",
      "        1.0313e+05, 8.7856e+04, 1.0516e+05, 8.9080e+04, 1.0668e+05, 9.1458e+04,\n",
      "        8.4442e+04, 7.9784e+04, 8.8041e+04, 8.8041e+04, 7.7707e+04, 7.3509e+04,\n",
      "        9.3104e+04, 7.7441e+04, 6.9974e+04, 6.0272e+04, 1.0309e+05, 9.4077e+04,\n",
      "        8.5939e+04, 7.8398e+04, 9.1986e+04, 8.4221e+04, 6.7494e+04, 5.8387e+04,\n",
      "        7.0286e+04, 6.2414e+04, 8.1859e+04, 1.1014e+05, 1.0252e+05, 1.3710e+05,\n",
      "        1.1299e+05, 1.4217e+05, 1.2242e+05, 1.2056e+05, 1.0092e+05, 1.6740e+05,\n",
      "        1.5274e+05, 9.8380e+04, 9.8588e+04, 1.1337e+05, 1.1175e+05, 6.0565e+04,\n",
      "        6.6810e+04, 8.6402e+04, 8.8977e+04, 8.7544e+04, 8.8801e+04, 8.6431e+04,\n",
      "        8.8964e+04, 4.8084e+04, 5.6614e+04, 5.9263e+04, 7.4519e+04, 4.5864e+04,\n",
      "        7.4339e+04, 5.3183e+04, 5.2964e+04, 7.0932e+04, 6.5811e+04, 6.9121e+04,\n",
      "        6.6020e+04, 2.1256e+04, 4.0795e+04, 4.8126e+04, 2.9160e+04, 4.1563e+04,\n",
      "        3.9019e+04, 3.7792e+04, 3.4649e+04, 4.1585e+04, 3.9003e+04, 4.9621e+04,\n",
      "        8.5912e+04, 8.6431e+04, 6.7820e+04, 7.1194e+04, 7.4352e+04, 7.8561e+04,\n",
      "        5.7773e+04, 5.8044e+04, 6.9051e+04, 6.8234e+04, 6.8971e+04, 1.0118e+05,\n",
      "        7.5276e+03, 6.6676e+03, 8.0638e+03, 7.5276e+03, 9.0508e+04, 1.8685e+04,\n",
      "        1.8063e+04, 1.3742e+04, 4.3361e+04, 5.8795e+04, 5.3828e+04, 6.7388e+04,\n",
      "        7.1222e+04, 6.7353e+04, 7.1210e+04, 5.6409e+04, 5.6405e+04, 5.6399e+04,\n",
      "        5.8541e+04, 4.4239e+04, 4.4534e+04, 3.5316e+04, 4.6755e+04, 4.7314e+04,\n",
      "        3.7121e+04, 4.5882e+04, 4.6799e+04, 5.2211e+04, 4.6392e+04, 4.6236e+04,\n",
      "        5.1580e+04, 5.7260e+04, 4.4299e+04, 4.5953e+04, 4.2308e+04, 4.2427e+04,\n",
      "        3.9757e+04, 5.4057e+04, 5.2552e+04, 6.5916e+04, 6.3119e+04, 5.9830e+04,\n",
      "        5.0949e+04, 6.2283e+04, 5.2984e+04, 5.3733e+04, 4.7064e+04, 5.6562e+04,\n",
      "        4.8351e+04, 5.3661e+04, 4.6008e+04, 5.2953e+04, 4.4773e+04, 4.4665e+04,\n",
      "        5.1886e+04, 4.5127e+04, 4.2634e+04, 4.5296e+04, 4.5348e+04, 4.4693e+04,\n",
      "        4.5376e+04, 3.6073e+04, 3.6205e+04, 3.7945e+04, 3.6492e+04, 3.6164e+04,\n",
      "        3.7925e+04, 3.2311e+04, 3.1607e+04, 2.5861e+04, 2.5940e+04, 1.5204e+04,\n",
      "        1.8867e+04, 2.3629e+04, 3.5378e+04, 3.8540e+04, 4.0095e+04, 4.4964e+04,\n",
      "        3.3444e+04, 3.2648e+04, 4.2721e+04, 4.2528e+04, 4.0434e+04, 4.0518e+04,\n",
      "        4.2162e+04, 4.2150e+04, 3.7462e+04, 3.8016e+04, 3.0944e+04, 3.7771e+04,\n",
      "        3.4768e+04, 3.7821e+04, 3.4493e+04, 4.7508e+04, 5.1373e+04, 4.5041e+04,\n",
      "        5.3562e+04, 4.8116e+04, 4.3621e+04, 4.6474e+04, 4.6017e+04, 4.7690e+04,\n",
      "        4.7903e+04, 4.6720e+04, 3.2534e+04, 1.7665e+04, 4.2739e+04, 4.0112e+04,\n",
      "        2.8543e+04, 5.6169e+04, 5.7802e+04, 4.7089e+04, 6.4150e+04, 6.9376e+04,\n",
      "        6.2299e+04, 7.8464e+04, 4.0405e+04, 5.8763e+04, 4.5847e+04, 4.9332e+03,\n",
      "        1.7000e+04, 9.6362e+02, 8.6288e+03, 7.8214e+03, 7.8186e+03, 1.6326e+04,\n",
      "        1.8241e+04, 1.2572e+04])\n"
     ]
    }
   ],
   "source": [
    " \n",
    "# Define the neural network model\n",
    "class RegressionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RegressionModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(5, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.fc3 = nn.Linear(32, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "model = RegressionModel()\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training the model\n",
    "num_epochs = 100\n",
    "batch_size = 1\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss = 0.0\n",
    "    for i in range(X_train.shape[0]):\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_train[i])\n",
    "        loss = criterion(outputs.squeeze(), y_train[i])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {format(epoch_loss/X_train.shape[0],\".2e\")}')\n",
    "\n",
    "# Evaluate the model\n",
    "model.eval()\n",
    "val_loss = 0.0\n",
    "relative_val_loss = 0.0\n",
    "sum_edge_labels = 0.0\n",
    "with torch.no_grad():\n",
    "    for i in range(X_val.shape[0]):\n",
    "        outputs = model(X_val[i])\n",
    "        loss = criterion(outputs.squeeze(), y_val[i])\n",
    "        val_loss += loss.item()\n",
    "        \n",
    "        # Calculate Relative Validation Loss\n",
    "        edge_labels_tensor = criterion(torch.zeros(torch.Tensor.size(y_val[i])), y_val[i])\n",
    "        sum_edge_labels += edge_labels_tensor.item()\n",
    "        relative_loss = loss/edge_labels_tensor\n",
    "        relative_val_loss += relative_loss.item()\n",
    "print(f'Absolute Validation Loss: {format(val_loss/X_val.shape[0],\".2e\")}')\n",
    "\n",
    "# calculate average edge_labels_tensor.item()\n",
    "ave_edge_labels = sum_edge_labels/X_val.shape[0]\n",
    "print(\"\\n Relative Loss = \", format(val_loss/X_val.shape[0], '.2e'), \" / \", format(ave_edge_labels,'.2e'), \" = \", format(relative_val_loss/X_val.shape[0],'.2e'), \"\\n\")\n",
    " \n",
    "\n",
    "# Make predictions\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for i in range(X_val.shape[0]):\n",
    "        predictions = model(X_val[i]).squeeze()\n",
    "        print(f'Sample {i+1} Predictions:\\n', predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transportation_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
