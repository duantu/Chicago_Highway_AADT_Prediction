{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a covpy of New_Data that tries to localize data normalization to the training loop\n",
    "datadir = \"../data/\"\n",
    "datafilename_network = \"Link_info_network_and_proj.xlsx\"\n",
    "datafilename_proj = 'Certainty.xlsx'\n",
    "datafilepath_network = datadir + datafilename_network\n",
    "datafilepath_proj = datadir + datafilename_proj\n",
    "\n",
    "from itertools import combinations\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import torch\n",
    "from torch.utils.data import Subset\n",
    "import wandb\n",
    "\n",
    "\n",
    "# # start a new wandb run to track this script\n",
    "# wandb.init(\n",
    "#     # set the wandb project where this run will be logged\n",
    "#     project=\"CGN to predict AADT\",\n",
    "\n",
    "#     # track hyperparameters and run metadata\n",
    "#     config={\n",
    "#      \"hidden_layer_sizes\": [12,24,48,64],\n",
    "#     }\n",
    "# )\n",
    "\n",
    "network_cols_used = ['LINK','ANODE','BNODE','LENGTH','A X_COORD','A Y_COORD','B X_COORD','B Y_COORD','LANES_AB','LEFT_AB', 'RIGHT_AB','SPEED_AB','FSPD_AB','CAP_AB','LANES_BA','LEFT_BA','RIGHT_BA','SPEED_BA','FSPD_BA','CAP_BA'\n",
    "]\n",
    "\n",
    "# all 30000 links in the general network\n",
    "df_network= pd.read_excel(datafilepath_network,sheet_name='Link_Info',usecols=network_cols_used).dropna(subset=['LINK'])\n",
    "\n",
    "#650 links in the 6 projects\n",
    "df_proj= pd.read_excel(datafilepath_network, sheet_name='Project_Links')\n",
    "\n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "# Create a directed graph for the entire network\n",
    "G_network = nx.DiGraph()\n",
    "\n",
    "Anodes = df_network['ANODE'].tolist\n",
    "Bnodes = df_network['BNODE'].tolist\n",
    "\n",
    "# Add edges_ab with attributes\n",
    "edges = [\n",
    "    (row['ANODE'], row['BNODE'], {\n",
    "        'length': row['LENGTH'], \n",
    "        '#lanes': row['LANES_AB'], \n",
    "        'speed': row['SPEED_AB'], \n",
    "        'FSPD': row['FSPD_AB'],\n",
    "        'capacity': row['CAP_AB'],\n",
    "        'Link ID': row['LINK'],\n",
    "        'AADT Before': 0,\n",
    "        'auto volume before': 0,\n",
    "        'VMT before': 0\n",
    "    })\n",
    "    for _, row in df_network.iterrows()\n",
    "]\n",
    "\n",
    "# Add edges_ba only if lanes_ba != 0\n",
    "edges += [\n",
    "    (row['BNODE'], row['ANODE'], {\n",
    "        'length': row['LENGTH'], \n",
    "        '#lanes': row['LANES_BA'], \n",
    "        'speed': row['SPEED_BA'], \n",
    "        'FSPD': row['FSPD_BA'],\n",
    "        'capacity': row['CAP_BA'],\n",
    "        'Link ID': row['LINK'],\n",
    "        'AADT Before': 0,\n",
    "        'auto volume before': 0,\n",
    "        'VMT before': 0\n",
    "    })\n",
    "    for _, row in df_network.iterrows() if row['LANES_BA'] != 0\n",
    "]\n",
    "\n",
    "# List of pairs of integers that corresond to eges in project\n",
    "proj_links = list(zip(df_proj['ANODE'], df_proj['BNODE']))\n",
    "\n",
    "\n",
    "# Identifying edges in G_network that correspond to the list of pairs\n",
    "# proj_edges = [(u, v) for u, v in proj_links if G_network.has_edge(u, v)]\n",
    "\n",
    "# for project links, the \"AADT Before\",\"auto voulme before\" and \"VMT before\" attributes are known and not zero\n",
    "for i, item in enumerate(edges):\n",
    "    if (edges[i][0], edges[i][1]) in proj_links:\n",
    "        idx = proj_links.index((edges[i][0], edges[i][1]))\n",
    "        edges[i][2]['AADT Before'] = df_proj.loc[idx]['AADT(2010)-B']\n",
    "        edges[i][2]['auto volume before'] = df_proj.loc[idx]['auto volume(2010)-B']\n",
    "        edges[i][2]['VMT before'] = df_proj.loc[idx]['VMT-B']\n",
    "        \n",
    "    \n",
    "\n",
    "\n",
    "G_network.add_edges_from(edges)\n",
    "\n",
    "\n",
    "# Creating the subgraph with only project edges\n",
    "G_proj = G_network.edge_subgraph(proj_links)\n",
    "\n",
    "\n",
    "G_network_dual = nx.line_graph(G_network) # dual graph of G_network\n",
    "G_proj_dual = nx.line_graph(G_proj) # dual graph of G_proj\n",
    "\n",
    "\n",
    "node_attrs = {}\n",
    "for edge in G_network_dual.nodes:\n",
    "    u, v = edge\n",
    "    edge_data = G_network[u][v]  # Get the attributes of the edge from the original graph\n",
    "    node_attrs[edge] = f\"length: {edge_data['length']}, #lanes: {edge_data['#lanes']}, speed: {edge_data['speed']}, FSPD: {edge_data['FSPD']}, capacity: {edge_data['capacity']}, Link ID: {edge_data['Link ID']}, AADT Before: {edge_data['AADT Before']}, auto volume before: {edge_data['auto volume before']}, VMT before: {edge_data['VMT before']}\"\n",
    "\n",
    "# Convert dictionary to DataFrame\n",
    "def parse_attributes(attr_string):\n",
    "    attributes = {}\n",
    "    for item in attr_string.split(', '):\n",
    "        key, value = item.split(': ')\n",
    "        attributes[key] = float(value)\n",
    "    return attributes\n",
    "\n",
    "# Create a DataFrame\n",
    "df_node_attr_network = pd.DataFrame(\n",
    "    [dict(**parse_attributes(value), index=key) for key, value in node_attrs.items()],\n",
    "    index=[key for key in node_attrs.keys()]\n",
    ")\n",
    "\n",
    "# the dataframe that contains all node attributes in the dual network graph\n",
    "df_node_attr_network = df_node_attr_network.iloc[:, :-1]\n",
    "\n",
    "df_network_link_id = df_node_attr_network['Link ID']\n",
    "\n",
    "# reorder columns of the data frame so that Link ID is the last column\n",
    "df_node_attr_network = df_node_attr_network.iloc[:,[0,1,2,3,4,6,7,8,5]]\n",
    "\n",
    "# Mapping node labels in G_network_dual, which are pairs of integers that coorespond to node labels in G_network,\n",
    "# to the natural numbers \n",
    "node_mapping = {node: i + 1 for i, node in enumerate(G_network_dual.nodes())}\n",
    "# Step 3: Construct a new graph with edges represented by mapped values\n",
    "mapped_G_network_dual = nx.DiGraph()\n",
    "\n",
    "# Add nodes with mapped attributes\n",
    "for original_node in G_network_dual.nodes(data=True):\n",
    "    original_label, attributes = original_node\n",
    "    new_label = node_mapping[original_label]\n",
    "    \n",
    "    # Add the new node with the same attributes\n",
    "    mapped_G_network_dual.add_node(new_label, **attributes)\n",
    "\n",
    "\n",
    "# Add edges to the mapped graph using the new labels\n",
    "for u, v in G_network_dual.edges():\n",
    "    new_u = node_mapping[u]\n",
    "    new_v = node_mapping[v]\n",
    "    mapped_G_network_dual.add_edge(new_u, new_v)\n",
    "\n",
    "# Identifying vertices in G_network_dual that correspond to links in G_proj\n",
    "proj_nodes = [pair for pair in proj_links if pair in G_network_dual.nodes()]\n",
    "\n",
    "# Keep the first occurrence of each unique element in proj_nodes\n",
    "# proj_nodes_unique = list(dict.fromkeys(proj_nodes))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torch_geometric.nn import GCNConv\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "node_attr_used = df_node_attr_network.columns[:-1]\n",
    "num_attr = len(node_attr_used)\n",
    "\n",
    "num_node_label = 3\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(num_attr, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels,hidden_channels)\n",
    "        self.conv3 = GCNConv(hidden_channels, num_attr)\n",
    "        self.head = nn.Linear(num_attr, num_node_label)\n",
    "        # add a multi-layer perceptron?\n",
    "        # reference examples of using GNN for classification eg. citation network\n",
    "        # can pretty much reuse all the previous layers (backbone) except for the last layer (layer head) which is specific to the application \n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = x.relu()\n",
    "\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = x.relu()\n",
    "\n",
    "        x = self.conv3(x, edge_index)\n",
    "        x = x.relu()\n",
    "        \n",
    "        x = self.head(x)\n",
    "        \n",
    "        # x = x.relu() # no activation function\n",
    "        # x = F.dropout(x, p=0.5, training=self.training)\n",
    "        # x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "\n",
    "###### Try this example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list of all sheet names\n",
    "\n",
    "num_projs = 6\n",
    "proj_list = list(range(1,num_projs+1))\n",
    "def generate_combinations(proj_list):\n",
    "    all_combinations = []\n",
    "    # Loop through lengths from 1 to 6\n",
    "    for length in range(1, 1+num_projs):\n",
    "        # Generate combinations of the current length\n",
    "        comb = combinations(proj_list, length)\n",
    "        # Convert each combination to a string, add \"p\", and add to the list\n",
    "        all_combinations.extend(['P' + ''.join(map(str, c)) for c in comb])\n",
    "    return all_combinations\n",
    "# Generate all combinations\n",
    "sheet_names = generate_combinations(proj_list)\n",
    "\n",
    "# Insert \"P0\" at the beginning of the list\n",
    "# sheet_names.insert(0, \"P0\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/duantu/soft/transportation_venv/lib/python3.12/site-packages/torch/cuda/__init__.py:619: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n"
     ]
    }
   ],
   "source": [
    "# import torch\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.loader import DataLoader\n",
    "# Prepare the dataset \n",
    "\n",
    "proj_cols_used = ['Link ID','ANODE','BNODE','A X_COORD','A Y_COORD','B X_COORD','B Y_COORD','Link Length(miles)','# of lanes-A','Capacity-A (veh/h)',\n",
    "            'auto volume(2010)-A','AADT(2010)-B','AADT(2010)-A','Speed(mph)-A','VMT-A']\n",
    "proj_features_used = ['']\n",
    "dataset = []\n",
    "\n",
    "# edge index is shared among all samples\n",
    "# Get the list of edges as tuples (source, target)\n",
    "# the entire network has edge_index \n",
    "edges = list(mapped_G_network_dual.edges())\n",
    "\n",
    "# Separate source and target nodes\n",
    "source_nodes = [edge[0] for edge in edges]\n",
    "target_nodes = [edge[1] for edge in edges]\n",
    "\n",
    "# Create the edge_index tensor: shape [2, num_edges]\n",
    "edge_index = torch.tensor([source_nodes, target_nodes], dtype=torch.long)-1\n",
    "\n",
    "\n",
    "for ii in range(len(sheet_names)):\n",
    "    # read the correct sheet \n",
    "    df_sample= pd.read_excel(datafilepath_proj,sheet_name=sheet_names[ii],usecols=proj_cols_used).dropna(subset=['Link ID'])\n",
    "    df_sample = df_sample.astype('float64')\n",
    "    df_sample_label = df_sample[['AADT(2010)-A','auto volume(2010)-A','VMT-A']]\n",
    "    df_sample_label.columns = ['AADT after','auto volume after','VMT after']\n",
    "    # # scale AADT values in df_sample \n",
    "    # df_sample_scaled = scaler.fit_transform(pd.DataFrame(df_sample[['AADT(2010)-A','auto volume(2010)-A','VMT-A']]))\n",
    "    # # put the scaled data into a dataframe\n",
    "    # df_sample_scaled = pd.DataFrame(df_sample_scaled,index=df_sample.index, columns=['scaled AADT after','scaled auto volume after','scaled VMT after'])\n",
    "\n",
    "\n",
    "    # x = node features\n",
    "    # The entire network has node features\n",
    "    df_node_attr_network_used = df_node_attr_network.iloc[:, :-1]\n",
    "    x = torch.tensor(np.array(df_node_attr_network_used), dtype=torch.float)\n",
    "################################################################################   \n",
    "    # # edge index\n",
    "    # already defined as above\n",
    "###################################################################################\n",
    "    \n",
    "    # Create the label tensor y\n",
    "        \n",
    "    # -1 for unlabeled nodes, correct class labels (positive real value) for labeled nodes in G_proj_dual\n",
    "    # create a all -1 dataframe to store labels of the entire network\n",
    "    df_labels_network = pd.DataFrame(-2, index = df_node_attr_network.index, columns = ['Link ID','AADT after','auto volume after','VMT after'])\n",
    "    network_links = list(G_network_dual.nodes)\n",
    "\n",
    "    df_labels_network['Link ID'] = df_network_link_id\n",
    "\n",
    "    # for index in range(1, len(df_sample)+1):  # Loop through each row in df_sample\n",
    "    #     link_id = df_sample.loc[index, 'Link ID']  # Get 'Link ID' for the current row\n",
    "    \n",
    "    # # Find the row in df_scaled_labels_network where 'Link ID' matches\n",
    "    #     row_idx = df_scaled_labels_network[df_scaled_labels_network['Link ID'] == link_id].index\n",
    "    \n",
    "    #     if not row_idx.empty:  # If a match is found\n",
    "    #     # Assign the value of 'AADT(2010)-A' from df_sample to 'AADT_after' in df_scaled_labels_network\n",
    "    #         df_scaled_labels_network = df_scaled_labels_network.astype('float64')\n",
    "    #         df_scaled_labels_network.loc[row_idx, 'scaled AADT after'] = df_sample_scaled.loc[index, 'scaled AADT after']\n",
    "    #         df_scaled_labels_network.loc[row_idx, 'scaled auto volume after'] = df_sample_scaled.loc[index, 'scaled auto volume after']\n",
    "    #         df_scaled_labels_network.loc[row_idx, 'scaled VMT after'] = df_sample_scaled.loc[index, 'scaled VMT after']\n",
    "\n",
    "    for index in range(1,len(df_sample)+1):  # Loop through each row in df_sample\n",
    "        pairwise_link_id = list(zip(df_sample.loc[:,'ANODE'], df_sample.loc[:,'BNODE']))[index-1]  # Get 'pairwise link ID' for the current row\n",
    "    \n",
    "    # Find the row in df_scaled_labels_network where 'pairwise Link ID' matches\n",
    "        row_idx = df_labels_network[df_labels_network.index == pairwise_link_id].index\n",
    "    \n",
    "        if not row_idx.empty:  # If a match is found\n",
    "        # Assign the value of 'AADT(2010)-A' from df_sample to 'AADT_after' in df_scaled_labels_network\n",
    "          \n",
    "            df_labels_network = df_labels_network.astype('float64')\n",
    "            df_labels_network.loc[row_idx, 'AADT after'] = df_sample_label.loc[index, 'AADT after']\n",
    "            df_labels_network.loc[row_idx, 'auto volume after'] = df_sample_label.loc[index, 'auto volume after']\n",
    "            df_labels_network.loc[row_idx, 'VMT after'] = df_sample_label.loc[index, 'VMT after']\n",
    "\n",
    "    \n",
    "    y = torch.tensor(np.array(df_labels_network[['AADT after','auto volume after','VMT after']]), dtype=torch.float)  \n",
    "\n",
    "##################################################################################  \n",
    "    # Create the train_mask so that training only happens on project links\n",
    "    train_mask = pd.DataFrame(False, index = df_node_attr_network.index, columns = ['Masked'])\n",
    "\n",
    "    row_idx = df_labels_network[df_labels_network['AADT after'] != -2].index\n",
    "\n",
    "    train_mask.loc[row_idx] = True\n",
    "    train_mask = torch.tensor(train_mask.values, dtype=torch.bool)\n",
    "\n",
    "    new_data = Data(x=x, edge_index=edge_index, y=y, train_mask=train_mask)\n",
    "\n",
    "\n",
    "    # Append all 63 samples\n",
    "    dataset.append(new_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define our data loader\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    \n",
    "    def __init__(self,data):\n",
    "        super().__init__()\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x = self.data[index]['x']\n",
    "        edge_index = self.data[index]['edge_index']\n",
    "        target = self.data[index]['y']\n",
    "        return x, edge_index, target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "val_indices =[26, 19, 52, 11, 50, 40, 46, 39, 60, 29, 21, 12, 24, 48, 10, 17, 32, 13]\n",
    "\n",
    "val_dataset = Subset(dataset, val_indices)\n",
    "\n",
    "val_dataset = CustomDataset(val_dataset)\n",
    "\n",
    "val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training settings\n",
    "# model = GCN(hidden_channels=24)\n",
    "# state = torch.load( \"./checkpoint.pth.tar\")\n",
    "# model.load_state_dict(state[\"model_state\"])\n",
    "# criterion = torch.nn.MSELoss()\n",
    "\n",
    "\n",
    "\n",
    "model = GCN(hidden_channels=24)\n",
    "state = torch.load( \"./savedModels/GCN model epoch_1143.pth\")\n",
    "model.load_state_dict(torch.load('./savedModels/GCN model epoch_1143.pth', weights_only=True))\n",
    "criterion = torch.nn.MSELoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Val sample # 26 Absolute MSE (AADT): 9.16e+01\n",
      "Val sample # 26 Relative MSE (AADT): 9.34e-01\n"
     ]
    }
   ],
   "source": [
    "# SCALE THINGS LAST MINUTE \n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# scaler = MinMaxScaler()\n",
    "scaler = StandardScaler()\n",
    "\n",
    "sum_target_2_norm_squared_AADT = 0\n",
    "for i, (x, edge_index, target) in enumerate(val_loader):\n",
    "    target_scaled = torch.tensor(scaler.fit_transform(target[0]))\n",
    "    sum_target_2_norm_squared_AADT += criterion(target_scaled[train_mask.squeeze()][:,0], torch.zeros(target_scaled[train_mask.squeeze()][:,0].shape))\n",
    "normalizing_const_ave_val_AADT = sum_target_2_norm_squared_AADT/len(val_loader)\n",
    "\n",
    "\n",
    "total_val_loss_AADT = 0\n",
    "df_rel_err = pd.DataFrame(columns = val_indices, index = np.array(df_node_attr_network['Link ID'],dtype = int).reshape(-1,1)[np.array(train_mask)==1])\n",
    "\n",
    "# df_unscaled_AADT_comparison = pd.DataFrame(columns = [sheet_names[i] for i in df_rel_err.columns],index = proj_order_list)\n",
    "\n",
    "for i, (x, edge_index, target) in enumerate(val_loader):\n",
    "    x_scaled = torch.tensor(scaler.fit_transform(x[0]))\n",
    "    x_scaled = x_scaled.to(torch.float32)\n",
    "    target_scaled = torch.tensor(scaler.fit_transform(target[0]))\n",
    "    out = model(x_scaled, edge_index[0]).squeeze()\n",
    "    normalizing_constant_single_val_set_AADT = criterion(target_scaled[train_mask.squeeze()][:,0], torch.zeros(target_scaled[train_mask.squeeze()][:,0].shape))\n",
    "    \n",
    "    loss_AADT = criterion(out[train_mask.squeeze()][:,0], target_scaled[train_mask.squeeze()][:,0])\n",
    "    # print(out[train_mask.squeeze()])\n",
    "    # print(target[0][train_mask.squeeze()][:,0])\n",
    "    relative_loss_AADT = loss_AADT/normalizing_constant_single_val_set_AADT.item()\n",
    "    \n",
    "    print(f'\\nVal sample # {val_indices[i]} Absolute MSE (AADT): {format(loss_AADT,\".2e\")}') \n",
    "    print(f'Val sample # {val_indices[i]} Relative MSE (AADT): {format(relative_loss_AADT,\".2e\")}')\n",
    "\n",
    "    #########\n",
    "    unscaled_output = scaler.inverse_transform(out[train_mask.squeeze()].detach().numpy())[:,0]\n",
    "    unscaled_node_label = scaler.inverse_transform(target[0][train_mask.squeeze()].detach().numpy())[:,0] \n",
    " \n",
    "\n",
    "    # # convert unscaled_label into a data frame\n",
    "    # df_unscaled_node_label = pd.DataFrame(data = unscaled_node_label)\n",
    "    # df_unscaled_node_label.columns = ['unscaled_node_label']\n",
    "    # df_unscaled_node_label.index = df_node_attr_network.index[train_mask.squeeze()]\n",
    "    # sorted_df_unscaled_node_label = df_unscaled_node_label.reindex(sorted_output_indices)\n",
    "\n",
    "    # df_real_label = pd.read_excel(datafilepath_proj,sheet_name=sheet_names[val_indices[i]],usecols=proj_cols_used).dropna(subset=['Link ID'])['AADT(2010)-A']\n",
    "\n",
    "    # print(list(zip(sorted_df_unscaled_node_label['unscaled_node_label'], df_real_label)))\n",
    "    \n",
    "\n",
    "    \n",
    "    df_rel_err.iloc[:,i] = abs(unscaled_output-unscaled_node_label)/unscaled_node_label\n",
    "\n",
    "    num = 0\n",
    "    for j in range(len(unscaled_node_label)):\n",
    "        if abs(unscaled_output[j]-unscaled_node_label[j])/unscaled_node_label[j]>0.9:\n",
    "            num+=1\n",
    "    # print([abs(unscaled_output-unscaled_node_label)/unscaled_node_label][0][491])\n",
    "    print('validation sample #', val_indices[i], 'has ',num, 'links with relative error > 0.9')\n",
    "    ########\n",
    "    \n",
    "    total_val_loss_AADT += loss_AADT.item()\n",
    "      \n",
    "# normalizing constant: squared l2 norm divided by length of y[train_mask] \n",
    "relative_ave_val_mse_AADT = total_val_loss_AADT/len(val_loader)/normalizing_const_ave_val_AADT.item()\n",
    "print(f\"\\nRelative average validation set mse (AADT) = {total_val_loss_AADT/len(val_loader):.2e} / {normalizing_const_ave_val_AADT.item():.2e} = {relative_ave_val_mse_AADT:.2e}\")\n",
    "\n",
    "# make a copy of the data frame that has all the relative error info of each validation sample \n",
    "import copy\n",
    "df_rel_err_copy = copy.copy(df_rel_err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transportation_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
